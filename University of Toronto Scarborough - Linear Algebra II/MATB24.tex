\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{dsfont}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{hyperref}

\newcommand{\ts}{\textsuperscript}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Linear Algebra II -- Summer 2017}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}


\begin{document}

\title{MATB24 Lecture Notes}
\date{University of Toronto Scarborough -- Summer 2017}
\author{Joshua Concon}
\maketitle

Pre-reqs are MATA23, which is Linear Algebra I.
Instructor is Dr. Louis de Thanhoffer de Volcsey. I highly recommend sitting at the front since he likes to teach with the blackboards. The only pre-requisite for this course is MATA23, which is linear algebra I. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

Note that from Sections Wednesday, May 17, 2017 and onwards, I stop typing up lecture notes as all the topics are sporatic and sometimes not clearly explained, so I begin to learn the content from the Textbook (Linear Algebra -- Edition 3
by John B. Fraleigh, Raymond A. Beauregard) and teach the concepts that the lectures are supposed to cover, but instead, purely based of the textbook.

\tableofcontents

\pagebreak

\section{Wednesday, May 3, 2017}

Before he got to the definition of fields, Dr. Louis de Thanhoffer de Volcsey talked a lot about how MATB24 was simply going to be a generalization of everything in MATA23 and various specific examples of fields as well as practical applications, but since fields were never defined, I will omit all of that stuff since it does not really make any sense without the formal definition of fields.\\
\\
In the definition of the field, the lecturer presents the unique additive identity as $0$, the unique additive inverse as $(-a)$ for $a$, the unique multiplicative identity as $1$, and the unique multiplicative inverse of $a$ as $(a^{-1})$. Although this is true for fields like $\mathbb{R}$, this is misleading, as the inverses and identities are supposed to be general, and thinking of the additive inverse for example as $(-a)$ is not necessarily true for some fields.\\
\\
Take for example the field $\mathbb{Z}_2 = \{ 0,1 \}$ (This is essentially all of $\mathbb{Z}$ but each element $a$ becomes the remainder of $a/2$, so $0,1,2,3,4,5$ becomes $0,1,0,1,0,1$).\\
\\
If you consider the case when $a = 1$, the additive inverse of 1 is 1 (\underline{i.e.} $1 + 1 = 0$). So using the definition of $(-a)$ as the additive inverse in this case is confusing since $(-a) = a = 1$. So in the definition of a field, these are all generalized.

\subsection{Fields}

\begin{tcolorbox}[title=Fields]
	A field ($\mathbb{F}$) is a set with two operations:
	\begin{itemize}
		\item{$+:\mathbb{F} \times \mathbb{F} \longrightarrow \mathbb{F}$}
		\item{$\circ:\mathbb{F} \times \mathbb{F} \longrightarrow \mathbb{F}$}
	\end{itemize}
	And these two operations must satisfy all of the following axioms:\\
	(Axioms are for $\forall a,b,c \in \mathbb{F}$)\\
	\textbf{Axioms for $+$:}
	\begin{itemize}
		\item{\underline{associative:} $(a + b) + c = a + (b + c)$}
		\item{\underline{commutative:} $a + b = b + a$}
		\item{\underline{(unique) additive identity:} $\exists j \in \mathbb{F} : j + a = a$}
		\item{\underline{(unique) additive inverse:} $\exists k \in \mathbb{F} : k + a = j$ where $j$ is the unique additive identity}
	\end{itemize}
	\textbf{Axioms for $\circ$:}
	\begin{itemize}
		\item{\underline{associative:} $(a \circ b) \circ c = a \circ (b \circ c)$}
		\item{\underline{commutative:} $a \circ b = b \circ a$}
		\item{\underline{(unique) multiplicative identity:} $\exists q \in \mathbb{F} : q \circ a = a$}
		\item{\underline{(unique) multiplicative inverse:} $\exists w \in \mathbb{F}\setminus\{ j \} : w \circ a = q$ where $q$ is the unique multiplicative identity and $j$ is the unique additive identity}
	\end{itemize}
	\textbf{Axioms for both:}
	\begin{itemize}
		\item{\underline{distributive:} $a \circ (b + c) = (a \circ b) + (c \circ a)$}
	\end{itemize}
\end{tcolorbox}

\textbf{Example 1:} Is $\mathbb{Z}$ a field?\\
\\
\textbf{Solution 1:} No, since $2 \in\mathbb{Z}$ has no multiplicative inverse in $\mathbb{Z}$\\ (\underline{i.e.} $\nexists a \in\mathbb{Z} : 2a = 1$), however, $\mathbb{Q}$ is a field and solves this problem.

\section{Friday, May 5, 2017}

\subsection{Vector Spaces}

Vector spaces are, in the paraphrased words of the lecturer, "simply the generalization of the real-valued vectors from MATA23". So they are a collection of elements where each element is in a field $(\mathbb{F})$.\\
\\
The following definition of Vector Spaces is also slightly inconsistent when compared to the lecture and the textbook definition to be more general for reasons provided before (for the definition of a field). Like a single mother who is pulled into a conversation with her child about the missing paternal figure, I would prefer to just tell a more truthful definition now that may be more abstract and slightly harder to understand in order to be more consistent, rather than clearing up disinformation later.

\begin{tcolorbox}[title=Vector Spaces]
A Vector Space over the field $\mathbb{F}$ is a set $V$ with the following operations:
\begin{itemize}
	\item{$+:V \times V \longrightarrow V$ (vector addition)}
	\item{$\circ:\mathbb{F} \times V \longrightarrow V$ (scalar multiplication)}
\end{itemize}
And these two operations must satisfy all of the following axioms:\\
(axioms are for $\forall v,w,u \in V$ and $\forall a,b \in\mathbb{F}$)\\
\textbf{Axioms for $+$:}
\begin{itemize}
	\item{\underline{associative:} $(v + w) + u = v + (w + u)$}
	\item{\underline{commutative:} $v + w = w + v$}
	\item{\underline{existence of the zero vector:} $\exists n \in V : n + v = v$ ($n$ must also be unique)}
	\item{\underline{existence of the inverse vector:} $\exists b \in V : b + v = n$ where $n$ is the zero vector and $b$ is unique for each vector $v$.}
\end{itemize}
\textbf{Axioms for $\circ$:}
\begin{itemize}
	\item{\underline{associative:} $(ab) \circ v = a \circ (b \circ v)$}
	\item{\underline{preservation of scale:} $\exists j \in\mathbb{F} : j \circ v = v$ ($j $ is usually 1 here)}
\end{itemize}
\textbf{Axioms for both:}
\begin{itemize}
	\item{\underline{distributive:} $(a + b) \circ v = (a \circ v) + (b \circ v)$\\
	$a \circ (v + w) = (a \circ v) + (a \circ w)$}
\end{itemize}
\end{tcolorbox}

\textbf{Example 1:} Prove the distributive axiom for $\mathbb{F}^n$ which is a vector space over $\mathbb{F}$ with operations $(+, \circ)$ from the real numbers $(\mathbb{R})$ and:
\begin{itemize}
	\item{$v = (v_1, ... ,  v_n) \in\mathbb{F}^n$}
	\item{$w = (w_1, ... ,  w_n) \in\mathbb{F}^n$}
	\item{$a,b \in\mathbb{F}$}
\end{itemize}

\textbf{Solution 1:} Since we are using the $(+, \circ)$ operations from the real numbers, then we know that...
\begin{itemize}
	\item{$v+w = (v_1 + w_1, ... ,  v_n + w_n)$}
	\item{$a \circ v = (a \circ v_1, ... , a \circ v_n)$}
\end{itemize}
Consider $a \circ (v + w)$
\begin{align*}
a \circ (v + w) &= a \circ (v_1 + w_1, ... ,  v_n + w_n)\\
&= (a \circ (v_1 + w_1), ... ,a \circ (v_n + w_n))\\
&= ((a \circ v_1) + (a \circ w_1), ... , (a \circ v_n) + (a \circ w_n))\\
&= ((a \circ v_1) + ... + (a \circ v_n)) + ((a \circ w_1) + ... + (a \circ w_n))\\
&= (a \circ (v_1, ... ,  v_n)) + (a \circ (w_1, ... ,  w_n))\\
&= (a \circ v) + (a \circ w)
\end{align*}
Consider $(a + b) \circ v$
\begin{align*}
	(a + b) \circ v &= (v_1, ... ,  v_n)\\
	&= (((a + b) \circ v_1), ... ,  ((a + b) \circ v_n))\\
	&= ((a \circ v_1) + (b \circ v_1), ... ,  (a \circ v_n) + (b \circ v_n))\\
	&= ((a \circ v_1), ... ,  (a \circ v_n)) + ((b \circ v_1), ... ,  (b \circ v_n))\\
	&= (a \circ (v_1, ... ,  v_n)) + (b \circ (v_1, ... ,  v_n))\\
	&= (a \circ v) + (b \circ v)
\end{align*}

\textbf{Example 2:} $\mathbb{M}_{k,l}(\mathbb{F})$ which is all $k \times l$ matrices with coefficients in $\mathbb{F}$.\\
This is also field, for example:\\
($\forall M,N \in \mathbb{M}_{k,l}(\mathbb{F})$, $\forall a \in\mathbb{F}$ and $\forall i,j : 1 \leq i \leq k, 1 \leq j \leq l$)

\begin{itemize}
	\item{$M + N = (M + N)$ where $(M+N)_{ij} = M_{ij} + N_{ij}$ (ij\ts{th} entry addition)}
	\item{0 is the zero matrix where $0_{ij} = 0$}
	\item{$a \circ M = (aM)$ where $(aM)_{ij} = a (M_{ij})$ (scalar multiplication)}
	\item{$M + (-M) = 0$ where $(-M)_{ij} = -(M_{ij})$ (existence of the inverse vector)}
\end{itemize}

\textbf{Example 3:} $\mathbb{M}_{k,l}^{1}(\mathbb{F})$ (all $k \times l$ matrices with coefficients in $\mathbb{F}$ and $\forall M \in \mathbb{M}_{k,l}^{1}(\mathbb{F})$, $M_{1,1} = 1$)\\
\\
This is not a Vector Space because scalar multiplication does not work if the scalar being multiplied by is not 1. (\underline{i.e.} if $a \neq 1$ then $a(M_{1,1}) = a \neq 1$)\\
\\
\textbf{Example 4:} $\mathbb{M}_{k,l}^{0}(\mathbb{F})$ however, is a vector space, since\\ $\forall a \in\mathbb{F}$ and $\forall M \in \mathbb{M}_{k,l}^{0}(\mathbb{F})$, that $a(M_{1,1}) = a \cdot 0 = 0$.\\
\\
\textbf{Example 5:} $P(\mathbb{F})$ which is all the polynomials over the field $\mathbb{F}$, defined by:
$$P(\mathbb{F}) = \langle \sum_{i=1}^{n} {\alpha_i \cdot x^i} \rangle$$
This is also a Vector Space, here are some examples of how the operations work.
\begin{itemize}
	\item{$\sum_{i=1}^{n} {\alpha_i \cdot x^i} + \sum_{i=1}^{n} {\beta_i \cdot x^i} = \sum_{i=1}^{n} {(\alpha_i + \beta_i) \cdot x^i}$ (vector addition)}
	\item{$\gamma\sum_{i=1}^{n} {\alpha_i \cdot x^i} = \sum_{i=1}^{n} {(\gamma\alpha_i) \cdot x^i}$ (scalar multiplication)}
\end{itemize}

\textbf{Example 6:} Take any set $X$, $V$ is a vector space over $\mathbb{F}$\\
$V^X = \{ functions : X \longrightarrow V \}$ is a vector space over $\mathbb{F}$\\
Consider $f,g \in V^X$, $\forall \alpha \in \mathbb{F}$
$$(f + g)(x) = f(x) + g(x)$$

\newpage

\section{Wednesday, May 10, 2017}

\subsection{Basics of Vector Spaces}

\textbf{Basic Example of a Vector Space:} $\mathbb{F}^n$

\begin{tcolorbox}[title=Definition: Subspace]
	A subspace $W$ of a vector space $V$ is a non-empty subset of $V$ with the following axioms:
	\begin{itemize}
		\item{$\forall w_1, w_2 \in W : w_1 + w_2 \in W$}
		\item{$\forall \alpha \in \mathbb{F}, w \in W : \alpha \cdot w \in W$}
	\end{itemize}
\end{tcolorbox}

\textbf{Example 1:} Is $P(\mathbb{F}) = \{ \sum_{i=0}^{\infty} \alpha_i \cdot x^i : \alpha_i \in \mathbb{F} \}$ a subspace?\\

\textbf{Solution 1:} $0 \in P(\mathbb{F})$, therefore $P(\mathbb{F})$ is non-empty.\\
\\
Consider $\forall w, v \in P(\mathbb{F})$ and $\forall r \in\mathbb{F}$\\
where $w = \sum_{i=0}^{\infty} w_i \cdot x^i$ and $v = \sum_{i=0}^{\infty} v_i \cdot x^i$
$$w+w = \sum_{i=0}^{\infty} w_i \cdot x^i + \sum_{i=0}^{\infty} v_i \cdot x^i = \sum_{i=0}^{\infty} (w_i + v_i) \cdot x^i \in P(\mathbb{F})$$
$$rw = r\sum_{i=0}^{\infty} w_i \cdot x^i = \sum_{i=0}^{\infty} (r\cdot w_i) \cdot x^i \in P(\mathbb{F})$$
Therefore $P(\mathbb{F})$ is a subspace\\
\\
\textbf{Example 2:} Is $P_n (\mathbb{F}) = \{ \sum_{i=0}^{\infty} \alpha_i \cdot x^i : \alpha_i \in \mathbb{F}, \text{ degree} = n \}$ a subspace?\\

\textbf{Solution 2:} Consider $1 + 2x + 3x^2, -3x^2 \in P_2 (\mathbb{F})$
$$1 + 2x + 3x^2 + (-3x^2) = 1 + 2x \text{ which is not in } P_2 (\mathbb{F})$$
Therefore $P_n (\mathbb{F})$ is not a vector space.\\
\\
\underline{Note:} The easiest way to check if something ($V$) is not a vector space is to check if $0 \in V$.\\
\\
\textbf{Example 3:} Are all matrices $A$ with $det(A) = 0$\\
\\
\textbf{Solution 3:} Consider $UT_n (\mathbb{F}) = \{ M \in M_{m,n} (\mathbb{F}) | M_{ij} = 0, i \geq j \}$\\
Take $M_1, M_2 \in UT_n (\mathbb{F})$\\
$$(M_1 + M_2)_{ij} = M_{1ij} + M_{2ij} = 0 + 0 = 0, \forall i,j$$
$$\forall \alpha \in \mathbb{F} : (\alpha M)_{ij} = \alpha (M_{ij}) = \alpha \cdot 0 = 0$$
Therefore $UT_n (\mathbb{F})$ is a subspace.

\subsection{Operations on Subspaces}
Considering subspaces $U,W$ of a vector space $V$
\begin{itemize}
	\item{$(U \cap W) = \{ v \in V : v\in U, v \in W \}$ is a subspace}
	\item{$(U + W) = \{ u+w : u \in U, w \in W \}$ is also a subspace}
	\item{However, $(U \cup W)$ is not a subspace because of the following example:\\
	Consider in $P(\mathbb{F})$ the subspaces $U = \{ \alpha x : \alpha \in \mathbb{F} \}$ and $W = \{ \alpha x^2 : \alpha \in \mathbb{F} \}$} and $U \cup W$ is not a subspace since it does not contain $x + x^2$, and so is not closed under vector addition.
\end{itemize}

\textbf{Recall:} if $v_1, ... , v_n$ is a basis, then all vectors are of the form $\sum_i \alpha_i v_i$
\begin{itemize}
\item{$sp\{v_1, ... , v_n\} = \{ \sum_i \alpha_i v_i : \alpha_i \in \mathbb{F} \}$}
\end{itemize}

\textbf{Result:} $sp(v_1, ... , v_n)$ is the smallest subspace containing $\{v_1, ... , v_n \}$\\
\begin{proof}
	(Proof of Result)\\
	Take $u,w \in sp(v_1, ... , v_n)$, so $u = \sum_i \alpha_i v_i, w = \sum_i \beta_i v_i$\\ so this implies that $u+w = \sum_i \alpha_i v_i + \sum_i \beta_i v_i = \sum_i (\beta_i + \alpha_i) v_i$.\\
	And Taking a $\gamma \in \mathbb{F} : \gamma u = \gamma \sum_i \alpha_i v_i = \sum_i (\alpha_i \cdot \gamma) v_i$\\
	So now we assume $W$ is the subspace containing $\{v_1, ... , v_n \}$\\
	$$\longrightarrow \forall \alpha \in \mathbb{F}, \alpha_i x_i \in W $$
	$$\longrightarrow sp\{v_1, ... , v_n \} \in W$$
	so $sp\{v_1, ... , v_n \}$ is the smallest subspace containing $\{v_1, ... , v_n \}$ since for every subspace $W$ containing $\{v_1, ... , v_n \}$, they must also contain $sp\{v_1, ... , v_n \}$
\end{proof}

\textbf{Example 4:} $sp\{1, 1+x, x^2 -1 \} \in P(\mathbb{F})$ ?\\
\\
\textbf{Solution 4:} A good war to check if a set is a subspace is to write the subspace as a span.\\
\begin{align*}
	sp\{1, 1+x, x^2 -1 \} &= a + b(1 + x) + c(x^2 - 1) | a,b,c \in\mathbb{F}\\
	&= (a+b-c) + bx + cx^2 | a,b,c \in\mathbb{F}\\
	&= P_{\leq 2} (\mathbb{F}) \in P(\mathbb{F})
\end{align*}

\textbf{Example 5:} $sp\{e_1, e_2 \} = sp\{(1,0), (0,1) \} \in M_{m,n} (\mathbb{F})$
$$E_{ij} \text{ has a coefficient of 0 everywhere except for a coefficient of 1 in entry } (i,j)$$

\textbf{Note:} $$\left( \begin{smallmatrix} a&b\\ c&d \end{smallmatrix} \right) = aE_{11} + bE_{12} + cE_{21} dE_{22}$$

\textbf{Example 5:}
\begin{align*}
	sp\{v_1, v_2, v_1 + v_2 \} &= av_1 + bv_2 + c(v_1 + v_2) |a,b,c \in\mathbb{F}\\
	&= (a+c)v_1 + (b+c)v_2 |a,b,c \in\mathbb{F}\\
	&= sp\{v_1, v_2 \}
\end{align*}
So when do we know that we have the minimum amount of terms to generate a Vector Space from a span of these terms?

\begin{tcolorbox}[title=Definition: Linearly Dependent]
	A vector $V$ is \textbf{linearly dependent} of vectors $\{v_1, ... , v_n \}$ if it is a linear combination of them.
\end{tcolorbox}

So if $v = (v_1, ... , v_n)$ is linearly dependent of $\{u_1, ... , u_n \}$ then $$sp\{v_1, ... , v_n \} = sp\{u_1, ... , u_n \}$$

A set of vectors is linearly independent if no vector is linearly dependent on the other vectors in the set.\\
\\
\textbf{Fact:} a set is linearly independent if $\sum_i a_i v_i = 0 \longrightarrow \forall a_i = 0$

\begin{proof}
	Assume $\sum_i a_i v_i = 0 \longrightarrow \forall a_i = 0$ and that some vector
	$$v_i = \sum_{j \neq i} a_j v_j, u_0 = \sum_{j = 0} a_j v_j$$
	$$|v_0 - a_1 v_1 - a_2 v_2 - ... - a_n v_n| = 0$$
	Assume that $\sum_i a_i v_i = 0$ (linearly independence)\\
	Assume that for some coefficient is not 0, say $a_1 \neq 0$.\\
	$$a_1 v_1 + \sum_{i\geq 2} a_i v_i = 0, v_1 = \sum_{i\geq 2} (\frac{-a_i}{a_1}) v_i$$
	This shows that $v_1$ is a linear combination of the other vectors, which violates our assumption that all these vectors $v_i$ are linearly independent. So it must be the case that $\sum_i a_i v_i = 0, \forall a_i = 0$ implies linear independence
\end{proof}

\newpage

\section{Friday, May 12, 2017}

\subsection{Bases of Vector Spaces}

\begin{tcolorbox}[title=Definition: A Basis of Vector Spaces]
	A basis for $V$ is a spanning set that is linearly independent
\end{tcolorbox}

\textbf{Result:} A spanning set $\{v_1, ..., v_n \}$ for $V$ is a basis $\Longleftrightarrow \forall v, \exists a_i, v = \sum^n_{i = 1} a_i v_i$

\begin{proof}(Proof of Result)\\
	\underline{Proof of $\longrightarrow$}\\
	$\forall v, \exists a_i, v = \sum^n_{i = 1} a_i v_i$, so since every vector $v$ can be made with the vectors $\{v_1, ..., v_n \}$, so this is a basis.\\
	\\
	\underline{Proof of $\longleftarrow$}\\
	For this, we need to show uniqueness. Consider $v = \sum^n_{i = 1} b_i v_i$
	\begin{align*}
		v = \sum^n_{i = 1} b_i v_i & \longrightarrow 0 = \sum^n_{i = 1} (a_i - b_i) v_i\\
		& \longrightarrow a_i - b_i = 0\\
		& \longrightarrow a_i = b_i
	\end{align*}
	Since these vectors are unique, this basis is unique, so every vector in $V$ can be formed by a linear combination of these unique vectors.
\end{proof}

\textbf{Corollary:} Assume we have a finite basis of size $n$.\\
Define a function
$$\{v_1, ..., v_n \} : V \mapsto \mathbb{F}^n, v \mapsto (a_1, ... , a_n) \text{ such that } v = \sum a_i v_i$$

These are the coordinates of the vector $v$ with respect to the basis $\{v_1, ..., v_n \}$

\begin{enumerate}
	\item{
	Every vector space has a basis, sometimes, this basis can be mysterious, such as:\\
	$\mathbb{R}^{\mathbb{R}}$, which does not have a basis that can be written.\\
	\\
	More precisely: every spanning set becomes a basis after removing linearly dependent vectors.\\
	\\
	\textbf{Corollary:} Consider spanning set $A = \{ v_1, ... ,v_n \}$ and basis $B = \{w_1, ..., w_n \}$.\\
	$A$ being a spanning set implies $m \leq n$. $B$ being a basis implies that $n \leq m$. Both of these imply that $n=m$
	}
	\item{
	Bases a Vector Space $V$ always have the same number of elements between other bases of $V$.
	}
\end{enumerate}

\subsection{Dimension}

\begin{tcolorbox}[title=Definition: Dimension]
	The number of elements in the basis of a Vector Space $V$ is called the dimension of $V$.
\end{tcolorbox}

\textbf{Example 1:} $\mathbb{F}^n = [e_1, ..., e_n]$ is a basis.\\
$\forall v\in V$ where $V$ is a Vector Space. $v = (v_1, ... , v_n) = \sum^n_{i=1} v_i\cdot e_i$\\
\\
\textbf{Corollary:} Any set of $n$ linearly independent vectors is a basis, and every vector can be represented as such:
$$\mathbb{F}\mapsto\mathbb{F}^n : v \mapsto (v_1, ..., v_n)$$

Consider $M_{m,n} (\mathbb{F})$ elementary matrices, where $E_{i,j}$ has 0 in all of it's entries except for a 1 at position $(i,j)$.\\
\\
\underline{Basis for $M_{m,n} (\mathbb{F})$:} $$\bigcup\limits_{j=1}^{n} \bigcup\limits_{i=1}^{m} E_{i,j}$$ Which is, if you consider a $M_{m,n} (\mathbb{F})$ matrix, a 1 in each one of that matrix's indices.\\
\\
For example: The Basis for $M_{2,2} (\mathbb{R})$ is $\{ 
\begin{bmatrix} 
1 & 0 \\
0 & 0 
\end{bmatrix},
\begin{bmatrix} 
0 & 0 \\
1 & 0 
\end{bmatrix},
\begin{bmatrix} 
0 & 1 \\
0 & 0 
\end{bmatrix},
\begin{bmatrix} 
0 & 0 \\
0 & 1 
\end{bmatrix}
 \}$
 
\newpage

\section{Wednesday, May 17, 2017 (Section 3.2)}

From this point, I will be explaining Lecture topics, but based off the textbook, as I have a hard time following the Lecture Notes.

\subsection{Basis (but in English)}

\begin{tcolorbox}[title=Basis for a Vector Space]
	Let $V$ be a vector space. A set of vectors in V is a basis for V if:
	(Let that set of vectors be $B$)
	\begin{itemize}
		\item{The set of vectors spans V. (i.e. span$(B) = V$)}
		\item{$B$ is linearly independent.}
	\end{itemize}
\end{tcolorbox}

\underline{Note:} Any linearly independent set of V can be extended to a basis of V.

\textbf{Example 1:} Find a basis in $P_3 (\mathbb{F})$ that contains $\{ (x^2 + 1),(x^2 - 1) \}$
\\
\\
\textbf{Solution 2:} Start with the set $B = \{ (x^2 + 1),(x^2 - 1) \}$\\
\\
We know that $\{1,x,x^2,x^3\}$ is a Basis for $P_3 (\mathbb{F})$, so we will combine these two sets. So now $B = \{ (x^2 + 1),(x^2 - 1),1,x,x^2,x^3 \}$, which is a spanning set of $P_3 (\mathbb{F})$\\
\\
Now we have to remove items from B until B is a linearly independent set.\\
\\
since $1 = \frac{1}{2}((x^2 + 1) - (x^2 - 1))$, we must remove 1.\\
since $x^2 =  \frac{1}{2}((x^2 + 1) + (x^2 - 1))$, we must remove $x^2$.\\
\\
So now $B = \{ (x^2 + 1),(x^2 - 1),x,x^3 \}$, which is a spanning set of $P_3 (\mathbb{F})$ and is linearly independent.\\
\\
\underline{Note:} $B$ contains the same amount of elements as the dimension of $P_3 (\mathbb{F})$ (which is 4).\\

\begin{tcolorbox}
	Any spanning set contains a basis.
\end{tcolorbox}

\begin{proof}
	Consider the spanning set $S$ of vector space $V$, with the dependent vectors $v_1,...,v_n$. (So $S \setminus \{v_1,...,v_n\}$ is linearly independent.)
	Now, considering $S$ and $S \setminus \{v_1,...,v_n\}$\\
	$$span(S \setminus \{v_1,...,v_n\}) \subseteq span(S)$$ as there are more elements in $S$ 	than in $S \setminus \{v_1,...,v_n\}$, but since for $v_1,...,v_n$ that each of these 	vectors can be written as a linear combination of all the elements in $S \setminus \{v_1,...,v_n\}$, then that means
	$$span(S \setminus \{v_1,...,v_n\}) \supseteq span(S)$$
	so this means that
	$$span(S \setminus \{v_1,...,v_n\}) = span(S)$$
	and since $S \setminus \{v_1,...,v_n\}$ is linearly independent, then it is a basis.
	
	Therefore, any spanning set S of vector space V contains a basis.
\end{proof}

\textbf{Example 2:} Consider $V = span\{[1,2,3],[2,1,1] \}$ and $W = span\{[1,0,1],[3,0,-1]\}$. Find a basis for $(V + W) = \{ v + w | v\in V, w \in W\}$
\\
\\
\textbf{Solution 2:} Let $U = V + W = span\{[1,2,3],[2,1,1],[1,0,1],[3,0,-1] \}$\\
\\
Since $2[2,1,1] - [1,2,3] = [3,0,-1]$, so $V + W = span\{[1,2,3],[2,1,1],[1,0,1]\}$ And we will put the remaining vectors in a matrix as column vectors and use Gauss-Jordan to prove that they are linearly independent.

$$\begin{amatrix}{3}
1 & 2 & 1 & 0 \\
2 & 1 & 0 & 0 \\
3 & 1 & 1 & 0
\end{amatrix} \thicksim
\begin{amatrix}{3}
1 & 2 & 3 & 0 \\
0 & -3 & -5 & 0 \\
0 & 0 & -2 & 0
\end{amatrix}$$

Since this matrix is in reduced row echelon form with the number of pivots equal to the number of column vectors, this implies linear independence.

\begin{tcolorbox}[title=Unique Combination Criterion for a Basis]
	Let $B$ be a set of nonzero vectors in a vector space $V$. Then $B$ is a basis for $V$ if and only if each vector $v \in V$ can be uniquely expressed in the form
	$$\sum_i r_i b_i \text{ where } b_i \in B, r_i \in \mathbb{F}$$
\end{tcolorbox}

\begin{proof}
	Consider two vectors $v,w$ of vector space $V$ of dimension $k$. Let these two vectors have the coordinate vectors $v=[v_1, ..., v_k]$ and $w=[w_1, ..., w_k]$, and assume that they both have the same representation for the Basis $B=[b_1, ..., b_k]$, this implies that...
	\begin{align*}
		\sum_{i=1}^k w_ib_i &= \sum_{i=1}^k v_ib_i\\
		\sum_{i=1}^k w_ib_i - \sum_{i=1}^k v_ib_i &= 0\\
		\sum_{i=1}^k (w_i - v_i)b_i&= 0
	\end{align*}
And since $B$ is a basis, $B$ is also linearly independent, so that must mean that $w_i - v_i = 0$ for all $i, 1 \leq i \leq k$, so $w_i = v_i$. So therefore, every vector can be uniquely expressed. 
	
\end{proof}

\subsection{Dimension (but in English)}

\begin{tcolorbox}[title=Dimension]
	Let V be a finitely generated vector space. The number of elements in a basis for $V$ is the dimension of $V$. Denoted by $dim(V)$
\end{tcolorbox}

\textbf{Properties of Dimension} (If $dim(V) = d$)
\begin{enumerate}
	\item{Any spanning set of $V$ with $d$ elements is a basis}
	\item{Any linearly independent set of $d$ elements in $V$ is also a basis}
\end{enumerate}

\newpage

\section{Friday, May 19, 2017 (Section 3.3)}

\subsection{Coordinatization of Vectors}

\begin{tcolorbox}[title=Coordinate Vector of $V$ relative to the ordered basis $B$]
	Let $B = (b_1, b_2, ..., b_n)$	be an ordered basis for a finite-dimensional vector space $V$, and let
	$$v = r_1 b_1 + r_2 b_2 + ... + r_n b_n$$
	The vector $[r_1, r_2, ... , r_n]$ is the coordinate vector of $v$ relative to the ordered basis $B$, and is denoted by $v_B$
\end{tcolorbox}

\textbf{Example 1:} What is the coordinate vector of $v = -x + x^3 + 2x^4$ in the Bases $B = (1,x,x^2,x^3,x^4), E = (x^4, x^3, x^2, x, 1)$\\
\\
\textbf{Solution 1:} $v_B = [0, -1, 0 , 1, 2], v_E = [2,1,0,-1,0]$\\

\underline{Note:} For coordinate vectors, the bases that they are relative to must be ordered rather than just a set, since if just a set (unordered) is used, both $v_B$ and $v_E$ could represent either basis $B$ or $E$, which would be confusing. So it would be more useful for each vector to have a unique coordinate vector relating to it.\\
\\
\underline{i.e.} If $E = (x^4, x^3, x^2, x, 1)$ was just a set rather than a ordered basis, $v_E$ could be any permutation of $2,1,0,-1,0 ([2,1,0,-1,0], [2,1,0,0,-1],...)$\\
\\
\textbf{Example 2:} Find the coordinate vector of $[1,2,-2]$ relative to the ordered basis $B = ([1,1,1], [1,2,0], [1,0,1])$\\
\\
\textbf{Solution 2:} To find the coordinate vector representation of $[1,2,-2]$, we must express it as a linear combination of the items in $B$.

$$a[1,1,1] + b[1,2,0] + c[1,0,1] = [1,2,-2]$$

To solve for $a,b,c$ we can put these system of equations into a matrix.

$$
\begin{amatrix}{3}
1 & 1 & 1 & 1 \\
1 & 2 & 0 & 2 \\
1 & 0 & 1 & -2
\end{amatrix} 
\thicksim
\begin{amatrix}{3}
1 & 1 & 1 & 1 \\
0 & 1 & -1 & 1 \\
0 & -1 & 0 & -3
\end{amatrix} 
\thicksim
\begin{amatrix}{3}
1 & 0 & 2 & 0 \\
0 & 1 & -1 & 1 \\
0 & 0 & -1 & -2
\end{amatrix}
\thicksim
\begin{amatrix}{3}
1 & 0 & 0 & -4 \\
0 & 1 & 0 & 3 \\
0 & 0 & 1 & 2
\end{amatrix}
$$

So the coordinate vector for $[1,2,-2]$ with respect to $B$ is $[-4,3,2]$ $([1,2,-2]_B = [-4,3,2])$.\\
\\
We can do this method of taking each basis vector in an augmented matrix with the vector we are trying to find the coordinate vector representation of, for all finite bases and vectors within the span of those finite bases.
\subsection{Coordinatization of Finite-Dimensional Vector Spaces}

Coordinatization of a Finite-Dimensional Vector Space $V$ just means that there's gonna be a one-to-one relationship between the vectors in a Vector Space $V$ and their Coordinate Vector Representations, which means that just like vectors in $V$, the coordinate vectors mirror the same operations of scalar multiplication and vector addition. i.e:

$$(v + w)_B = v_B + w_B$$
$$(tv)_B = t(v)_B$$

For vectors $v,w \in V$ and $t \in \mathbb{F}$. This is generally easy to prove.

\begin{proof} Consider vector $v \in V$ where $v_B = [v_1, ... ,v_n]$ and vector $w \in V$ where $w_B = [w_1, ... ,w_n]$ and a scalar $t \in \mathbb{F}$\\
(Proof of closure under vector addition)
\begin{align*}
	v_B + w_B &= [v_1, ... ,v_n] + [w_1, ... ,w_n]\\
	&= [v_1 + w_1, ... ,v_n + w_n]\\
	&= (v + w)_B
\end{align*}

(Proof of closure under scalar multiplication)
\begin{align*}
	t(v_B) &= t[v_1, ... ,v_n]\\
	&= [t(v_1), ... ,t(v_n)]\\
	&= (tv)_B
\end{align*}

\end{proof}

This tells us that a vectors in a vector space $V$ can be renamed in their unique coordinate vector relative to a basis $B$ (that coordinate vector space would be $\mathbb{F}^n$) and this coordinate vector space $\mathbb{F}^n$ would be structurally identical to $V$. We say that $V$ and $\mathbb{F}^n$ are isomorphic vector spaces, or that every vector space $V$ with a basis of $n$ vectors is isomorphic to $\mathbb{F}^n$, as each vector in $V$ can be renamed to a vector in $\mathbb{F}^n$ relative to $B$. We use 'isomorphic' to describe algebraic structures that are identical except in name and elements.\\
\\
For $V$ to be isomorphic to $\mathbb{F}^n$, all of the algebraic properties of $V$ must also apply to vectors in $\mathbb{F}^n$. Since we know this about $V$ and $\mathbb{F}^n$, we can use coordinate vectors in $\mathbb{F}^n$ and vector properties to learn about properties of the corresponding vectors space $V$.\\
\\
\textbf{Example 2:} Determine whether $x^2 - 3x + 2, 3x^2 + 5x - 4, 7x^2 + 21x - 16$ are linearly independent in the vector space $P_2 (\mathbb{F})$\\
\\
\textbf{Solution 2:} We can just convert these to coordinate vectors and put them in a matrix and do row reduction to find out if they are linearly independent.\\
\\
Let's use basis $B = (x^2, x, 1)$\\
$$(x^2 - 3x + 2)_B = [1,-3,2]$$
$$(3x^2 + 5x - 4)_B = [3,5,-4]$$
$$(7x^2 + 21x - 16)_B = [7,21,-16]$$

And now let's put them in a matrix as column vectors and do row reduction.

$$\begin{bmatrix} 
1 & 3 & 7\\
-3 & 5 & 21\\
2 & -4 & -16 
\end{bmatrix}
\thicksim
\begin{bmatrix} 
1 & 3 & 7\\
0 & 14 & 42\\
0 & -10 & -30 
\end{bmatrix}
\thicksim
\begin{bmatrix} 
1 & 3 & 7\\
0 & 1 & 3\\
0 & 0 & 0 
\end{bmatrix}$$

Since there are 3 coordinate vectors, but only 2 pivots in the resulting matrix, we can conclude that 1 of the vectors is linearly dependent in the set. So that set of polynomials is linearly dependent.\\
\\
\underline{Note:} Since the pivots are in the positions of the vectors $(x^2 - 3x + 2)_B = [1,-3,2]$ and $(3x^2 + 5x - 4)_B = [3,5,-4]$, that means $\{ x^2 - 3x + 2, 3x^2 + 5x - 4 \}$ is linearly independent, so this is helpful for finding the basis of a Vector Space.

\textbf{Example 3:} Find the coordinate vector of $p(x) = x^3 + x^2 - x - 1$ relative to the ordered basis $A = ((x+1)^3, (x+1)^2, x+1, 1)$\\
\\
\textbf{Solution 3:} The plan here is to convert each vector in the basis $A$ into an coordinate vector relative to an easier basis to work with. Let's use the basis $B = (x^3, x^2, x, 1)$ for $P_3 (\mathbb{R})$.

$$((x+1)^3)_B = (x^3 + 3x^2 + 3x + 1)_B = [1,3,3,1]$$
$$((x+1)^2)_B = (x^2 + 2x + 1)_B = [0,1,2,1]$$
$$(x+1)_B = [0,0,1,1]$$
$$(1)_B = [0,0,0,1]$$
\\
$$(x^3 + x^2 - x - 1)_B = [1,1,-1,-1]$$

Now if we put the coordinate vectors of the basis as column vectors into a matrix and augment the vector we are trying to solve for ($p(x)$), we get.

$$
\begin{amatrix}{4}
1 & 0 & 0 & 0 & 1\\
3 & 1 & 0 & 0 & 1\\
3 & 2 & 1 & 0 & -1\\
1 & 1 & 1 & 1 & -1
\end{amatrix}
\thicksim
\begin{amatrix}{4}
1 & 0 & 0 & 0 & 1\\
0 & 1 & 0 & 0 & -2\\
0 & 2 & 1 & 0 & -4\\
0 & 1 & 1 & 1 & -2
\end{amatrix}
\thicksim
\begin{amatrix}{4}
1 & 0 & 0 & 0 & 1\\
0 & 1 & 0 & 0 & -2\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0
\end{amatrix}
$$

Therefore $p(x)_B = [1,-2,0,0]$ and so...
$$x^3 + x^2 - x - 1 = (x+1)^3 - 2(x+1)^2$$

\newpage

\end{document}