\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{graphicx}
\graphicspath{ {imgs/} }

\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage{textcomp}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}

\begin{document}

\title{CSCC11 Anki}
\maketitle

\section{LEC 1}

\paragraph{What 2 phases are ML methods broken into?} Training Phase and Application Phase
\paragraph{Training Phase} A model is learned from a collection of training data
\paragraph{Application Phase} the model is used to make decisions about some new test data

\paragraph{What are the different types of Machine Learning?} Supervised Learning, Unsupervised Learning, Reinforcement Learning
\paragraph{Supervised Learning} The training data is labelled with the correct answers
\paragraph{What are the types of Supervised Learning?} Classification and Regression
\paragraph{Regression} Supervised Learning where outputs are real-valued
\paragraph{Classification} Supervised Learning where outputs are discrete labels
\paragraph{Unsupervised Learning} Deriving patterns and structure from unlabelled data
\paragraph{Reinforcement Learning} An agent seeks to learn optimal actions to take based on the state of the world, and learn from the consequences of past actions.

\paragraph{What are the questions we must ask ourselves when using Machine Learning?}

\begin{itemize}
  \item How do we parameterize the curve?
  \item How do we determine fitness?
  \item How much time do we have?
  \item How do we prevent overfitting?
\end{itemize}

\paragraph{Overfitting} When a model fits the training data well, but performs poorly on test data

\paragraph{Noise} {Properties of the input that are not relevant to the task at hand}

\section{LEC 2}

\paragraph{Least-Squares Regression} summing up the squared errors $E(w,b) = \sum^N_{i=1} e_i^2$

\paragraph{Linear Regression} Mapping 1-D input to 1-D output $y = f(x) = wx + b$

\paragraph{Multi-Dimensional Inputs} Mapping D-D inputs to 1-D output $f(x) = w^T x + b = \sum^D_{j=1} w_j x_j + b$

\paragraph{Folding the bias $b$ in Multi-Dimensional Inputs} $\tilde{w} = \begin{bmatrix}
   w_{1} \\
   \vdots \\
   w_{D}\\
   b
 \end{bmatrix}, \tilde{x} = \begin{bmatrix}
   x_{1} \\
   \vdots \\
   x_{D} \\
   1
 \end{bmatrix}, f(x) = \tilde{w}^T\tilde{x}$

\paragraph{pseudoinverse of $\tilde{X}$}{$\tilde{X}^+ \equiv (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T$}

\paragraph{Multi-dimensional Outputs}{Mapping $D$-D inputs to $K$-D outputs $y = W^T x, y\in\mathbb{R}^K, W \in\mathbb{R}^{(D+1) \times K}$}

\paragraph{Euclidean Norm of $v$}{$||v|| = \sqrt{\sum_i v_i^2}$}

\paragraph{Frobenius Norm of $Y$}{$||Y||_F = \sqrt{\sum_{i,j} Y^2_{i,j}}$}

\end{document}