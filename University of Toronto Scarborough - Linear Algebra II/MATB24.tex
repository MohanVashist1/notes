\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{dsfont}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage[english]{babel}

\usepackage{tikz}

\newcommand{\ts}{\textsuperscript}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Linear Algebra II -- Summer 2017}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}


\begin{document}

\title{MATB24 Lecture Notes}
\date{University of Toronto Scarborough -- Summer 2017}
\author{Joshua Concon}
\maketitle

Pre-reqs are MATA23, which is Linear Algebra I.
Instructor is Dr. Louis de Thanhoffer de Volcsey. I highly recommend sitting at the front since he likes to teach with the blackboards. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{Wednesday, May 3, 2017}

Before he got to the definition of fields, Dr. Louis de Thanhoffer de Volcsey talked a lot about how MATB24 was simply going to be a generalization of everything in MATA23 and various specific examples of fields as well as practical applications, but since fields were never defined, I will omit all of that stuff since it does not really make any sense without the formal definition of fields.\\
\\
In the definition of the field, the lecturer presents the unique additive identity as $0$, the unique additive inverse as $(-a)$ for $a$, the unique multiplicative identity as $1$, and the unique multiplicative inverse of $a$ as $(a^{-1})$. Although this is true for fields like $\mathbb{R}$, this is misleading, as the inverses and identities are supposed to be general, and thinking of the additive inverse for example as $(-a)$ is not necessarily true for some fields.\\
\\
Take for example the field $\mathbb{Z}_2 = \{ 0,1 \}$ (This is essentially all of $\mathbb{Z}$ but each element $a$ becomes the remainder of $a/2$, so $0,1,2,3,4,5$ becomes $0,1,0,1,0,1$).\\
\\
If you consider the case when $a = 1$, the additive inverse of 1 is 1 (\underline{i.e.} $1 + 1 = 0$). So using the definition of $(-a)$ as the additive inverse in this case is confusing since $(-a) = a = 1$. So in the definition of a field, these are all generalized.

\subsection{Fields}

\begin{tcolorbox}[title=Fields]
	A field ($\mathbb{F}$) is a set with two operations:
	\begin{itemize}
		\item{$+:\mathbb{F} \times \mathbb{F} \longrightarrow \mathbb{F}$}
		\item{$\circ:\mathbb{F} \times \mathbb{F} \longrightarrow \mathbb{F}$}
	\end{itemize}
	And these two operations must satisfy all of the following axioms:\\
	(Axioms are for $\forall a,b,c \in \mathbb{F}$)\\
	\textbf{Axioms for $+$:}
	\begin{itemize}
		\item{\underline{associative:} $(a + b) + c = a + (b + c)$}
		\item{\underline{commutative:} $a + b = b + a$}
		\item{\underline{(unique) additive identity:} $\exists j \in \mathbb{F} : j + a = a$}
		\item{\underline{(unique) additive inverse:} $\exists k \in \mathbb{F} : k + a = j$ where $j$ is the unique additive identity}
	\end{itemize}
	\textbf{Axioms for $\circ$:}
	\begin{itemize}
		\item{\underline{associative:} $(a \circ b) \circ c = a \circ (b \circ c)$}
		\item{\underline{commutative:} $a \circ b = b \circ a$}
		\item{\underline{(unique) multiplicative identity:} $\exists q \in \mathbb{F} : q \circ a = a$}
		\item{\underline{(unique) multiplicative inverse:} $\exists w \in \mathbb{F}\setminus\{ j \} : w \circ a = q$ where $q$ is the unique multiplicative identity and $j$ is the unique additive identity}
	\end{itemize}
	\textbf{Axioms for both:}
	\begin{itemize}
		\item{\underline{distributive:} $a \circ (b + c) = (a \circ b) + (c \circ a)$}
	\end{itemize}
\end{tcolorbox}

\textbf{Example 1:} Is $\mathbb{Z}$ a field?\\
\\
\textbf{Solution 1:} No, since $2 \in\mathbb{Z}$ has no multiplicative inverse in $\mathbb{Z}$\\ (\underline{i.e.} $\nexists a \in\mathbb{Z} : 2a = 1$), however, $\mathbb{Q}$ is a field and solves this problem.

\section{Friday, May 5, 2017}

\subsection{Vector Spaces}

Vector spaces are, in the paraphrased words of the lecturer, "simply the generalization of the real-valued vectors from MATA23". So they are a collection of elements where each element is in a field $(\mathbb{F})$.\\
\\
The following definition of Vector Spaces is also slightly inconsistent when compared to the lecture and the textbook definition to be more general for reasons provided before (for the definition of a field). Like a single mother who is pulled into a conversation with her child about the missing paternal figure, I would prefer to just tell a more truthful definition now that may be more abstract and slightly harder to understand in order to be more consistent, rather than clearing up disinformation later.

\begin{tcolorbox}[title=Vector Spaces]
A Vector Space over the field $\mathbb{F}$ is a set $V$ with the following operations:
\begin{itemize}
	\item{$+:V \times V \longrightarrow V$ (vector addition)}
	\item{$\circ:\mathbb{F} \times V \longrightarrow V$ (scalar multiplication)}
\end{itemize}
And these two operations must satisfy all of the following axioms:\\
(axioms are for $\forall v,w,u \in V$ and $\forall a,b \in\mathbb{F}$)\\
\textbf{Axioms for $+$:}
\begin{itemize}
	\item{\underline{associative:} $(v + w) + u = v + (w + u)$}
	\item{\underline{commutative:} $v + w = w + v$}
	\item{\underline{existence of the zero vector:} $\exists n \in V : n + v = v$ ($n$ must also be unique)}
	\item{\underline{existence of the inverse vector:} $\exists b \in V : b + v = n$ where $n$ is the zero vector and $b$ is unique for each vector $v$.}
\end{itemize}
\textbf{Axioms for $\circ$:}
\begin{itemize}
	\item{\underline{associative:} $(ab) \circ v = a \circ (b \circ v)$}
	\item{\underline{preservation of scale:} $\exists j \in\mathbb{F} : j \circ v = v$ ($j $ is usually 1 here)}
\end{itemize}
\textbf{Axioms for both:}
\begin{itemize}
	\item{\underline{distributive:} $(a + b) \circ v = (a \circ v) + (b \circ v)$\\
	$a \circ (v + w) = (a \circ v) + (a \circ w)$}
\end{itemize}
\end{tcolorbox}

\textbf{Example 1:} Prove the distributive axiom for $\mathbb{F}^n$ which is a vector space over $\mathbb{F}$ with operations $(+, \circ)$ from the real numbers $(\mathbb{R})$ and:
\begin{itemize}
	\item{$v = (v_1, ... ,  v_n) \in\mathbb{F}^n$}
	\item{$w = (w_1, ... ,  w_n) \in\mathbb{F}^n$}
	\item{$a,b \in\mathbb{F}$}
\end{itemize}

\textbf{Solution 1:} Since we are using the $(+, \circ)$ operations from the real numbers, then we know that...
\begin{itemize}
	\item{$v+w = (v_1 + w_1, ... ,  v_n + w_n)$}
	\item{$a \circ v = (a \circ v_1, ... , a \circ v_n)$}
\end{itemize}
Consider $a \circ (v + w)$
\begin{align*}
a \circ (v + w) &= a \circ (v_1 + w_1, ... ,  v_n + w_n)\\
&= (a \circ (v_1 + w_1), ... ,a \circ (v_n + w_n))\\
&= ((a \circ v_1) + (a \circ w_1), ... , (a \circ v_n) + (a \circ w_n))\\
&= ((a \circ v_1) + ... + (a \circ v_n)) + ((a \circ w_1) + ... + (a \circ w_n))\\
&= (a \circ (v_1, ... ,  v_n)) + (a \circ (w_1, ... ,  w_n))\\
&= (a \circ v) + (a \circ w)
\end{align*}
Consider $(a + b) \circ v$
\begin{align*}
	(a + b) \circ v &= (v_1, ... ,  v_n)\\
	&= (((a + b) \circ v_1), ... ,  ((a + b) \circ v_n))\\
	&= ((a \circ v_1) + (b \circ v_1), ... ,  (a \circ v_n) + (b \circ v_n))\\
	&= ((a \circ v_1), ... ,  (a \circ v_n)) + ((b \circ v_1), ... ,  (b \circ v_n))\\
	&= (a \circ (v_1, ... ,  v_n)) + (b \circ (v_1, ... ,  v_n))\\
	&= (a \circ v) + (b \circ v)
\end{align*}

\textbf{Example 2:} $\mathbb{M}_{k,l}(\mathbb{F})$ which is all $k \times l$ matrices with coefficients in $\mathbb{F}$.\\
This is also field, for example:\\
($\forall M,N \in \mathbb{M}_{k,l}(\mathbb{F})$, $\forall a \in\mathbb{F}$ and $\forall i,j : 1 \leq i \leq k, 1 \leq j \leq l$)

\begin{itemize}
	\item{$M + N = (M + N)$ where $(M+N)_{ij} = M_{ij} + N_{ij}$ (ij\ts{th} entry addition)}
	\item{0 is the zero matrix where $0_{ij} = 0$}
	\item{$a \circ M = (aM)$ where $(aM)_{ij} = a (M_{ij})$ (scalar multiplication)}
	\item{$M + (-M) = 0$ where $(-M)_{ij} = -(M_{ij})$ (existence of the inverse vector)}
\end{itemize}

\textbf{Example 3:} $\mathbb{M}_{k,l}^{1}(\mathbb{F})$ (all $k \times l$ matrices with coefficients in $\mathbb{F}$ and $\forall M \in \mathbb{M}_{k,l}^{1}(\mathbb{F})$, $M_{1,1} = 1$)\\
\\
This is not a Vector Space because scalar multiplication does not work if the scalar being multiplied by is not 1. (\underline{i.e.} if $a \neq 1$ then $a(M_{1,1}) = a \neq 1$)\\
\\
\textbf{Example 4:} $\mathbb{M}_{k,l}^{0}(\mathbb{F})$ however, is a vector space, since\\ $\forall a \in\mathbb{F}$ and $\forall M \in \mathbb{M}_{k,l}^{0}(\mathbb{F})$, that $a(M_{1,1}) = a \cdot 0 = 0$.\\
\\
\textbf{Example 5:} $P(\mathbb{F})$ which is all the polynomials over the field $\mathbb{F}$, defined by:
$$P(\mathbb{F}) = \langle \sum_{i=1}^{n} {\alpha_i \cdot x^i} \rangle$$
This is also a Vector Space, here are some examples of how the operations work.
\begin{itemize}
	\item{$\sum_{i=1}^{n} {\alpha_i \cdot x^i} + \sum_{i=1}^{n} {\beta_i \cdot x^i} = \sum_{i=1}^{n} {(\alpha_i + \beta_i) \cdot x^i}$ (vector addition)}
	\item{$\gamma\sum_{i=1}^{n} {\alpha_i \cdot x^i} = \sum_{i=1}^{n} {(\gamma\alpha_i) \cdot x^i}$ (scalar multiplication)}
\end{itemize}

\textbf{Example 6:} Take any set $X$, $V$ is a vector space over $\mathbb{F}$\\
$V^X = \{ functions : X \longrightarrow V \}$ is a vector space over $\mathbb{F}$\\
Consider $f,g \in V^X$, $\forall \alpha \in \mathbb{F}$
$$(f + g)(x) = f(x) + g(x)$$

\newpage

\section{Wednesday, May 10, 2017}

\subsection{Basics of Vector Spaces}

\textbf{Basic Example of a Vector Space:} $\mathbb{F}^n$

\begin{tcolorbox}[title=Definition: Subspace]
	A subspace $W$ of a vector space $V$ is a non-empty subset of $V$ with the following axioms:
	\begin{itemize}
		\item{$\forall w_1, w_2 \in W : w_1 + w_2 \in W$}
		\item{$\forall \alpha \in \mathbb{F}, w \in W : \alpha \cdot w \in W$}
	\end{itemize}
\end{tcolorbox}

\textbf{Example 1:} Is $P(\mathbb{F}) = \{ \sum_{i=0}^{\infty} \alpha_i \cdot x^i : \alpha_i \in \mathbb{F} \}$ a subspace?\\

\textbf{Solution 1:} $0 \in P(\mathbb{F})$, therefore $P(\mathbb{F})$ is non-empty.\\
\\
Consider $\forall w, v \in P(\mathbb{F})$ and $\forall r \in\mathbb{F}$\\
where $w = \sum_{i=0}^{\infty} w_i \cdot x^i$ and $v = \sum_{i=0}^{\infty} v_i \cdot x^i$
$$w+w = \sum_{i=0}^{\infty} w_i \cdot x^i + \sum_{i=0}^{\infty} v_i \cdot x^i = \sum_{i=0}^{\infty} (w_i + v_i) \cdot x^i \in P(\mathbb{F})$$
$$rw = r\sum_{i=0}^{\infty} w_i \cdot x^i = \sum_{i=0}^{\infty} (r\cdot w_i) \cdot x^i \in P(\mathbb{F})$$
Therefore $P(\mathbb{F})$ is a subspace\\
\\
\textbf{Example 2:} Is $P_n (\mathbb{F}) = \{ \sum_{i=0}^{\infty} \alpha_i \cdot x^i : \alpha_i \in \mathbb{F}, \text{ degree} = n \}$ a subspace?\\

\textbf{Solution 2:} Consider $1 + 2x + 3x^2, -3x^2 \in P_2 (\mathbb{F})$
$$1 + 2x + 3x^2 + (-3x^2) = 1 + 2x \text{ which is not in } P_2 (\mathbb{F})$$
Therefore $P_n (\mathbb{F})$ is not a vector space.\\
\\
\underline{Note:} The easiest way to check if something ($V$) is not a vector space is to check if $0 \in V$.\\
\\
\textbf{Example 3:} Are all matrices $A$ with $det(A) = 0$\\
\\
\textbf{Solution 3:} Consider $UT_n (\mathbb{F}) = \{ M \in M_{m,n} (\mathbb{F}) | M_{ij} = 0, i \geq j \}$\\
Take $M_1, M_2 \in UT_n (\mathbb{F})$\\
$$(M_1 + M_2)_{ij} = M_{1ij} + M_{2ij} = 0 + 0 = 0, \forall i,j$$
$$\forall \alpha \in \mathbb{F} : (\alpha M)_{ij} = \alpha (M_{ij}) = \alpha \cdot 0 = 0$$
Therefore $UT_n (\mathbb{F})$ is a subspace.

\subsection{Operations on Subspaces}
Considering subspaces $U,W$ of a vector space $V$
\begin{itemize}
	\item{$(U \cap W) = \{ v \in V : v\in U, v \in W \}$ is a subspace}
	\item{$(U + W) = \{ u+w : u \in U, w \in W \}$ is also a subspace}
	\item{However, $(U \cup W)$ is not a subspace because of the following example:\\
	Consider in $P(\mathbb{F})$ the subspaces $U = \{ \alpha x : \alpha \in \mathbb{F} \}$ and $W = \{ \alpha x^2 : \alpha \in \mathbb{F} \}$} and $U \cup W$ is not a subspace since it does not contain $x + x^2$, and so is not closed under vector addition.
\end{itemize}

\textbf{Recall:} if $v_1, ... , v_n$ is a basis, then all vectors are of the form $\sum_i \alpha_i v_i$
\begin{itemize}
\item{$sp\{v_1, ... , v_n\} = \{ \sum_i \alpha_i v_i : \alpha_i \in \mathbb{F} \}$}
\end{itemize}

\textbf{Result:} $sp(v_1, ... , v_n)$ is the smallest subspace containing $\{v_1, ... , v_n \}$\\
\begin{proof}
	(Proof of Result)\\
	Take $u,w \in sp(v_1, ... , v_n)$, so $u = \sum_i \alpha_i v_i, w = \sum_i \beta_i v_i$\\ so this implies that $u+w = \sum_i \alpha_i v_i + \sum_i \beta_i v_i = \sum_i (\beta_i + \alpha_i) v_i$.\\
	And Taking a $\gamma \in \mathbb{F} : \gamma u = \gamma \sum_i \alpha_i v_i = \sum_i (\alpha_i \cdot \gamma) v_i$\\
	So now we assume $W$ is the subspace containing $\{v_1, ... , v_n \}$\\
	$$\longrightarrow \forall \alpha \in \mathbb{F}, \alpha_i x_i \in W $$
	$$\longrightarrow sp\{v_1, ... , v_n \} \in W$$
	so $sp\{v_1, ... , v_n \}$ is the smallest subspace containing $\{v_1, ... , v_n \}$ since for every subspace $W$ containing $\{v_1, ... , v_n \}$, they must also contain $sp\{v_1, ... , v_n \}$
\end{proof}

\textbf{Example 4:} $sp\{1, 1+x, x^2 -1 \} \in P(\mathbb{F})$ ?\\
\\
\textbf{Solution 4:} A good war to check if a set is a subspace is to write the subspace as a span.\\
\begin{align*}
	sp\{1, 1+x, x^2 -1 \} &= a + b(1 + x) + c(x^2 - 1) | a,b,c \in\mathbb{F}\\
	&= (a+b-c) + bx + cx^2 | a,b,c \in\mathbb{F}\\
	&= P_{\leq 2} (\mathbb{F}) \in P(\mathbb{F})
\end{align*}

\textbf{Example 5:} $sp\{e_1, e_2 \} = sp\{(1,0), (0,1) \} \in M_{m,n} (\mathbb{F})$
$$E_{ij} \text{ has a coefficient of 0 everywhere except for a coefficient of 1 in entry } (i,j)$$

\textbf{Note:} $$\left( \begin{smallmatrix} a&b\\ c&d \end{smallmatrix} \right) = aE_{11} + bE_{12} + cE_{21} dE_{22}$$

\textbf{Example 5:}
\begin{align*}
	sp\{v_1, v_2, v_1 + v_2 \} &= av_1 + bv_2 + c(v_1 + v_2) |a,b,c \in\mathbb{F}\\
	&= (a+c)v_1 + (b+c)v_2 |a,b,c \in\mathbb{F}\\
	&= sp\{v_1, v_2 \}
\end{align*}
So when do we know that we have the minimum amount of terms to generate a Vector Space from a span of these terms?

\begin{tcolorbox}[title=Definition: Linearly Dependent]
	A vector $V$ is \textbf{linearly dependent} of vectors $\{v_1, ... , v_n \}$ if it is a linear combination of them.
\end{tcolorbox}

So if $v = (v_1, ... , v_n)$ is linearly dependent of $\{u_1, ... , u_n \}$ then $$sp\{v_1, ... , v_n \} = sp\{u_1, ... , u_n \}$$

A set of vectors is linearly independent if no vector is linearly dependent on the other vectors in the set.\\
\\
\textbf{Fact:} a set is linearly independent if $\sum_i a_i v_i = 0 \longrightarrow \forall a_i = 0$

\begin{proof}
	Assume $\sum_i a_i v_i = 0 \longrightarrow \forall a_i = 0$ and that some vector
	$$v_i = \sum_{j \neq i} a_j v_j, u_0 = \sum_{j = 0} a_j v_j$$
	$$|v_0 - a_1 v_1 - a_2 v_2 - ... - a_n v_n| = 0$$
	Assume that $\sum_i a_i v_i = 0$ (linearly independence)\\
	Assume that for some coefficient is not 0, say $a_1 \neq 0$.\\
	$$a_1 v_1 + \sum_{i\geq 2} a_i v_i = 0, v_1 = \sum_{i\geq 2} (\frac{-a_i}{a_1}) v_i$$
	This shows that $v_1$ is a linear combination of the other vectors, which violates our assumption that all these vectors $v_i$ are linearly independent. So it must be the case that $\sum_i a_i v_i = 0, \forall a_i = 0$ implies linear independence
\end{proof}

\newpage

\section{Friday, May 12, 2017}

\subsection{Bases of Vector Spaces}

\begin{tcolorbox}[title=Definition: A Basis of Vector Spaces]
	A basis for $V$ is a spanning set that is linearly independent
\end{tcolorbox}

\textbf{Result:} A spanning set $\{v_1, ..., v_n \}$ for $V$ is a basis $\Longleftrightarrow \forall v, \exists a_i, v = \sum^n_{i = 1} a_i v_i$

\begin{proof}(Proof of Result)\\
	\underline{Proof of $\longrightarrow$}\\
	$\forall v, \exists a_i, v = \sum^n_{i = 1} a_i v_i$, so since every vector $v$ can be made with the vectors $\{v_1, ..., v_n \}$ 
\end{proof}



\end{document}