\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{ esint }

\usepackage{amsthm}

\usepackage{graphicx}

\graphicspath{ {imgs/} }

\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage{textcomp}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Multivariable Calculus II -- Winter 2018}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\theoremstyle{plain}

\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}

\begin{document}

\title{MATB42: Multivariable Calculus II\\ Lecture Notes}
\date{University of Toronto Scarborough -- Winter 2018}
\author{Joshua Concon}
\maketitle
Pre-reqs are MATB41. Instructor is Eric Moore. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{Friday, January 5, 2018}

\subsection{Fourier Expansions}

In this section, we will focus on single variable calculus, (so where $f:\mathbb{R} \mapsto \mathbb{R}$)\\
\\
Let us say that we have a function $f(x)$ and we want to approximate it. We can use an $n$th degree Taylor Polynomial, but this requires that $f(x)$ has at least $n$ derivatives at some point $x_0$ and the $k$th derivative of $f$ ($f^{(k)} (x)$) is determined by properties of $f$ in some neighbourhood of $x_0$, but what about outside this neighbourhood? How can we be certain of the approximation outside of this neighbour.\\
\\
Our problem here is that Taylor Polynomial may only approximate "near" $x_0$\\
\\
Now, consider the following function:

$$\Delta (x) = \begin{cases}
1, &\lfloor x \rfloor < x, \lfloor x \rfloor\text{ is odd} \\
0, &\lfloor x \rfloor < x, \lfloor x \rfloor\text{ is even}\\
\end{cases}
$$

In this function, Tayler returns either 0 or 1 depending on your choice of $x_0$ and cannot work for an $x_0 = p \in \mathbb{Z}$. Therefore Taylor polynomials cannot reflect the true nature of this function. Taylor provides a "local" approximation, but we want a "global" approximation. We need an approximation that is more precise over an interval at the cost of being not as precise as precise at any particular $x_0$.\\
\\
Note that the example function is \textbf{periodic}.\\
\\

\begin{definition}
	A function $y=f(x)$ such that $f(x)=f(x+p), p \neq 0, \forall x$ is said to be \textbf{periodic} of period $p$
\end{definition}

\begin{example}
	The periodic function $\Delta (x)$ is of period 2.
\end{example}

What we want is a global approximation of a periodic function, and the Fourier Approximation will be periodic, so we can use it for exactly that.

\begin{definition}
	A \textbf{trigonometric polynomial of degree $N$} is an expression of the form
	$$\frac{a_0}{2}+ \sum^N_{k-1} a_k cos(kx) + b_k sin(kx)$$ where the $a_i, b_i$ are constants.
\end{definition}

We know that $sin(x)$ and $cos(x)$ are the simplest periodic functions and repeat in intervals of $2\pi$, so $cos(kx)$ and $sin(kx)$ have period $\frac{2\pi}{k}$, but the smallest shared period is $2\pi$. If a trigonometric polynomial has period $2\pi$ and $f(x)$ has period $p$, then we must set $x=\frac{pt}{2\pi}$ to fix the period (where $t$ is a variable).\\
\\
So to approximate $y=f(x)$ by $F_N (x)$ for some $N$, we use the following equation:

$$F_N (x) = \frac{a_0}{2}+ \sum^N_{k=1} a_k cos(kx) + b_k sin(kx)$$

Now we need to choose the $a_k, b_k$. We can define it in the following way:

$$a_0 = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) dx$$
$$a_k = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) cos(kx) dx, k=1,2,3,...$$
$$b_k = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) sin(kx) dx, k=1,2,3,...$$

When defined in this way, $a_i, b_i$ are called the \textbf{Fourier Coefficients} of $f$ over the interval $[-\pi, \pi]$ and we call $F_N (x)$ the \textbf{Fourier Polynomial of degree $N$}.\\
\\
So why do we add the $\frac{a_0}{2}$? It is the average value of $f$ over $[-\pi, \pi]$.

\begin{note}
	sometimes you will see $a_0$ used instead of $\frac{a_0}{2}$ in the Fourier polynomial where $$a_0 = \frac{1}{2\pi} \int^{\pi}_{-\pi} f(x) dx$$
\end{note}

\begin{example}
	Consider $f(x)=\frac{-x}{2}$ over $[-\pi, \pi]$. Use Fourier Approximation.\\
	\\
	$$a_k = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) cos(kx) dx = \frac{1}{\pi} \int^{\pi}_{-\pi} (-\frac{x}{2}) cos(kx) dx \overset{odd}{=} 0$$
	$$a_0 = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) dx = \frac{1}{\pi} \int^{\pi}_{-\pi} (-\frac{x}{2}) dx \overset{odd}{=} 0$$
	\begin{align*}
		b_k &= \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) sin(kx) dx\\
		&= \frac{1}{\pi} \int^{\pi}_{-\pi} (-\frac{x}{2}) sin(kx) dx\\
		&\overset{even}{=} - \frac{1}{\pi} \int^{\pi}_{-\pi}  x sin(kx) dx\\
		&\underset{u=x, dv=sin(kx)dx}{\overset{even}{=}} - \frac{1}{\pi} [-\frac{1}{k} x cos(kx) + \frac{1}{k^2} sin(kx)]^\pi_0\\
		&= \frac{1}{\pi k} [\pi cos(k\pi)]\\
		&= \frac{1}{k} cos(k\pi)\\
		&= \frac{(-1)^k}{k}
	\end{align*}

	Thus we have:
	$$F_N (x) = -sin(x) + \frac{1}{2}sin(2x) - \frac{1}{3}sin(3x) + \frac{1}{4}sin(4x) + ...$$
	$$F_1 (x) = -sin(x)$$
	$$F_2 (x) = -sin(x) + \frac{1}{2}sin(2x)$$
	$$F_3 (x) = -sin(x) + \frac{1}{2}sin(2x) - \frac{1}{3}sin(3x)$$
	$$...$$
	And so on.

\end{example}

\newpage

\section{Monday, January 8, 2018}

continuing from the last lecture...

\begin{example}
    (continued from example 1.4)\\
    $f(x) = \frac{-x}{2}$\\
    $F_N (x) = -sin(x) + \frac{1}{2}sin(2x) - \frac{1}{3}sin(3x) + \frac{1}{4}sin(4x) + ...$\\
    \\
    This can be extended to a Fourier Series:
    $$F_N (x) = \sum^\infty_{k=1} (-1)^k \frac{sin(kx)}{k}$$
\end{example}

\begin{definition}
    For $f:\mathbb{R}\mapsto\mathbb{R}$, the Fourier Series for $f$ is
    $$F(x) = \frac{a_0}{2}+ \sum^\infty_{k=1} a_k cos(kx) + b_k sin(kx)$$
    where $a_i, b_i$ are Fourier coefficients.
\end{definition}

The $N$th degree Fourier Polynomial can be regarded as the $N$th partial sum of the series.\\
\\
We haven't talked about convergence yet, but for now, we will assume the series converges $(f(x)=F(x))$

\begin{definition}
    Function $a_k cos(kx) + b_k sin(kx)$ is the $k$th harmonic of $f$. The Fourier Series expresses $f$ in terms of its harmonics.
\end{definition}

\begin{note}
    (Looking at Harmonics in a Musical Sense):\\
    the $1$st harmonic is the fundamental harmonic of $f$ (the fundamental tone).\\
    The $2$nd harmonic is the first overtone.
\end{note}

(completely rewrite this amplitude section)

\begin{definition}
    The amplitude of the $k$th harmonic is
    $$A_k = \sqrt{(a_k)^2 + (b_k)^2}$$
    And note that
    $$a_k = A_k sin\alpha, b_k = A_k cos\alpha$$
\end{definition}

\begin{definition}
    The energy $E$ of a periodic function $f$ of period $2\pi$ is
    $$E = \frac{1}{\pi} \int^\pi_{-\pi} [f(x)]^2 dx$$
\end{definition}

So the energy of the $k$th harmonic is
$$E = \frac{1}{\pi} \int^\pi_{-\pi} [a_k cos(kx) + b_k sin(kx)]^2 dx = (a_k)^2 + (b_k)^2 = (A_k)^2$$

And the energy of the constant term is

$$\frac{1}{\pi} \int^\pi_{-\pi} [a_0]^2 dx = 2(a_0)^2$$
So we put $A_0 = \frac{1}{\sqrt{2}} a_0$.\\
\\
For a "nice" periodic function, we have the following equation:

$$E = A_0^2 + A_1^2 + A_2^2 + ...$$
This is known as the Energy Theorem, and comes from the study of periodic waves.\\
\\
We can draw a graph of this as $A_k^2$ against $k$ (This graph is known as the Energy Spectrum of $f$). It shows how the energy of $f$ is distributed among its harmonics.\\
\\
\begin{note}
    Notice that
    $$E = \frac{1}{\pi} \int^\pi_{-\pi} [f(x)]^2 dx = \frac{a_0^2}{2}+ \sum^\infty_{k=1} (a_k^2 + b_k^2) \text{ Parseval's Equation}$$
\end{note}

Assume a function $f$ of period $2\pi$ is the sum of a trigonometric series

$$f(x) = \frac{a_0}{2}+ \sum^\infty_{k=1} (a_k cos(kx) + b_k sin(kx)) \text{ on the interval } [-\pi,\pi]$$

Multiply by $cos(mx)$ and integrate to get
\begin{align*}
    \int^\pi_{-\pi} f(x)cos(mx) dx &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx + \int^\pi_{-\pi} [\sum^\infty_{k=1} (a_k cos(kx) + b_k sin(kx))]dx\\
    &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx +  \sum^\infty_{k=1} (a_k \int^\pi_{-\pi}cos(kx)dx + b_k \int^\pi_{-\pi}sin(kx)dx)
\end{align*}

\begin{note}
    Recall the following trigonometric identities:
    \begin{enumerate}
        \item $cosAcosB = \frac{1}{2} [cos(A+B) + cos(A-B)]$
        \item $cosAsinB = \frac{1}{2} [sin(A+B) + sin(A-B)]$
        \item $sinAsinB = \frac{1}{2} [cos(A-B) - cos(A+B)]$
    \end{enumerate}
\end{note}

\newpage

\section{Friday, January 12, 2018}

Continuing from where we left off.

\begin{align*}
    \int^\pi_{-\pi} f(x)cos(mx) dx &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx + \int^\pi_{-\pi} [\sum^\infty_{k=1} (a_k cos(kx) + b_k sin(kx))]dx\\
    &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx +  \sum^\infty_{k=1} (a_k \int^\pi_{-\pi}cos(kx)dx + b_k \int^\pi_{-\pi}sin(kx)dx)
\end{align*}

We know the following from trigonometric identities
$$\int^\pi_{-\pi} cos(kx)cos(mx) dx = \begin{cases}
    0, &k\neq m\\
    \pi, &k=m
\end{cases}$$
As well as the following from odd function properties
$$\int^\pi_{-\pi} cos(kx) dx = 0$$
$$\int^\pi_{-\pi} sin(kx)cos(mx) dx = 0$$
So now we get
\begin{align*}
    \int^\pi_{-\pi} f(x)cos(mx) dx
    &=  \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx +  \sum^\infty_{k=1} (a_k \int^\pi_{-\pi}cos(kx)dx + b_k \int^\pi_{-\pi}sin(kx)dx)\\
    &= a_m \pi\\
    &\implies a_m = \frac{1}{\pi} \int^\pi_{-\pi} f(x)cos(mx) dx
\end{align*}

\begin{example}
    Lets take
    $$f(x) = \begin{cases}
    1, &0\leq x < \pi\\
    -1, &-\pi \leq x < 0
    \end{cases}$$

    Note that this is an odd function, therefore $a_k = 0, \forall k \geq 0$. So now lets calculate $b_k$.

    \begin{align*}
        b_k &= \frac{1}{\pi} \int^\pi_{-\pi} f(x)sin(kx) dx\\
        &\overset{even}{=} \frac{2}{\pi} \int^\pi_{0} f(x)sin(kx) dx\\
        &= \frac{2}{\pi} \int^\pi_{0} sin(kx) dx\\
        &= \frac{2}{k\pi} [ -cos(kx) ]^\pi_0\\
        &= \begin{cases}
            \frac{4}{k\pi}, &\text{$k$ is odd}\\
            0, &\text{$k$ is even}
        \end{cases}
    \end{align*}

    And now lets right out the Fourier Polynomial $(F_N (x))$\\
    \underline{if $N$ is odd:}
    $$F_N (x) = \frac{4}{\pi}sin(x) + \frac{4}{3\pi}sin(3x) + \frac{4}{5\pi}sin(5x) + ...$$
    \underline{if $N$ is even:}
    $$F_N (x) = F_{N-1} (x)$$
    We can also write it as a Fourier Series
    $$F(x) = \frac{4}{\pi} \sum^\infty_{l=0} \frac{sin((2l+1)x)}{2l+1}$$

    The energy of the function is
    $$E = \frac{1}{\pi} \int^\pi_{-\pi} [f(x)]^2 dx = \frac{2}{\pi} \int^\pi_0 dx = 2$$

    The amplitutde of the $k$th harmonic is
    $$A_k = \sqrt{a_k^2 + b_k^2} = \sqrt{0 + \frac{16}{k^2\pi^2}} = \frac{4}{k\pi}$$

    The energy of the $k$th harmonic is
    $$A_k^2 = \frac{16}{k^2 \pi^2}$$

    Note that for this example, both the energy and the amplitude are 0 at an even $k$.\\
    \\
    Lets now evaluate the energy spectrum:
    $$k=1, E= \frac{16}{\pi^2} \approx 1.62, \frac{1.62}{2}=0.81=81\%$$
    $$k=3, E= \frac{16}{9\pi^2} \approx 0.18, \frac{0.18}{2}=0.09=9\%$$
    $$k=5, E= \frac{16}{25\pi^2} \approx 0.06, \frac{0.06}{2}=0.03=3\%$$
    $$k=7, E= \frac{16}{49\pi^2} \approx 0.03, \frac{0.03}{2}=0.015=1.5\%$$

\end{example}

However, we do not need to exclusively work with the interval $[-\pi, \pi]$ we can even work over any interval of length $2\pi$.

\subsection{General Fourier Series (interval of length $2\pi$)}

$$a_k = \frac{1}{\pi} \int^{c+2\pi}_{c} f(x) cos(kx) dx, k=1,2,3,...$$
$$b_k = \frac{1}{\pi} \int^{c+2\pi}_{c} f(x) sin(kx) dx, k=1,2,3,...$$

What about for $f$ if $f$ has period $p$?

$$f(x+p) = f(x), \forall x, \exists p \neq 0$$

We then substitute $x = \frac{pt}{2\pi}$ which gives a new function $f_p (t) = f(\frac{pt}{2\pi})$ with period $2\pi$. So

$$f_p (t+2\pi) = f(\frac{p}{2\pi} (t+2\pi)) = f(\frac{pt}{2\pi} + p) = f(\frac{pt}{2\pi}) = f_p (t)$$

So how about the Fourier Expansion for $f_p (t)$? To find this, we must replace $t$ by $\frac{2\pi x}{p}$ giving for $f(x)$.

$$F(x) = \frac{a_0}{2}+ \sum^\infty_{k-1} a_k cos(\frac{2nx\pi}{p}) + b_k sin(\frac{2nx\pi}{p})$$
$$a_k = \frac{2}{p} \int^{c+p}_{c} f(x) cos(\frac{2nx\pi}{p}x) dx, k=1,2,3,...$$
$$b_k = \frac{2}{p} \int^{c+p}_{c} f(x) sin(\frac{2nx\pi}{p}x) dx, k=1,2,3,...$$

For any function defined on $[a,b]$, we can extend $f$ to all of $\mathbb{R}$ as a periodic function. Given a periodic function $f_E$ from $f$ of period $p=b-a$, we now have:

$$a_k = \frac{2}{b-a} \int^{b}_{a} f(x) cos(\frac{2nx\pi}{b-a}x) dx, k=1,2,3,...$$
$$b_k = \frac{2}{b-a} \int^{b}_{a} f(x) sin(\frac{2nx\pi}{b-a}x) dx, k=1,2,3,...$$

\begin{example}
    Take the function $f(x) = x, 0\leq x < 1$ and extend it with period 1. For $k\neq 0$:
    \begin{align*}
        a_k &= 2 \int^{1}_{0} xcos(2k\pi x) dx\\
        &\overset{parts}{\underset{u=v, dv=cos(2\pi kx)dx}{=}} 2[\frac{xsin(2k\pi x)}{2\pi k}]^1_0 - \frac{2}{2k \pi} \int^1_0 sin(2k\pi x) dx\\
        &=0
    \end{align*}
    $$a_0 = 2 \int^1_0 x dx = [x^2 ]^1_0$$
    \begin{align*}
        b_k &= 2\int^1_0 x sin(2k\pi x)dx\\
        &\underset{u=x, dv=sin(2k\pi x)dx}{\overset{parts}{=}} 2[\frac{-xcos(2k\pi x)}{2\pi k}]^1_0 + \frac{1}{k\pi} \int^1_0 cos(2k\pi x)dx\\
        &= \frac{-cos(2k\pi)}{k\pi}\\
        &= \frac{-1}{k\pi}
    \end{align*}
    So the Fourier Series will be
    $$F(x) = \frac{1}{2} - \frac{1}{\pi} [sin(2\pi x) + \frac{sin(4\pi x)}{2} + \frac{sin(6\pi x)}{3} + ...]$$
\end{example}

\begin{example}
    $f(x) = |x|, -\pi < x \leq \pi$. Since $f(x)$ is even, $b_k=0, \forall k\in\mathbb{N}$.

    $$a_0 = \frac{1}{\pi} \int^\pi_{-\pi} |x|dx \overset{even}{=} \frac{2}{\pi} \int^\pi_0 x dx= \pi$$

   \begin{align*}
   	a_k &= \frac{1}{\pi} \int^\pi_{-\pi} |x| cos(kx) dx\\
	&\overset{even}{=} \frac{2}{pi} \int^\pi_0 x cos(kx) dx\\
	&\underset{u=x, dv=cos(kx)dx}{\overset{parts}{=}} \frac{2}{\pi} [\frac{x sin(kx)}{k} + \frac{cos(kx)}{k^2}]^\pi_0\\
	&= \frac{2}{\pi k^2} (cos(k\pi) - 1)\\
	&=\begin{cases}
		0, &\text{$k$ is even}\\
		\frac{-4}{\pi k^2}, &\text{$k$ is odd}
	\end{cases}
   \end{align*}

   So we end up with the Fourier Series

   $$F(x) = \frac{\pi}{2} - \frac{4}{\pi} \sum^\infty_{l=0} \frac{cos((2l+1)x)}{(2l+1)^2}$$

\end{example}

\begin{example}
	$f(x) = x, -\pi < x \leq \pi$. Note that since $f(x)$ is odd, $a_k = 0, \forall k \geq 0$
	\begin{align*}
		b_k &= \frac{1}{\pi} \int^\pi_{-\pi} x sin(kx) dx\\
		&\underset{u=x, dv=sin(kx)dx}{\overset{parts}{=}} \frac{1}{\pi} [\frac{-xcos(kx)}{k}]^\pi_{-\pi} + \frac{1}{\pi} \int^\pi_{-\pi} cos(kx) dx\\
		&= \frac{1}{\pi} [\frac{-\pi cos(k\pi)}{k} + \frac{-\pi cos(-k\pi)}{k}]\\
		&= \frac{-2}{k} cos(k\pi)\\
		&= \frac{(-1)^{k+1} 2}{k}
	\end{align*}
	And the Fourier Series of this is
	$$F(x) = \sum^\infty_{k=1} (-1)^{k+1} \frac{2}{k} sin(kx)$$
\end{example}

To get a Fourier cosine series or a Fourier sine series, we need
\begin{itemize}
	\item{an $f$ defined on the interval $[0,a]$, and we must extend this interval to also include $[-a,0)$ to give an even or odd function on $[-a,a]$}
	\item{$f(-t)=f(t), -a\leq t < 0$ for the even extension}
	item{$f(-t)=-f(t), -a\leq t < 0$ for the odd extension}
\end{itemize}

\newpage

\section{Monday, January 15, 2018}

\begin{example}
	We want to express $f(x)=x, 0\leq x < \pi$ as both a cosine series and a sine series.\\
	\\
	\underline{cosine series}
	For this, we need to extend the function as an even function, so we extend the function to $f(x)=|x|, -\pi < x \leq x$. This is a previous example that we computated before and gives us a cosine series\\
	\\
	\underline{sine series}
	For this, we must extend $f(x)$ as an odd function $f(x)=x, -\pi \geq x < \pi$. We've already seen from previous examples that is a sine series.\\
	\\
	So for $f(x)=x, 0 \leq x < pi$, the cosine series looks like this:
	$$F(x) = \frac{\pi}{2} - \frac{4}{\pi} \sum^\infty_{l=0} \frac{cos((2l+1)x)}{(2l+1)^2}$$
	And the sine series looks like this:
	$$F(x) = \sum^\infty_{k=0} (-1)^{k+1} \frac{2}{k} sin(kx)$$
	Both of these series on the interval $[0,\pi)$ represent $f(x)=x$
\end{example}

\begin{definition}
	A function $f(x)$ defined for $x\in[a,b]$ is \textbf{piece-wise continuous} if there exists a finite partition $P: a=t_0< .... < t_n =b$ such that $f$ is continuous on $x\in(t_{i=1}, t_i), \forall i$ and both $\lim_{x\to t^+_{i-1}} f(x)$ and $\lim_{x\to t^-_{i-1}} f(x)$ both exist and are both finite.
\end{definition}

\begin{note}
	On the $i$th subinterval, $f(x)$ coincides with some $f_i (x)$ that is continuous on that subinterval.
\end{note}

\begin{definition}
	If $f_i (x) \forall i$ has continuous 1st derivatives, $f(x)$ is called \textbf{piecewise smooth}
\end{definition}

\begin{definition}
	If $f_i (x) \forall i$ has continuous 2nd derivatives, $f(x)$ is called \textbf{piecewise very smooth}
\end{definition}

\begin{definition}
	The Fourier Series obtained from $f(x)$ converges to $f(x)$ if $f(x) = \lim_{N\to\infty} F_N (x)$

	\underline{i.e.} $$f(x) = \lim_{N\to\infty} F_N (x) = \frac{a_0}{2}+ \sum^N_{k-1} a_k cos(kx) + b_k sin(kx)$$

	assuming a period of $2\pi$ (and can be adjusted for other periods). The $a_k$ and $b_k$ are the Fourier coefficients.
\end{definition}

\begin{theorem}
	Let $f(x)$ be continuous and piece-wise very smooth for all $x$ and let $f(x)$ have period $2\pi$. Then the Fourier Series of $f(x)$ converges uniformly to $f(x), \forall x$
\end{theorem}

This helps with examples that have jump discontinuities.

\begin{theorem}
	Let $f(x)$ be defined and piece-wise very smooth for $x\in[-\pi,\pi]$ and let $f(x)$ be defined outside this interval to have period $2\pi$. Then the Fourier Series of $f(x)$ converges uniformly to $f(x)$ in each interval containing no discontinuity of $f(x)$. At each discontinuity, $x_0$, the series converges to
	$$\frac{1}{2} [\lim_{x\to x_0^+} f(x) + \lim_{x\to x_0^-} f(x)]$$
	This is the \textbf{Fundamental Theorem (for Fourier Series)}. The Theorem can be reinstated for $f$ defined on any interval of length $p\neq 0$
\end{theorem}

\begin{example}
	\begin{enumerate}
		\item $f(x) = \frac{-x}{2}$ on the interval $[-\pi, \pi]$
		\item $f(x) = \begin{cases}
			1, &0\leq x < \pi\\
			-1, &-\pi\leq x < 0
		\end{cases}
$
	\end{enumerate}
	Note that they both are piece-wise continuous and both satisfy the previous theorem.\\
	\\
	so (1.) converges to
	$$\sum^\infty_{k=1} \frac{(-1)}{k} sin(kx) = \begin{cases}
		-\frac{x}{2}, &x\in (-\pi, \pi)\\
		0, &x = \pm \pi
	\end{cases}
$$

And (2.) converges to

$$\sum^\infty_{k=1} \frac{sin((2k+1)x)}{2k+1} = \begin{cases}
		-1, &-\pi < x < 0\\
		1, &0 < x < \pi\\
		0, &x = -\pi, 0,\pi
	\end{cases}
$$

In (2.) set $x = \frac{\pi}{2}$ and we'll get
\begin{align*}
	1 &= \frac{4}{\pi}\sum^\infty_{k=1} \frac{sin((2k+1)\frac{\pi}{2})}{2k+1}\\
	1 &=\frac{4}{\pi}\sum^\infty_{k=1} \frac{(-1)^k}{2k+1}\\
	\frac{\pi}{4}&= \sum^\infty_{k=1} \frac{(-1)^k}{2k+1}\\
	\frac{\pi}{4}&=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+...
\end{align*}

So now we have $\frac{\pi}{4}$ represented by a series of numbers.

\end{example}

However, the domain of the function also play a role in the series.

\begin{example}
	For $f(x), x\in[0,1)$, we get
	$$F(x) = \frac{1}{2} - \frac{1}{\pi}\sum^\infty_{k=1} \frac{sin((2k+1)x)}{2k+1}$$
	The theorem still applies and we get $F(x) = x$ on the interval $[0,1)$, and converge to $\frac{1}{2}$ at $0$.
\end{example}

\begin{example}
	For $f(x)=|x|, x\in [-\pi,\pi]$ we get.
	$$F(x) = \frac{\pi}{2} - \frac{1}{\pi}\sum^\infty_{k=1} \frac{cos((2k+1)x)}{(2k+1)^2}$$
	on $(0,1), |x|=x$, and since $f$ is piece-wise very smooth, we can write:
	$$x=\frac{\pi}{2} - \frac{1}{\pi}\sum^\infty_{k=1} \frac{cos((2k+1)x)}{(2k+1)^2}$$
\end{example}

\section{Friday, January 19, 2018}

\begin{definition}
	The \textbf{orthonormal bases for $\mathbb{R}^n$}
	$$\{ e_1, ..., e_n \}$$
	are all orthogonal to each other and are all unit vectors.\\
	\\
	Each $v\in \mathbb{R}^n$ has a unique representation of
	$v = \lambda_1 e_1 + ... + \lambda_n e_n$ where $\lambda_k = v \cdot e_k$
\end{definition}

Given continuous functions $f,g$ that map from $[-\pi, \pi] \mapsto \mathbb{R}$ we can define an inner product by
$$<f,g> = frac{1}{\pi} \int^{\pi}_{-\pi} f(x) g(x) dx$$ defined on all continues real-valued functions defined on the interval $[-\pi, \pi] $

\begin{example}
	The set of functions $\{ cos(kx), sin(nx) \}_{n,k\in\mathbb{Z}}$ act like an orthonormal basis. since

	$$<cos(mx), cos(nx)> = \begin{cases}1, &\text{ if } m=n\\ 0, &\text{ otherwise} \end{cases}$$
	$$<sin(mx), sin(nx)> = \begin{cases}1, &\text{ if } m=n\\ 0, &\text{ otherwise} \end{cases}$$
	$$<sin(mx), cos(nx)> = 0$$

	for positive integers $m,n$
\end{example}

We can regard the Fourier Coefficients as the components of $f$ under this basis. This explains why you dont need to recalculate the coefficients for each $N$ for $F_N (x)$

\begin{corollary}
	If a function $f$ can be represented as a trigonometric polynomial, then that trigonometric polynomial is a Fourier Expansion of $f$, if the Fourier Series converges.
\end{corollary}

\begin{example}
	What is the fourier series of $f(x)=cos^2(x) - sin^2(x)$\\
	\\
	We have trigonometric identities to solve this:
	$$F(x)=cos^2(x) - sin^2(x)=cos(2x)$$
\end{example}

\begin{example}
	\begin{align*}
	f(x) &=cos^4(x)\\
	&=(cos^2(x))^2\\
	&=(cos^2(x))^2\\
	&= \frac{1}{4}(1+2cos(2x) + cos^2 (2x))\\
	&= \frac{1}{4}(1 + 2cos(2x) + \frac{1}{2} + \frac{1}{2}cos(4x))
	&= \frac{3}{8} + \frac{cos(2x)}{2} + \frac{1}{8}cos(4x) = F(x)
	\end{align*}
\end{example}

\begin{definition}
	The \textbf{Total Square Error} of $g(x)$ relative to $f(x)$ is:
	$$E = \int^\pi_{-\pi} [f(x) - g(x)]^2 dx$$
\end{definition}

\begin{note}
	if $f=g, E=0$
\end{note}

We want a constant function $y=g_0$ so the square error is as small as possible

\begin{align*}
	E(g_0) &= \int^\pi_{-\pi} [f(x) - g_0]^2 dx\\
	&= \int^\pi_{-\pi} f(x)^2 dx - 2 g_0 \int^\pi_{-\pi} f(x) dx + g_0^2 (2\pi)
\end{align*}

And if we let $A,B$ be constants such that $$A = \int^\pi_{-\pi} f(x)^2 dx, B = \int^\pi_{-\pi} f(x) dx$$

Then
$$E(g_0) = A - 2 B g_0 + 2 \pi g_0^2$$

With implies that $E(g_0)$ is a quad function in $g$ having a minimum value when its derivative is equal to 0.

$$-2B + 4\pi g_0 = 0$$

$$g_0 = \frac{B}{2\pi} = \frac{1}{2\pi} \int^\pi_{-\pi} f(x) dx = \frac{1}{2} a_0$$

Therefore $\frac{1}{2} a_0$ is the best constant approximation in the sense of the square error. We can show that the best approximation by a trigonometric polynomial is the Fourier Polynomial.

\begin{definition}
	If $f$ is piece-wise continuous on $[-\pi, \pi]$ we see that

	$$\frac{a_0}{2}+ \sum^N_{k=1} a_k^2 + b_k^2 \leq \frac{1}{\pi} \int_{-\pi}^\pi [f(x)]^2 dx = <f,f>$$
	This is called \textbf{Bessel's Inequality} and it shows that $\sum^\infty_{k=1} a_k^2 + b_k^2$ converges
\end{definition}

\begin{theorem}
(Uniqueness Theorem) Let $f(x), g(x)$ be piecewise continuous functions on the interval $[-\pi,\pi]$ and have all the same same Fourier coefficients. Then $f(x)=g(x)$ except, perhaps, at discontinuities.
\end{theorem}

\subsection{Vector-Valued Functions}

We will now spend time with vector-valued functions ($f:\mathbb{R}^n \mapsto A$)

\begin{definition}
	A \textbf{curve (or path)} in $\mathbb{R}^n$ is a function
	$$\gamma : [a,b] \subset \mathbb{R} \mapsto \mathbb{R}^n$$
	We usually call the image of $\gamma$ the curve and the function $\gamma$ as the parameterization of the curve
\end{definition}

\begin{note}
	This curve has a direction given by $\gamma$ and runs from $\gamma (a)$ to $\gamma (b)$, which are the endpoints. $\gamma (a)$ being the beginning and $\gamma (b)$ being the end
\end{note}

Think of $t, t\in [a,b]$ as a variable and $\gamma (t)$ as "tracing out" the curve in $\mathbb{R}$ as $t$ goes from $a$ to $b$.

This interval be adapted to any $(a,b)$ and even to $\mathbb{R}$ itself.

$$\gamma (t) = (\gamma_1 (t), \gamma_2 (t), ..., \gamma_n (t)), t\in[a,b]$$

\begin{example}
	where $t \in \mathbb{R}$
	\begin{align*}
		\gamma (t) &= (x_0, y_0, z_0) + t(v_1,v_2,v_3)\\
		&= x + tv
	\end{align*}
	This is a parametric representation of a line. Where $x$ is a point and $v$ is a direction vector

\end{example}

\begin{definition}
	$\gamma$ is \textbf{continuous} at $c\in (a,b)$ if $\lim_{t\to c} \gamma (t) = \gamma (c)$ iff the components $\gamma_i (t)$ for $i=1,...,n$ are continuous at $c$
\end{definition}

\begin{definition}
	If $\gamma$ is continuous at all points, $\gamma$ is called a \textbf{Continuous Path}
\end{definition}

\begin{example}
	Consider a circle of radius $3$ in $\mathbb{R}^2$ centred around the origin. A path of this circle is
	$$\gamma (t) = (3cost, 3sint), t \in [0,2\pi]$$
	We say this curve is oriented in the counter clockwise direction
\end{example}

\begin{example}
	The curve for $y=x^3 +1$ is
	$$\gamma (t) = (t, t^3 +1), t\in\mathbb{R}$$
\end{example}

\begin{example}
	In $\mathbb{R}^3, \gamma (t) = (3cost, 3sint, t) , t \in [0,2\pi]$.\\
	\\
	Since $x^2 + y^2 = 9$, this curve must live in the cylinder $x^2 + y^2 = 9$ and since $z=t$, the curve spirals upwards, this is called a helix
\end{example}

\begin{example}
	$\gamma (t)= (t, t^2, t^3), t\in [-2,2]$. This is a twisted cubic.
\end{example}

\begin{example}
	Find the parameterization of the curve $C$ of the intersection of the cylinder $x^2 + y^2 = 1$ and the plane $y+z = 2$. The projection into the $x-y$ plane is the circle $x^2 + y^2 = 1, z=0$, so

	$$x=cost, y=sint$$

	From the equation of the plane
	$$z = 2-y = 2-sint$$
	We now parameterize the curve $C$ by
	$$\gamma (t) = (cost, sint, 2-sint), t\in [0,2\pi]$$
\end{example}

\section{Monday, January 22, 2018}

\subsection{parameterization (continued)}

\begin{note}
	For any given curve, there may be several possible parameterizations of that curve
\end{note}

\begin{example}

\end{example} The $1$st quadrant piece of the unit circle in a counter clock-wise direction, from $(1,0)$ to $(0,1)$

$$\gamma_1 (t) = (cost, sint), t\in [0, \frac{\pi}{2}]$$
$$\gamma_2 (t) = (cos(\frac{t}{2}), sin(\frac{t}{2})), t\in [0, \pi]$$
$$\gamma_3 (t) = (\sqrt{1-t^2}, t), t\in [0, 1]$$
All parameterize the same curve.

\subsection{Derivatives}

\begin{definition}
	The \textbf{Derivative of a path} is
	$$D(\gamma(t)) = (\gamma_1'(t), \gamma_2'(t), ... , \gamma_n'(t))$$
	provided each $\gamma_i'(t)$ exists for $t=1,...,n$\\
	\\
	We usually write $\gamma' (t) = (\gamma_1'(t),...,\gamma_n'(t))$
\end{definition}

\begin{definition}
	If $\gamma'(t)$ exists, $\gamma$ is called a \textbf{differentiable path}
\end{definition}

\begin{definition}
	If $\gamma$ is a differentiable path with a continuous derivative, except at finitely many places, the image of $\gamma$ is called a \textbf{piece-wise smooth curve}
\end{definition}

\begin{definition}
	$\gamma'(t)$ is the tanget $v$ to the curve at the point $\gamma (t)$. Hence, the tanget line at $\gamma (t_0)$ is given by
	$$\gamma (t_0) + \lambda \gamma ' (t_0), \lambda \in \mathbb{R}$$
\end{definition}

Note that $\gamma (t)$ must be smooth (class $C^1$) for $t$ near $t_0$, be careful if $\gamma (t_0) = 0$

\begin{remark}
	If we think of $\gamma (t)$ representing the position of a particle at time $t$, we can regard $\gamma ' (t)$ as its velocity and $\gamma '' (t)$ as its acceleration.\\
	\\
	If $\gamma ' (t)$ is the velocity, then $||\gamma ' (t)||$ is the speed (magnitude of the velocity)
\end{remark}

\begin{example}
	Computing the period of a satellite when the radius of its orbit is known. The particle has constant speed $s$, and when $t=0$, assume that $\gamma (t)$ is at the point $(r,0)$.\\
	\\
	So....
	$$\gamma(t) = (rcos(kt), rsin(kt))$$
	We need to find $k$.
	$$\gamma ' (t) = (-rksin(kt), rkcos(kt))$$
	$$s = ||\gamma ' (t)|| = \sqrt{r^2 k^2 (sin^2 (kt) + cos^2 (kt))} = rk$$
	And this implies that $k = \frac{s}{r}$.\\
	\\
	Note that $r,k \in \mathbb{R}^+ > 0$

	$$\gamma '' (t) = (-rk^2 cos(kt), -rk^2 sin(kt)) = -k^2 \gamma (t)$$

	Assuming particle is a satellite of mass $m$ orbiting the earth of mass $M$.

	\begin{note}
		$$F = ma = m(\gamma '' (t)) = -mk^2 \gamma (t)$$
		$$F(\vec{v}) = \frac{-G m M \vec{v}}{||\vec{v}||^3}$$
		where $G$ is the gravitational constant and $\vec{v} = \gamma (t)$
	\end{note}

	$$-mk^2 \gamma (t) = \frac{-G m M \gamma (t)}{r^3}$$

	And this implies that $k^2 = \frac{GM}{r^3}$.\\
	\\
	What is the period? $p = \frac{2\pi}{k}$ this implies that $k = \frac{2\pi}{p}$.

	$$k^2 = (\frac{2\pi}{p})^2 = \frac{GM}{r^3}$$
	This implies that
	$$p^2 = \frac{4 \pi^2}{GM} r^3$$

	The period of a satellite squared is proportional to the radius of its orbit cubed (One of Kepler's Laws)

\end{example}

\subsection{Path Integrals}

\begin{definition}
	The \textbf{path integral of $f$} or the \textbf{integral of $f$ along the path $\gamma$} denoted
	$$\int_\gamma f ds$$
	is defined by
	$$\int_\gamma f ds = \int^b_a f(\gamma(t)) ||\gamma ' (t) || dt$$
	Whenever $\gamma: [a,b]\mapsto \mathbb{R}^n$ is a smooth curve and the composite function $t \mapsto f(\gamma(t))$ is continuous on $[a,b]$
\end{definition}

If $\gamma (t)$ is only piece-wise smooth or $f(\gamma (t))$ is only piece-wise continuous, we break $\gamma (t)$ into finitely many smooth pieces, and sum the integrals over the various pieces.

\section{Friday, January 26, 2018}

\subsection{Path Integral (continued)}

Why is this definition of a path integral a good definition?\\
\\
We can think of speed $s=||\gamma ' (t)||$ as a rate of change in position relative to change in time $t$. Often we denote $ds=||\gamma ' (t)||dt$\\
\\
If $f=1$ then
$$\int_\gamma ds = \int_a^b ||\gamma ' (t)|| ds$$
Which is just the length of the curve.

\begin{definition}
	\textbf{The length of the curve} is $$\int_\gamma ds = \int_a^b ||\gamma ' (t)|| ds$$
\end{definition}

The length of the path is the "total distance travelled."

\begin{example}
	Find the length of a circle of radius 1 from 0 to $2\pi$.\\
	\\
	$$\gamma (t) = (cos(t), sin(t)), t\in [0, 2\pi]$$
	$$\gamma ' (t) = (-sin(t), cos(t))$$
	$$||\gamma ' (t)|| = \sqrt{sin^2(t) + cos^2(t)} = 1$$


	$$s = \int_\gamma ds = \int_0^{2\pi} 1 ds = 2\pi$$
\end{example}

\begin{example}
	Find the arclength of $\gamma (t) = (sin(5t), cos(5t), \frac{10}{3} t^{1.5})$ from $t=0$ to $t=3$.\\
	\\
	$$\gamma ' (t) = (5cost, -5sint, 5 \sqrt{t})$$

	$$||\gamma ' (t)|| = \sqrt{25cos^2 (t) + 25sin^2 (t), 25 t} = 5\sqrt{1+t}$$

	$$s = \int_\gamma ds = 5 \int_0^3 \sqrt{1+t} dt = \frac{70}{3}$$
\end{example}

\begin{example}
	$f(x,y,z) = xy-z^2$ along $\gamma (t) = (3t, -2t, \sqrt{3}t), t\in [0,1]$\\
	\\
	$$\gamma ' (t) = (3, -2, \sqrt{3})$$
	$$||\gamma ' (t)|| = 4$$
	\begin{align*}
		f(\gamma (t)) &= f(3t, -2t, \sqrt{3}t)\\
		&=(3t)(-2t) - (\sqrt{3}t)^2\\
		&= -6t^2 - 3t^2 = -9t^2
	\end{align*}

	$$\int_\gamma f ds = \int_0^1 f(\gamma (t))||\gamma ' (t)||dt = \int_0^1 (-9t^2)4dt = -36 \int_0^1 t^2 dt = -12$$

\end{example}

\begin{remark}
	mass of a wire $(m)$ $=$ density $D$ $\times$ length
	$$m = \int_\gamma dm = \int_\gamma D ds$$
\end{remark}

\begin{note}
	$\int_\gamma f ds$ is independent of the choice of parameterization
\end{note}

\begin{note}
	If a curve in $\mathbb{R}^2$ is given by $y=f(x)$ and $x$ is in the domain of $[a,b]$, then the arclength is $$\int_a^b \sqrt{1+f'(x)^2} dx$$
\end{note}

\subsection{Vector Fields}

\begin{definition}
	A function $F:A \subset \mathbb{R}^n \mapsto \mathbb{R}^n$ is called a \textbf{vector field} where
	$$F(x) = (F_1(x), ..., F_n(x))$$
\end{definition}

\begin{definition}
	a function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is called a \textbf{scalar field}, the components of a vector field are scalar fields.
\end{definition}

We think of a vector field $F$ as attaching to each point $x$ a vector $f(x)$

\begin{example}
	Newtons Gravitational Law\\
	\\
	$$F(x) =  -\frac{mMG}{||x||^3} x$$
	"pull of a satellite towards the earth"\\
	\\
	The strength of the force depends on $d$, the distance away. Force is in the opposite direction to the position vector.

	$$F(x) = F(x,y,z) = (- \frac{mMG}{||(x,y,z)||^3}x, - \frac{mMG}{||(x,y,z)||^3}y, - \frac{mMG}{||(x,y,z)||^3}z)$$

\end{example}

\begin{example}
	Put a toy boat into a stream, a velocity field is the velocity at each point of the stream.\\
	\\
	Let the path be $\gamma (t)$, then $\gamma ' (t) = F(\gamma (t)), \forall t$
\end{example}

\begin{definition}
	If $F$ is a vector field, a \textbf{flowline} for $F$ is a path $\gamma (t)$ such that $\gamma ' (t) = F(\gamma (t))$
\end{definition}

\begin{definition}
	The \textbf{flow of a vector field} is the family of all flow lines
\end{definition}

\begin{example}
	$F(x,y) = (3,4)$, vector $(3,4)$ at every point will be straight lines. Want to know flow line at $(1,2)$

	$$\gamma (t) = (x(t), y(t))$$
	$$\gamma ' (t) = F(\gamma (t)) = (3,4)$$
	$$\gamma (t) = (3t + 1, 4t+2)$$
	as it passes through $(1,2)$.
\end{example}

\newpage

\section{Monday, January 29, 2018}

\subsection{Line Integrals}

\begin{example}
	For time $t$, force $F = F(\gamma(t)) \cdot \frac{\gamma ' (t)}{||\gamma ' (t)||} = F(\gamma (t)) \cdot T (\gamma (t))$

	$$\Delta w = F(\gamma(t)) \cdot T (\gamma (t)) \Delta s$$

	$$w= \int_\gamma F \cdot T ds$$

\end{example}

\begin{definition}
	Let $F$ be a vector field in $\mathbb{R}^n$ and let $\gamma : [a,b] \mapsto \mathbb{R}^n$ be a smooth curve in $\mathbb{R}^n$. The \textbf{line integral} of $F$ over $\gamma$, denoted $\int_\gamma F ds$ is defined by
	$$\int_\gamma F \cdot ds = \int_\gamma ( F \cdot T) ds$$
\end{definition}

\begin{remark}
	$(F \cdot T)$ is a scalar field, and as produced before:
	$$\int_\gamma ( F \cdot T) ds = \int_a^b F(\gamma(t)) \cdot \gamma ' (t) dt$$
\end{remark}

\begin{remark}
	Within signs, the line integral is independent of the parameterization. ie.
	$$\int_\gamma F \cdot ds = - \int_{-\gamma} F \cdot ds$$
\end{remark}

\begin{remark}
	We can work over piecewise smooth curves by dividing it into smooth sub intervals and summing them up.
\end{remark}

\begin{example}
	$F(x,y) = (y,x^2), \gamma (t) = (t^2, t^3), t\in[0,1]$
	$$F(\gamma (t)) = (t^3, t^4)$$
	$$\gamma ' (t) = (2t, 3t^2)$$

	\begin{align*}
		\int_\gamma F \cdot ds &= \int_0^1 (2t^4 + 3t^6) dt\\
		&= [\frac{2}{5} t^5 + \frac{3}{7} t^7]^1_0\\
		&= \frac{2}{5}+ \frac{3}{7} = \frac{29}{35}
	\end{align*}

\end{example}

\begin{example}
	$$F(x,y) = (2y-x, -y)$$
	$$\gamma (t) = (-cost, sint), t\in[0,\pi]$$
	$$\gamma_0 (t) = (cost, sint), t\in[0,\pi]$$

	\begin{align*}
		\int_\gamma F ds &= \int_0^\pi F(\gamma(t)) \cdot \gamma ' (t) dt\\
		&= \int_0^\pi (2sin^2 t + costsint - sint cost) dt\\
		&= 2\int_0^\pi (\frac{1}{2}-\frac{1}{2}cos(2t))dt = \pi
	\end{align*}

	\begin{align*}
		\int_{\gamma_0} F ds &= \int_0^\pi F(\gamma_0 (t)) \cdot \gamma_0 ' (t) dt\\
		&= \int_0^\pi (-2sin^2 t + costsint - sint cost) dt\\
		&= -2\int_0^\pi (\frac{1}{2}-\frac{1}{2}cos(2t))dt = -\pi
	\end{align*}

\end{example}

\begin{note}
	Notice from the previous example that
	$$\int_\gamma F ds = \int_0^\pi (2sint + cost)(sint)-(sint)(cost)$$
	we notice this is in the form
	$$A = \int_a^b F_1(\gamma(t)) \gamma_1 '(t) + ... + F_n(\gamma(t)) \gamma_n '(t) dt $$
	We introduce a new notation that can take advantage of this form and rewrite $A$ as
	$$\int_a^b F_1 dx_1 + ... + F_n dx_n $$
	where $dx_i = \gamma ' (t)dt, i=1,...,n$
\end{note}

\begin{definition}
	A (first-order) \textbf{differential form} (1-form) in $\mathbb{R}^n$ is an expression of the form
	$$f_1 dx_1 + ... + f_n dx_n$$
\end{definition}

\begin{definition}
	Let $\omega = f_1 dx_1 + ... + f_n dx_n$ be a differential form in $\mathbb{R}^n$ and let $\gamma : [a,b] \mapsto \mathbb{R}^n$ be a smooth curve in $\mathbb{R}^n$. The integral of $\omega$ over $\gamma$, denoted $$\int_\gamma \omega$$ is defined by
	$$\int_\gamma \omega = \int_\gamma F \cdot ds$$
	where $F = (f_1, ..., f_n)$
\end{definition}

So

\begin{align*}
	\int_\gamma \omega &= \int_a^b [f_1 (\gamma(t)) \gamma_1 ' (t) + ... + f_n (\gamma(t)) \gamma_n ' (t)]dt\\
	&= \int_a^b F(\gamma(t)) \gamma ' (t) dt = \int_\gamma F \cdot ds
\end{align*}
where $F = (f_1, ... , f_n)$\\
\\
We can think of $F = (f_1, ... , f_n)$ and $ds = (dx_1, ... , dx_n)$ where $dx_i = \frac{d(x_i (t))}{dt} dt, i=1,...,n$ which means the dot product notation makes sense.

\begin{example}
	$$\int_\gamma zx^2 dx + yzdy + arcsin(x)dz$$
	where $\gamma (t) = (t, t^2, 1), t\in [0,1]$

	\begin{align*}
		\int_\gamma \omega &= \int_0^1 [(t^2)(1) + (t^2)(2t) + (0)(arcsint) ]dt\\
		&= \int_0^1 (t^2+ 2t^3)dt\\
		&= [\frac{1}{3} t^3 + \frac{1}{2} t^4]_0^1\\
		&=\frac{5}{6}
	\end{align*}

\end{example}

Given a vector field $F=(f_1, ...,f_n)$ we have the (1-form) $\omega = f_1 dx_1 + ... + f_n dx_n$ and vice versa

\begin{definition}
	Let $f:\mathbb{R}^n \mapsto \mathbb{R}$. The differential form corresponding to the vector field $\nabla f$, denoted $df$, is called the \textbf{total differential of $f$}
	$$\nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}), df = \frac{\partial f}{\partial x_1} d x_1 + \frac{\partial f}{\partial x_2} d x_2 + ... + \frac{\partial f}{\partial x_n} d x_n$$
\end{definition}

If we differentiate with respect to $f$, we get
$$\frac{df}{dt} = \frac{\partial f}{\partial x_1} \frac{d x_1}{dt} + \frac{\partial f}{\partial x_2} \frac{d x_2}{dt} + ... + \frac{\partial f}{\partial x_n} \frac{d x_n}{dt}$$
"the total derivative of $f$ with respect to $t$"

\begin{definition}
	Let $F:\mathbb{R}^n \mapsto \mathbb{R}^n$ be a vector field. If $F = \nabla g$, for some $g: \mathbb{R}^n \mapsto \mathbb{R}$ then $F$ is said to be \textbf{conservative}.
\end{definition}

\begin{definition}
	Let $\omega$ be a first order differential form in $\mathbb{R}^n$. If $\omega = dg$ for some $g: \mathbb{R}^n \mapsto \mathbb{R}$ then $\omega$ is said to be \textbf{exact}.
\end{definition}

\begin{note}
	$(F_1, ..., F_n) = F = \nabla g \Longleftrightarrow dg = \omega = F_1 dx_1 + F_2 dx_2 + ... + F_n dx_n$
	So this implies that $F$ is conservative iff $\omega$ is exact
\end{note}

\begin{example}
	$\int_\gamma xy dx + xdy$ where
	\begin{enumerate}
		\item{$\gamma$ is a parameterization of the top half of the unit circle}
		\item{$\gamma$ is a parameterization of the line $(1,0) \to (-1,0)$}
	\end{enumerate}

	\begin{enumerate}
	\item{
		$$\gamma (t) = (cost t, sin t), t \in [0,\pi]$$
		\begin{align*}
			\int_\gamma xy dx + xdy &= \int^\pi_0 ((cost)(sint)(-sint) + (cost)(cost))dt\\
			&= \int^\pi_0 (sin^2 t + cost + cos^2 t) dt\\
			&= \int^\pi_0 \frac{1}{2} dt = \frac{\pi}{2}
		\end{align*}

	}
	\item{
	$$\gamma (t) = (1-t,0), t\in [0,2]$$
		\begin{align*}
			\int_\gamma xy dx + xdy &= \int^2_0 (0)(-1) + (1-t)(0)dt = 0
		\end{align*}

	}
	\end{enumerate}
\end{example}

\begin{note}
	The line integral depends on the path chosen, not on how you parameterize that path
\end{note}

Think of a parameterization as a bijection, except possibly at finitely many of those points.

\begin{theorem}
	Suppose $F:\mathbb{R}^n \mapsto \mathbb{R}$ is of class $C^1$ and $\gamma: [a,b] \mapsto \mathbb{R}^n$ is a piecewise smooth path in $\mathbb{R}^n$ then
	$$\int_\gamma \nabla f ds = f(\gamma(b)) - f(\gamma(a))$$
\end{theorem}

\begin{proof}
	Apply the chain rule to the composite function $G:\mathbb{R} \mapsto \mathbb{R}$ given by $G(t) = (f \circ \gamma)(t)$
	$$G'(t) = \nabla f(\gamma(t)) \gamma '(t) \in \mathbb{R}$$
	Then we apply FTOC to get
	\begin{align*}
		\int^b_a G'(t) dt &= G(b) - G(a)\\
		&= f(\gamma(b)) - f(\gamma(a))
	\end{align*}

	$$\int_\gamma \nabla f ds = \int^b_a \nabla f(\gamma(t)) \gamma '(t) = f(\gamma(b)) - f(\gamma(a))$$

\end{proof}

\begin{corollary}
	If $F = \nabla f$ is a conservative vector field and $\gamma$ is a curve joining $x_0, x_1$ then
	$$\int_\gamma Fds = f(x_1) - f(x_0)$$
\end{corollary}

The line integral of a conservative vector field is independent of the choice of path

\underline{Remarks:}
\begin{enumerate}
	\item{This theorem and corollary can be restated in terms of differential forms. For example, If $\omega = df$ is exact and $\gamma$ is a curve joining $x_0, x_1$ then $$\int_\gamma \omega = f(x_1) - f(x_0)$$}
	\item{
	This result (any form) is a generalization of the FTOC and is often called the "General Fundamental Theorem of Calculus" (GFTC) ($\int_\gamma d\omega = \int_{\partial \gamma} \omega$)
	}
\end{enumerate}

\newpage

\section{Friday, February 2, 2018}

\begin{definition}
  A \textbf{closed curve} is a curve which joins a point $x_0$ to itself
\end{definition}

\begin{definition}
  A \textbf{simple curve} is a curve which does not intersect itself
\end{definition}

\begin{definition}
  A \textbf{Jordan curve} is a non-self-intersecting continuous loop in the plane
\end{definition}

\begin{corollary}
  If $F$ is a conservative vector field, and $\gamma$ is a closed curve, then
  $$\int_\gamma F\cdot ds = 0$$
\end{corollary}

\begin{definition}
  If $F=\nabla g$ is a conservative vector field then $g$ is called a potential function for the vector field F
\end{definition}

\begin{theorem}
  If $F$ is a vector field and $\int_\gamma F\cdot ds = 0, \forall$ closed curves $\gamma$ then $F$ is convservative. (The idea is that:)
  $$\int_{\gamma+\sigma} F\cdot ds = \int_\gamma F\cdot ds + \int_\sigma F\cdot ds = 0$$
\end{theorem}

\begin{example}
  $$\int_\gamma y^2 dx + 2xydy, \gamma (t) = (arctan(sin(\frac{\pi}{2}t)), e^{cos(\frac{\pi}{2}t)}), t\in [0,1]$$
  $g(x,y) = xy^2 + c$ is a potential function for $\omega = y^2 dx + 2xydy$.
  \\
  \begin{align*}
    \int_\gamma y^2 dx + 2xydy &= g(\gamma (1)) - g(\gamma (0))
  \end{align*}
  $$\gamma (1) = (arctan(1), e^0) = (\frac{\pi}{4}, 1)$$
  $$\gamma (0) = (arctan(0), e) = (0,e)$$
  This implies that $\int_\gamma = \frac{\pi}{4} (1) - (0)(e^2) = \frac{\pi}{4}$. Assume that $F_1,...,F_n $ are of class $C^1$, then $F = (F_1, ..., F_n)$ or $\omega = F_1 dx_1 + ... + F_n dx_n$\\
  \\
  So if $F = v \cdot g$ or $\omega = dg$ then $F_i = \frac{\partial g}{\partial x_i}$\\
  \\
  $F_1,...,F_n$ are $C^1 \Longrightarrow$ g must be $C^2$

  $$\frac{\partial F_i}{\partial x_j} = \frac{\partial^2 g}{\partial x_j \partial x_i} = \frac{\partial^2 g}{\partial x_i\partial x_j} = \frac{\partial F_j}{\partial x_i}$$
  $\Longrightarrow$ a necessary condition for $F$ to be conservative or $\omega$ to be exact is
  $$\frac{\partial F_i}{\partial x_j} = \frac{\partial F_j}{\partial x_i}, i,j \in [1,n], i\neq j$$
\end{example}

\begin{definition}
  Let $\omega = F_1 dx_1 + ... + F_n dx_n$. If
  $$\frac{\partial F_i}{\partial x_j} = \frac{\partial F_j}{\partial x_i}, i,j \in [1,n], i\neq j$$
  Then $\omega$ is said to be closed.
\end{definition}

\begin{note}
    exact $\Longrightarrow$ closed
\end{note}

\begin{definition}
  A subset $S\subset \mathbb{R}^n$ is said to be \textbf{simply connected} if given two curves with the same endpoints on $S$, one can be continuously deformed in the other within $S$.\\
  \\
  In $\mathbb{R}^n$, if $S$ is connected, it is also simply connected if for every closed curve $\gamma$ in $S$, the interior of $\gamma$ is also in $S$.
\end{definition}

\begin{example}
  $\mathbb{R}^3 \setminus \{ 0 \}$ is simply connected
\end{example}

\begin{example}
  $\mathbb{R}^3 \setminus \{ (x,0,0) | x\in \mathbb{R} \}$ is not simply connected
\end{example}

\begin{definition}
  If $\omega = F_1 dx_1 + ... + F_n dx_n$ the domain of $\omega$ is the intersection of the domains of all $F_i, i\in [1,n]$
\end{definition}

\begin{theorem}
  If $\omega$ is a differential form whose domain is simply connected then $\omega$ is closed iff $\omega$ is exact. $\mathbb{R}^n$ is simply connected $\Longrightarrow$ If $\omega$ is defined on all $\mathbb{R}^n$, $\omega$ is exact iff $\omega$ is closed
\end{theorem}

\newpage

\section{Monday, February 5, 2018}

\begin{theorem}
	Let $\omega = F_1 dx_1 + ... + F_n dx_n$ be 1-form defined on all of $\mathbb{R}^n$, then this implies that $\omega$ is exact $\Longleftrightarrow$ it is also closed, since it is defined on all of $\mathbb{R}^n$.
\end{theorem}

\begin{note}
	exact means there exists a potential function $$g:\mathbb{R}^n \mapsto \mathbb{R}, \exists \omega = dg$$
\end{note}

And so we want to find $g$. We have to do the following steps to find $g$

\subsection{Finding the potential function}

(This is the setup for $n=2, \mathbb{R}^2$).


	Let $\omega = F_1 dx + F_2 dy$\\
	(is it closed? We're checking for $\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}$).\\
	$\mathbb{R}^2$ is simply connected, so if closed $\Longrightarrow$ exact then the potential function $g$ exists.\\
	We know that $\frac{\partial g}{\partial x} = F_1$ and $\frac{\partial g}{\partial y} = F_2$
	$$g(x,y) = \int F_1 (x,y)dx + \frac{dh(x)}{dy}$$
	And then we need to find $h$
	$$\frac{\partial g(x,y)}{\partial y} = \frac{\partial}{\partial y}(\int F_1 (x,y)dx + \frac{dh(x)}{dy})$$
	$$\frac{dh(x)}{dy} = F_2 (x,y) + \frac{\partial}{\partial y}(\int F_1 (x,y)dx)$$
	All $x$ should cancel from the right hand side, leaving a function of $y$ a lone, which we can integrate to get $h$.
	$$\frac{\partial}{\partial x}(F_2 (x,y) + \frac{\partial}{\partial y}(\int F_1 (x,y)dx))$$
	$$\Longrightarrow \frac{\partial F_2}{\partial x} - \frac{\partial^2}{\partial x\partial y}  (\int F_1 (x,y)dx)$$
	$$\frac{\partial F_2}{\partial x} - \frac{\partial^2}{\partial x\partial y}  (\int F_1 (x,y)dx) = \frac{\partial F_2}{\partial x} - \frac{\partial F_2}{\partial y} = 0$$

	\begin{example}
		(This example is in $n=3, \mathbb{R}^3$)\\

		$$\omega = (z^2 + 2xy)dx + (x^2 +2yz)dy + (y^2 + 2xz + cos(z))dz$$
		$$F_1 = (z^2 + 2xy)$$
		$$F_2 = (x^2 +2yz)$$
		$$F_3 = (y^2 + 2xz + cos(z))$$

		So we start out by checking if omega is closed, and since
		$$\frac{\partial F_2}{\partial x} = 2x = \frac{\partial F_1}{\partial y}$$
		$$\frac{\partial F_1}{\partial z} = 2z = \frac{\partial F_3}{\partial x}$$
		$$\frac{\partial F_2}{\partial z} = 2y = \frac{\partial F_3}{\partial y}$$
		Since the cross partials are equal, it is closed. More formally, the Domain of $\omega$ is $\mathbb{R}^3$ which is simply connected, and closed $\Longrightarrow$ exact.\\
		\\
		We know there is a potential function $g:\mathbb{R}^3 \mapsto \mathbb{R}$\\
		\\
		Now we must find and construct $g$.

		$$\frac{\partial g}{\partial x} = F_1 = (z^2 + 2xy)$$
		$$\Longrightarrow g(x,y,z) = \int (z^2 + 2xy) + h(y,z)$$
		$$\int (z^2 + 2xy) + h(y,z) =  xz^2 + x^2y + h(y,z)$$

		And since we know that

		$$F_2 = x^2 + 2yz = \frac{\partial}{\partial y} (xz^2 + x^2y + h(y,z))$$
		$$2yz = \frac{\partial h}{\partial y}$$
		$$\Longrightarrow \int 2yz dy + k(z) = h(x,y)$$
		$$\Longleftrightarrow h(x,y) = \int 2yz dy + k(z) = y^2 z + k(z)$$
		Now we have
		$$g(x,y,z) = xz^2 + x^2y + y^2 z + k(z)$$
		$$F_3 = y^2 + 2xz + cos(z) = \frac{\partial}{\partial z}(y^2 z + k(z))$$
		$$2xz + cos(z) = \frac{\partial k(z)}{\partial z}$$
		$$k(z) = \int (2xz + cos(z)) dz = x^z + sin(z) + C$$
		Therefore, we have
		$$g(x,y,z) = 2xz^2 + x^2 y + y^2 z + sin(z) + C$$

	\end{example}

	\subsection{Green's Theorem}

	\begin{Theorem}
		Let $\gamma$ be a smooth Jordan Curve in $\mathbb{R}^2$ oriented in the counterclockwise direction, and let $R$ be the region enclosed by $\gamma$. Let $\omega = F_1 dx + F_2 dy$ where $F_1, F_2 : \mathbb{R} \mapsto \mathbb{R}$ are of class $C^1$ throughout $\mathbb{R}$. then
		$$\int_{\gamma = \partial R} F_1 dx + F_2 dy = \int_R (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA$$
	\end{Theorem}

	\begin{note}
		Can restate in terms of a vector field. The conclusion would look like this:
		$$\int_{\gamma = \partial R} F\cdot ds = \int_R (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA$$ where $F = (F_1, F_2)$
	\end{note}

	\begin{note}
		The theorem can easily be generalized to the case where $\gamma$ is a Plane Simple Jordan Curve
	\end{note}

	\begin{note}
		Sometimes the LHS is given as $$\varointctrclockwise_{\gamma = \partial R} F_1 dx + F_2 dy$$ which indicates a closed curve in the counterclockwise direction  ($\varointclockwise$ for the other direction).
	\end{note}

	\begin{note}
		Should note that this theorem is of the form $$\int_{\partial R} f = \int_R df$$
	\end{note}

	\begin{example}
		Let $R$ be a disc of radius 3 centered at the origin. Verify Green's Theorem by direct calculation when $F_1 (x,y) =  xy$ and $F_2 (x,y) = x$.\\
		\\
		So to verify Green's Theorem, we will calculate the left hand side and the right hand side separately and compare our results.\\
		\underline{Left Hand Sign}\\
		\\
		$\partial R$ is a circle of radius 3 centered at the origin\\
		We can parameterize it by $\gamma (t) = (3cost, 3sint), t\in [0,2\pi]$
		\begin{align*}
			\int_\gamma F_1 dx + F_2 dy &= \int_\gamma xydx + xdy\\
			&= \int^{2\pi}_0 (3cost)(3sint)(-3sint)+ (3cost)(3cost)dt\\
			&= 9 \int^{2\pi}_0 cos^2 t dt\\
			&= 9 \int^{2\pi}_0 (\frac{1}{2} + \frac{1}{2}cos(2t)) dt\\
			&= \frac{9}{2} \int^{2\pi}_0 dt = 9\pi
		\end{align*}

		\underline{Left Hand Sign}\\
		\\
		\begin{align*}
			\iint_{\text{disk}} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA &= \iint_{\text{disk}} (1-x) dA\\
			&\overset{\text{polars}}{=} \int^{2\pi}_0 \int^3_0 (1-rcos\theta)rdrd\theta\\
			&=\int^{2\pi}_0 \int^3_0 (r-r^2cos\theta)drd\theta\\
			&=\int^{2\pi}_0 [\frac{1}{2}r^2 - \frac{1}{3} r^2 cos\theta]^3_0 d\theta\\
			&=\int^{2\pi}_0 (\frac{9}{2}-0) d\theta = 9\pi
		\end{align*}
		Since both sides are the same, Green's theorem holds for this example.
	\end{example}

	\begin{corollary}
		(Corollary 1 of Green's Theorem)\\
		If $\gamma$ is a smooth Jordan Curve in $\mathbb{R}^2$ and $R$ is the region enclosed by $\gamma$ then the area of $R$ is given by
		$$A(R) = \frac{1}{2} \int_\gamma xdy-ydx$$
	\end{corollary}

	If we apply Green's theorem to this corollary, we get
	$$\frac{1}{2} \iint_R (1+1) dA = \iint_R dA = \text{The area of $R$}$$

	\begin{corollary}
		If the domain of $\omega$ (in $\mathbb{R}^2$) is simply connected, the $\omega$ is closed $\Longrightarrow$ exact.
	\end{corollary}

	$\omega = F_1 dx + F_2 dy$, so closed would be $(\frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y})$\\
	If $\gamma$ is any closed curve then
	\begin{align*}
		\int_{\partial R = \gamma} \omega &= \iint_R (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) dA\\
		&= \iint_R 0 dA = 0
	\end{align*}

	\begin{example}
		Find the area of the ellipse
		$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$$\\
		\\
		$\gamma (t) = (acost, bsint), t\in [0,2\pi]$
		\begin{align*}
			\text{Area} &= \frac{1}{2} \int_\gamma xdy-ydx\\
			&= \frac{1}{2} \int^{2\pi}_0 (acost)(bcost)-(bsint)(-asint)dt\\
			&= \frac{1}{2} \int^{2\pi}_0 ab (cos^2 t + sin^2 t)dt\\
			&= \pi ab
		\end{align*}
	\end{example}

	\begin{example}
		Recall the previous examples,\\
		$\int_\gammma xyd + xdy$ can go from $(1,0)\to (-1,0)$ in 2 differents way\\
		\begin{enu}
			\item{Along top half of the unit circle}
			\item{Along the $x$-axis}
		\end{enu}
		This time we'll take $\gamma$ as the top half of the unit cricle followed along the $x$-axis back to $(1,0)$.\\
		Now since Green's Theorem holds, since it's closed:
		\begin{align*}
			\int_\gamma xydx + xdy &\overset{\text{Green's}}{=} \int^1_{-1} \int_0^{\sqrt{1-x^2}} (1-x) dydx\\
			&\overset{\text{polar}}{=} \int^\pi_0 \int^1_0 (r^2 - r^2 cos\theta) dr d\theta\\
			&= \int^\pi_0 [\frac{1}{2} - \frac{1}{2} cos\theta]d\theta\\
			&= \frac{\pi}{2}
		\end{align*}
		Does this agree with our previous calculation?\\
		Yes.
	\end{example}

	\subsection{Winding Numbers}

	\begin{example}
		$$\omega = \frac{-y}{x^2+y^2} + \frac{x}{x^2+y^2}$$
		$$\frac{\partial F_1}{\partial y} = \frac{y^2 - x^2}{(x^2+y^2)^2}$$
		$$\frac{\partial F_2}{\partial y} = \frac{y^2 - x^2}{(x^2+y^2)^2}$$
		$\Longrightarrow$ closed, but not exact, since it is not simply connected as the domain is $\mathbb{R}^2 - \{ (0,0) \}$\\
		\\
		If it were exact, $\int_\gamma \omega = 0$ for all closed $\gamma$\\
		\\
		Let $\gamma$ parameterize the unit circle in the counterclockwise direction.\\
		$\gamma (t) = (cos t, sin t), t \in [0,2\pi]$\\
		\begin{align*}
			\int_\gamma \omega &= \int^{2\pi}_0 (-sint)(-sint) + (cost)(cost) dt\\
			&= \int^{2\pi}_0 (sin^2 t + cos^2 t) dt\\
			&= \int^{2\pi}_0 dt\\
			&= 2\pi \neq 0
		\end{align*}
		We have a closed curve that is $\neq 0$\\
		\\
		If the radius were $R$.
		$$(\frac{-R sint}{R^2})(-Rsint) + (\frac{-R cost}{R^2})(Rcost) &= \frac{R^2}{R^2} sin^2 t + \frac{R^2}{R^2} cos^2 t = 1$$
		$\Longrightarrow$ the integral would still be $2\pi$\\
		\\
		Does this contradict Green's Theorem?\\
		No, Since $F_1, F_2$ are not defined throughout the region $R$ enclosed by $\gamma$ and $F_1, F_2$ is not defined at $(0,0)$\\
		\\
		Is this unique to circles?\\
		No.

	\end{example}

\newpage

\section{Friday, February 9, 2018}

Continuing from the example from before:

\begin{example}
	$\omega = \frac{-y}{x^2+y^2} + \frac{x}{x^2+y^2}$ is closed but not exact. $\int_\gamma \omega = 2\pi$ when $\gamma$ is a counterclockwise parameterization of a unit circle of radius $R$.
\end{example}

	Generalizing, we have:
	\begin{theorem}
		Let $\sigma$ be any Jordan curve which circles the origin in the counterclockwise direction.
	\end{theorem}
	\underline{CLAIM:} $\int_\sigma \omega = 2\pi$\\
	\\
	\begin{proof}
		\begin{enumerate}
			\item{Does not depend on the shape of $\sigma$ (as long as it's around the origin)}
			\item{
			Any such curve can be continuously deformed in the circle\\
			Let $\sigma = \sigma_1 + \sigma_2$\\
			$\gamma = -\sigma_1 - \sigma_2$ (since it is counterclockwise)\\
			Without loss of generality, assume $\sigma$ is longer than $\gamma$\\
			$$\int_{\sigma_1} \omega = \int_\alpha \omega + \int_{\gamma_1} \omega + \int_{\beta} \omega \overset{\text{Green's}}{=} \int_{R_1} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) dA = 0$$
			$$\int_{\sigma_2} \omega = \int_{-\beta} \omega + \int_{\gamma_2} \omega + \int_{-\alpha} \omega \overset{\text{Green's}}{=} \int_{R_2} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) dA = 0$$
			Note that $(\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) = 0 \Longrightarrow$ closed
			}
		\end{enumerate}
		And now, if we add the 2 equations, then we get:
		\begin{align*}
			\int_{\sigma_1} \omega + \int_{\sigma_2} \omega + \int_{\gamma_1} \omega + \int_{\gamma_2} \omega &= \int_\sigma \omega + \int_{-\gamma} \omega\\
			&= \int_\sigma \omega + -\int_{\gamma} \omega = 0
		\end{align*}
		$\Longrightarrow \int_\sigma \omega = \int_{\gamma} \omega = 2\pi$
	\end{proof}

	\begin{note}
		$\frac{1}{2\pi} \int_{\gamma} \omega$ is always an integer. It will give the number of times $\gamma$ encircles the origin in the counterclockwise direction
	\end{note}

	\begin{definition}
		If $\gamma$ is a closed curve which wraps around the origin, the integrals
		$$\frac{1}{2\pi} \int_{\gamma} \frac{-y}{x^2+y^2} + \frac{x}{x^2+y^2}$$
		is called the \textbf{winding number} of $\gamma$
	\end{definition}

	\begin{example}
		if $\gamma$ never completely encircles the origin, the winding number is 0
	\end{example}

	\begin{example}
		if $\gamma$ encircles the origin once in the counterclockwise direction, then winding number is $-1$
	\end{example}

	\begin{note}
		The corresponding vector field to the 1-form used above is
		$$F(x,y) = (\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2})$$
		This vector field is called a \textbf{vortex field}.
	\end{note}

	\begin{example}
		Evaluate $\int_\gamma F ds$ where $F$ is the vortex field and $\gamma$ is the boundary curve of the square $[-1,1] \times [-1,1]$ oriented in the clockwise direction.\\
		\\
		\begin{align*}
			\varointclockwise F ds &= \int_\gamma (\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2}) ds\\
			&= 2\pi \cdot (\text{Winding Number})\\
			&= 2\pi (-1)\\
			&= -2\pi
		\end{align*}
	\end{example}

	\begin{example}
		$f(x,y) = -tan^{-1} (\frac{x}{y}) + C, y\neq 0$\\
		\\
		$$\frac{\partial f}{\partial x} = \frac{-y}{x^2+y^2}, \frac{\partial f}{\partial y} = \frac{x}{x^2+y^2}$$
		But $f(x,y)$ is not defined for all of $\mathbb{R}^2$, but the same set without the point $(0,0)$.\\
		We can find a potential function on some subset, say $\{ (x,y)\in\mathbb{R}^2 | y>0 \}$ (simply connected subset).
	\end{example}

\newpage

\section{Friday, March 2, 2018}

We'll determine the orientation by choosing a unit normal vector.
\begin{itemize}
	\item{we know that for most surfaces, there are 2 normals, $u_1, u_2$, with\\ $u_2 = -u_1$}
	\item{We'll choose one of the normals}
	\begin{itemize}
		\item{It could be one that points up}
		\item{It could be one that points out (which is arbitrary)}
		\item{etc.}
	\end{itemize}
\end{itemize}

Let the unit sphere be oriented by the outward pointing unit normal. This means it is outside to the positive side and the inside is the negative side.\\
\\
There are surfaces where much choices are not possible.

\begin{definition}
	An \textbf{Oriented Surface} is a 2-sided regular (smooth) surface to which we have attached, at each point, a unit normal vector (which varies continuously from point to point)
\end{definition}

\begin{definition}
	An \textbf{Orientable surface is a 2-sided regular (smooth) surface to which we can attach, at each point, a unit normal vector}
\end{definition}

If $S$ is the boundary of a 3-dimension region. Then it is oriented by the outward pointing unit normal.

\begin{definition}
	Let $F$ be a vector field defined on a surface $S$ which is oriented by a unit normal $n$. The \textbf{(vector) surface integral of $F$} over $S$, denoted
	$$\int_S F \cdot dS$$
	is defined by
	$$\int_S F \cdot dS = \int_S F \cdot n dS$$
\end{definition}

\begin{note}
	\begin{enumerate}
		\item{$F \cdot n$ is a scalar field}
		\item{From physics, $\int_S F \cdot n dS$ is called a flux of the vector field $F$ across $S$. When the surface is closed, we sometimes use the notation
		$$\oiint_S F \cdot n dS $$
		In this case, refer to the \textbf{flux out of $S$} if $n$ is the unit outer normal and \textbf{flux into $S$} if $n$ is the unit inner normal.
		}
		\item{There are other forms of (vector) surface integrals, but this is the only one we'll consider in this course}
		\item{I'll (Dr. Eric Moore) will use both notations}
	\end{enumerate}
\end{note}

Why is this a reasonable definition?\\
\\
It can be written as components. One in the normal direction and the other in the tangential direction.

$F = (F \cdot n) n + G$ where $G \cdot n = 0$. $G$ is tangent to the surface and $F \cdot n$ is the normal component. It is the normal piece that we need to consider to see what passes through the surface.

$$\int_S (F \cdot n) dS = \text{(area of S)(average normal )}$$

Take $S$ to be a porous surface. What flows through in time $t$?\\
\\
Say $\delta s$ is a small piece of the surface. The amount that flows through $\delta s$ will fill a parallelepiped with length $t\cdot || F ||$ in direction $F$. What's the volume?\\
\\
$$\Delta s (||\text{projection of $t\cdot F$ onto the normal $(t\cdot \Delta S)$}||) = (\Delta s)(tF \cdot n) = (F \cdot n \Delta s)$$

How do we calculate? (Vector surface integrals)
\begin{itemize}
	\item{restrict to $\mathbb{R}^3$}
	\item{assume $S$ is a 2-dimension surface in $\mathbb{R}^3$ oriented by $u$ and assume $\Phi (u,v)$ is a regular parameterization of $S$.}
\end{itemize}
We find $\phi_u, \phi_v$ and $\phi_u \times \phi_v$ (a normal)

$$\frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||} \text{ is a unit normal}$$

This implies that
$$\frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||} = \pm n$$
which implies that
$$\phi_u \times \phi_v = \pm || \phi_u \times \phi_v|| n$$

The plus and minus determines if it is in the same direction as $n$ or in the opposite direction.

\begin{note}
	it is an adequate to check at a single point the orientation of the surface
\end{note}


\begin{align*}
	\int_\Phi F \cdot dS &= \int_\Phi F \cdot n dS\\
	&= \int_D F (\Phi (u,v)) \cdot n (\Phi(u,v)) || \phi_u \times \phi_v|| dA\\
	&= \pm \int_D F (\Phi (u,v)) \cdot \frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||} ||\phi_u \times \phi_v|| dA\\
	&= \pm \int_D (F(\Phi (u,v)) \cdot \phi_u \times \phi_v) dA
\end{align*}

\begin{example}
	Evaluate $\int_S F \cdot dS$

	$F(x,y,z) = (x,y,z), S$ is the part of the place $x+y+z=1$ which lies in the first octant and $n$ points upward.\\
	\\
	We parameterize $S$ by $\Phi (u,v) = (u,v, 1-u-v)$ where $(u,v)\in D$\\
	\\
	$D$ is the projection in the $xy$-plane.
	$$0 \leq v \leq 1-u$$
	$$0 \leq u \leq 1$$
	$$\phi_u \times \phi_v = (1,1,1)$$

	Since the $z$-component is positive, it points relatively upward. So its in the correct direction.

	\begin{align*}
		\int_\Phi F \cdot dS &= \int_D (F(\Phi (u,v)) \cdot \phi_u \times \phi_v) dA\\
		&= \int_0^1 \int_0^{1-u} (u,v,1-u-v) \cdot (1,1,1) dv du\\
		&= \int_0^1 \int_0^{1-u} (u+v+1-u-v) dv du\\
		&=\int_0^1 \int_0^{1-u} dv du\\
		&= \int_0^1 (1-u) du\\
		&= [u=\frac{1}{2}u^2]_0^1 = 1 - \frac{1}{2} = \frac{1}{2}
	\end{align*}



\end{example}

\begin{example}
	$F(x,y,z) = (x,y,z), S$ is that part of the paraboloid $z=4-x^2-y^2$ with $0 \leq x \leq 1$ and $0 \leq y \leq 1$ with $n$ pointing outward\\
	\\
	Parameterize $S$ by $\Phi (u,v) = (u,v, 4-u^2-v^2)$ where $0 \leq u \leq 1$ and $0 \leq v \leq 1$ and $\phi_u \times \phi_v = (2u, 2v, 1)$\\
	\\
	Since $z=1 > 0$, this is the correct direction.

	\begin{align*}
		\int_\Phi F \cdot dS &= \int_0^1 \int_0^1 F(\Phi (u,v)) \cdot \phi_u \times \phi_v dudv\\
		&= \int_0^1 \int_0^1 (u,v,4-u^2-v^2) \cdot (2u, 2v, 1) dudv\\
		&= \int_0^1 \int_0^1 (2u^2 + 2v^2 + 4-u^2-v^2)dudv\\
		&= \int_0^1 \int_0^1 (4+u^2+v^2) dudv\\
		&= \int_0^1 [4u+ \frac{u^3}{3} + v^2 u]^1_0 dv\\
		&= \int_0^1 (4+\frac{1}{3} + v^2)dv\\
		&=[4v + \frac{v}{3} + \frac{v^3}{3}]_0^1\\
		&= 4 + \frac{2}{3} = \frac{14}{3}
	\end{align*}

\end{example}

\begin{example}
	Integrate $F(x,y,z) = (x,y,z)$ on the covered bowl:
	\underline{bowl:} $S_1 : z = x^2 + y^2$, $0 \leq z \leq 1$\\
	\underline{cover:} $S_2 : (z-1)^2 + x^2 + y^2 =1$, $1 \leq z \leq 2$\\
	Where both $S_1, S_2$ are oriented by the outward unit normal.\\
	\\
	\underline{bowl ($S_1$):} $\Phi (u,v) = (ucos(v), usin(v), u^2)$ where $0 \leq u \leq 1$, $0 \leq v \leq 2 \pi$
	$$\phi_u = (cosv, sinv, 2u)$$
	$$\phi_v = (-usinv, ucosv, 0)$$
	$$\phi_u \times \phi_v = u(-2ucosv, -2usinv, 1)$$

	For the unit outer normal, the $z$-component would have to be negative. However, the $z$ component of $\phi_u \times \phi_v$ is $1 > u > 0$ so the unit normal vector

	\begin{align*}
		-\int_{S_1} F \cdot n ds &= - \int_{u,v} (ucosv, u^2, usinv) \cdot (-2u^2 cosv, -2u^2 sinv, u) dA\\
		&= -\int_0^1 \int_0^{2\pi} (-2u^3cos^2 v - 2u^4 sinv + u^2 sinv) dv du = \frac{\pi}{2}
	\end{align*}

	\underline{cover ($S_2$):} $\Phi (u,v) = (cos(v)sin(u), sin(v)sin(u), 1+cos(u))$ where $0\leq v \leq$, $0 \leq u \leq \frac{\pi}{2}$\\
	\\
	$$\phi_u = (cos(v)cos(u),sin(v)cos(u), -sin(u))$$
	$$\phi_v = (-sin(v)sin(u), cos(v)sin(u), 0)$$
	$$\phi_u \times \phi_v =sin(u)(cos(v)sin(u), sin(v)sin(u), cos(u))$$
	The $z$-component of both $u$ and $\phi_u \times \phi_v$ are positive, so they are the correct direction.

\end{example}






\end{document}
