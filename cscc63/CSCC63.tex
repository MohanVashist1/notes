\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{graphicx}
\graphicspath{ {imgs/} }

\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{tcolorbox}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage{textcomp}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Computability and Complexity -- Winter 2020}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}


\begin{document}

\title{CSCC63: Computability and Complexity\\ Lecture Notes}
\date{University of Toronto Scarborough -- Winter 2020}
\author{Joshua Concon}
\maketitle
Instructor is Eric Corlett. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{LEC 1: Monday, January 6, 2020}

In this course, we want to explore the limitations of computation that go beyond processing power.

Consider the following problems:

\begin{enumerate}
  \item Given $x,y \in \mathbb{Z}$, what is $x^2 + y^3$?
  \item Given $x,y,z \in \mathbb{Z}$, is $z = x^2 + y^3$ true?
  \item Given $x,z$, does there exist a $y$ such that $z = x^2 + y^3$?
\end{enumerate}

Note that the first problem is not a \textbf{decision problem}, because it's solution is not yes/no or true/false, but the other two are. This course will mostly revolve around decision problems.

\paragraph{Decision Problem} A problem whose answer is Yes or No.

Now, consider the following. We'll define $f(x,y,z)$ as a program that solves the second problem. So the second problem returns true iff this function returns true and vice versa.
\\
Now, we'd call $(x,y,z)$ the input to the program $f$, but we want to separate the problem from the program. So we'll call the $(x,y,z)$ an \textbf{instance} of the second problem.

\paragraph{Instance} the input of a problem

\paragraph{Yes-Instance} the input of a decision problem that returns a yes

\paragraph{No-Instance} the input of a decision problem that returns a no
\\
\\
If we wanted to work with graphs, we'd have to encode a graph in a computer as an adjacency matrix, which we can turn into a string (which is computer readable). If we encode a graph $G$, we refer to its encoded format as $\langle G \rangle$
\\
\\
Note that there are some things we cannot represent in a computer, like $\pi$

\paragraph{Algorithm} A finite sequence of logical steps that always terminates.

\section{LEC 2: Wednesday, January 8, 2020}

Note that some problems cannot be solved and that we cannot build an algorithm to solve them. For example, "does one set of statements (in first order logic) imply another?"
\\
\\
How do we prove that a problem is not computable? And not by examples...
\\
\\
We have various Formulisms to understand algorithms:
\begin{itemize}
    \item Lambda Calculus
    \item Turing Machine -- automatons with arbitrary memory
    \item Post Machine --  automatons with states and arbitrary memory
\end{itemize}
\\
\\
These Formulisms emulate computers and each other.
\\
\\
Before expanding on Turing Machines, let us first revisit automata.

\begin{tcolorbox}
 DFSA (Deterministic Finite State Automata)
 \begin{itemize}
     \item have states and arrows
     \item doesn't store anything
 \end{itemize}
\end{tcolorbox}

Note that DFSAs cannot solve any problem that would require some sort of counting.
\\
\\
To solve this, we introduce:

\begin{tcolorbox}
 PDA (Push-down automata)
 \begin{itemize}
     \item has stack memory
     \item this means we cannot access memory in an arbitrary order
     \item we cannot interleave patterns
     \item size limitation: input string
 \end{itemize}
\end{tcolorbox}

Now, consider the following: How can we check if a number is prime?
\\
\\
\begin{itemize}
    \item By trying to divide it by every number? This is inefficient
    \item But there is a polynomial time algorithm that checks primality
    \item It's not enough to simply show an algorithm as 'proof' that there exists no efficient algo that solves a problem
\end{itemize}

So how do we show that there is no algorithm to efficiently solve some problem?
\\
\\
We can use Formulisms, such as Turning Machines, which are stronger versions of Finite State Automata.

Recall that:
\begin{itemize}
    \item DFSAs are deciders, they answer YES or NO
    \item DFSAs recognize regular languages
    \item the string or the "tape" is read left to right, char by char, each time going to the next state
\end{itemize}

Also Recall that:
\begin{itemize}
    \item PDAs can also recognize context free languages
\end{itemize}

Now, A Turning Machine is a more powerful version of these automata. It is defined as such:

\begin{tcolorbox}
 Turing Machines:
 \begin{itemize}
     \item Has unrestricted computing time as the head of the tape can move around upon reading a character (like a cursor)
     \item Unrestricted memory, as we are allowed to write on the tape and we assume the tape has infinite blank characters to its right
 \end{itemize}
\end{tcolorbox}

A more formal Definition of a Turing Machine is as such:
\begin{tcolorbox}
 a TM $M$ is a tuple
 
 $$M = (Q, \Sigma, \Gamma, \delta, s, q_A, q_R)$$
 
 \begin{itemize}
     \item $Q$ is a set of states
     \item $\Sigma$ is the input alphabet (does not include blanks)
     \item $Gamma$ is the tape alphabet
     \item $\delta$ is the transition function, defined as $$\delta: Q \times \Gamma \mapsto Q \times \Gamma \times \{ L,R \}$$
     \item $s$ is the start state
     \item $q_A$ is the accept state
     \item $q_R$ is the reject state
 \end{itemize}
 
\end{tcolorbox}

Note that: $\Sigma \subseteq \Gamma$
\\
\\
Note that: If we ever reach a point where there's no arrow out of the current state, we immediately go into an implicit state, reject, and halt.
\\
\\
Note that: If we're on the left side of the tape and try to move left, we just stay in the same place.
\\
\\
We have been looking at deterministic TMs, but non-deterministic ones also exist.
\\
\\
We'll give our one more definition -- Configurations represent the state of the TM, and what char is being looked at in the input string.
\\
\\
We'll be focusing on TMs, but not on the automata themselves. We like them for 2 reasons:
\begin{itemize}
    \item We can basically program them like regular computers
    \item Their configurations are well-behaved, and so we can do math on them. This will be what allows us to relate TMs (and computability results on TMs) to non-TM objects later on
\end{itemize}
We'll explore more on the second reason later.
\\
\\
So how do we build a TM from a language ourselves?
\begin{enumerate}
    \item Convert the language into code for a recognizer for that language
    \item Convert that code into a flowchart
    \item Convert the flowchart into a Turing Machine
\end{enumerate}

Can we tell when (and if) a TM will halt on a given input? Nope.
\\
\\
This is because we have an unlimited tape length, thus unlimited memory, and unlimited possible configurations!
\\
\\
We cannot use something like the pumping lemma to prove / disprove that a program will halt on an input.

\section{LEC 3: Monday, January 13, 2020}

The Turing Machines are good for our purpose of describing algorithms because we can program them in a way that's similar to regular computers.
\\
\\
Of course, if there's another way of building these machines that's more powerful then that type of machine would be better.
\\
\\
Let's look at a couple of ways we might try to make these automata more powerful.
\\
\\
Here's some ways we might want to try to do this:
\begin{itemize}
    \item Using more than one tape
    \item Using non-deterministic machines
    \item Allowing the TM to access arbitrary places in its memory
\end{itemize}

We'll start this week by arguing that these modifications \textbf{don't change the expressive power of the TM}, we can prove this by emulating these machines using a regular TM.
\\
\\
Let's first look at multiple tapes

\subsection{Multiple Tape Turing Machines}

This means we have multiple input strings, with tape heads (that may be in different positions) for each tape.
\begin{itemize}
    \item multi-tape TMs are just as powerful as regular TMs
    \item but multi-tape TMs are faster
    \item there is actually a guaranteed amount of slow down when using single-tape TMs
\end{itemize}

We can emulate a multi-tape TM using a single-tape TM by doing the following:
\begin{itemize}
    \item concatenate the input tapes into a single one, without loss of info
    \begin{itemize}
        \item So we omit intermediate blank space symbols and replace them with another symbol (say, \#)
        \item We also use another symbol (say, \$), to denote the beginning char
        \item We'll also need another symbol to denote the tape heads (say, $.$)
    \end{itemize}
    \item How it works: two passes on the concatenated string will be used for each "next step"
    \begin{itemize}
        \item \textbf{rightwards sweep}, which will take note of current states and the next symbols for each string. By the end of the sweep, we know the states and next symbols of each string
        \item \textbf{leftwards sweep}, which will use the info we gathered beforehand to update each of the strings
    \end{itemize}
    \item We need to do some bookkeeping to remember what symbols we've seen
\end{itemize}

So we can emulate multiple tapes with a single-tape TM.
\\
\\
We end up with a time cost, though: if our strings have a maximum length of $n$, the process of moving back and forth on the tape to update our positions will take up to $n$ times as many steps.

\subsection{Non-deterministic Turing Machines}

\begin{itemize}
    \item non-determinism are for the questions of the form: "does there exist a solution for this problem"?
    \item while determinism questions are like: "is x a solution for this problem"?
\end{itemize}

Non-deterministic TMs (NTM) are defined as having multiple outgoing arrows from a state, and that we can follow any one of these arrows.

\textbf{How NTMs accept \slash reject a string}?
\begin{itemize}
    \item the input forms a tree that results from branching from all possible arrows
    \item if any one of the branches leads to a configuration with $q_\text{accept}$, then we accept the string, otherwise reject.
\end{itemize}
\\
\\
We can do this by performing a breath first search (BFS) on the configuration tree.
\\
\\
Why not use DFS? Because using DFS might cause us to explore an infinitely looping branch, even if there exists another branch with an accepting state. BFS avoids this by exploring branches level by level.
\\
\\
emulating a NTM using a deterministic TM is possible but has a cost $\Theta (k^n)$

\subsection{RAM in a Turing Machine}

Finally, we discuss RAM (random access memory)
\begin{itemize}
    \item The ability to jump to arbitray memory locations
    \item gives $n^6$ slowdown
    \item still doable
\end{itemize}
\\
\\
Key Points:
\begin{itemize}
    \item We have different ways to implement Turing Machines but all have the same expressive power
    \item all we need from these variants is that they
    \begin{itemize}
        \item allow arbitrary memory access
        \item do finite amount of work in any step
    \end{itemize}
\end{itemize}

This leads us to the Church-Turing thesis.

\begin{tcolorbox}
 The Church-Turing Thesis: Anything that we can reasonably expect to do with an algorithm can also be decided using a TM. So a problem can be solved by an algorithm iff the problem can also be solved using Turing Machines
\end{tcolorbox}
\\
\\
Of course, so far we've only talked about decision problems. If we want to talk about programs that return some non-Boolean value, we just treat whatever is on the TM's tape when it halts as its return value.
\\
\\
Now that we've argued that TMs can encode any algorithm, we'll start to look at all the things that TMs cannot do.

\end{document}