\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{graphicx}
\graphicspath{ {imgs/} }

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{tcolorbox}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage{textcomp}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Computability and Complexity -- Winter 2020}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}


\begin{document}

\title{CSCC63: Computability and Complexity\\ Lecture Notes}
\date{University of Toronto Scarborough -- Winter 2020}
\author{Joshua Concon}
\maketitle
Instructor is Eric Corlett. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{LEC 1: Monday, January 6, 2020}

In this course, we want to explore the limitations of computation that go beyond processing power.

Consider the following problems:

\begin{enumerate}
  \item Given $x,y \in \mathbb{Z}$, what is $x^2 + y^3$?
  \item Given $x,y,z \in \mathbb{Z}$, is $z = x^2 + y^3$ true?
  \item Given $x,z$, does there exist a $y$ such that $z = x^2 + y^3$?
\end{enumerate}

Note that the first problem is not a \textbf{decision problem}, because it's solution is not yes/no or true/false, but the other two are. This course will mostly revolve around decision problems.

\paragraph{Decision Problem} A problem whose answer is Yes or No.

Now, consider the following. We'll define $f(x,y,z)$ as a program that solves the second problem. So the second problem returns true iff this function returns true and vice versa.
\\
Now, we'd call $(x,y,z)$ the input to the program $f$, but we want to separate the problem from the program. So we'll call the $(x,y,z)$ an \textbf{instance} of the second problem.

\paragraph{Instance} the input of a problem

\paragraph{Yes-Instance} the input of a decision problem that returns a yes

\paragraph{No-Instance} the input of a decision problem that returns a no
\\
\\
If we wanted to work with graphs, we'd have to encode a graph in a computer as an adjacency matrix, which we can turn into a string (which is computer readable). If we encode a graph $G$, we refer to its encoded format as $\langle G \rangle$
\\
\\
Note that there are some things we cannot represent in a computer, like $\pi$

\paragraph{Algorithm} A finite sequence of logical steps that always terminates.

\section{LEC 2: Wednesday, January 8, 2020}

Note that some problems cannot be solved and that we cannot build an algorithm to solve them. For example, "does one set of statements (in first order logic) imply another?"
\\
\\
How do we prove that a problem is not computable? And not by examples...
\\
\\
We have various Formulisms to understand algorithms:
\begin{itemize}
    \item Lambda Calculus
    \item Turing Machine -- automatons with arbitrary memory
    \item Post Machine --  automatons with states and arbitrary memory
\end{itemize}
\\
\\
These Formulisms emulate computers and each other.
\\
\\
Before expanding on Turing Machines, let us first revisit automata.

\begin{tcolorbox}
 DFSA (Deterministic Finite State Automata)
 \begin{itemize}
     \item have states and arrows
     \item doesn't store anything
 \end{itemize}
\end{tcolorbox}

Note that DFSAs cannot solve any problem that would require some sort of counting.
\\
\\
To solve this, we introduce:

\begin{tcolorbox}
 PDA (Push-down automata)
 \begin{itemize}
     \item has stack memory
     \item this means we cannot access memory in an arbitrary order
     \item we cannot interleave patterns
     \item size limitation: input string
 \end{itemize}
\end{tcolorbox}

Now, consider the following: How can we check if a number is prime?
\\
\\
\begin{itemize}
    \item By trying to divide it by every number? This is inefficient
    \item But there is a polynomial time algorithm that checks primality
    \item It's not enough to simply show an algorithm as 'proof' that there exists no efficient algo that solves a problem
\end{itemize}

So how do we show that there is no algorithm to efficiently solve some problem?
\\
\\
We can use Formulisms, such as Turning Machines, which are stronger versions of Finite State Automata.

Recall that:
\begin{itemize}
    \item DFSAs are deciders, they answer YES or NO
    \item DFSAs recognize regular languages
    \item the string or the "tape" is read left to right, char by char, each time going to the next state
\end{itemize}

Also Recall that:
\begin{itemize}
    \item PDAs can also recognize context free languages
\end{itemize}

Now, A Turning Machine is a more powerful version of these automata. It is defined as such:

\begin{tcolorbox}
 Turing Machines:
 \begin{itemize}
     \item Has unrestricted computing time as the head of the tape can move around upon reading a character (like a cursor)
     \item Unrestricted memory, as we are allowed to write on the tape and we assume the tape has infinite blank characters to its right
 \end{itemize}
\end{tcolorbox}

A more formal Definition of a Turing Machine is as such:
\begin{tcolorbox}
 a TM $M$ is a tuple
 
 $$M = (Q, \Sigma, \Gamma, \delta, s, q_A, q_R)$$
 
 \begin{itemize}
     \item $Q$ is a set of states
     \item $\Sigma$ is the input alphabet (does not include blanks)
     \item $Gamma$ is the tape alphabet
     \item $\delta$ is the transition function, defined as $$\delta: Q \times \Gamma \mapsto Q \times \Gamma \times \{ L,R \}$$
     \item $s$ is the start state
     \item $q_A$ is the accept state
     \item $q_R$ is the reject state
 \end{itemize}
 
\end{tcolorbox}

Note that: $\Sigma \subseteq \Gamma$
\\
\\
Note that: If we ever reach a point where there's no arrow out of the current state, we immediately go into an implicit state, reject, and halt.
\\
\\
Note that: If we're on the left side of the tape and try to move left, we just stay in the same place.
\\
\\
We have been looking at deterministic TMs, but non-deterministic ones also exist.
\\
\\
We'll give our one more definition -- Configurations represent the state of the TM, and what char is being looked at in the input string.
\\
\\
We'll be focusing on TMs, but not on the automata themselves. We like them for 2 reasons:
\begin{itemize}
    \item We can basically program them like regular computers
    \item Their configurations are well-behaved, and so we can do math on them. This will be what allows us to relate TMs (and computability results on TMs) to non-TM objects later on
\end{itemize}
We'll explore more on the second reason later.
\\
\\
So how do we build a TM from a language ourselves?
\begin{enumerate}
    \item Convert the language into code for a recognizer for that language
    \item Convert that code into a flowchart
    \item Convert the flowchart into a Turing Machine
\end{enumerate}

Can we tell when (and if) a TM will halt on a given input? Nope.
\\
\\
This is because we have an unlimited tape length, thus unlimited memory, and unlimited possible configurations!
\\
\\
We cannot use something like the pumping lemma to prove / disprove that a program will halt on an input.

\section{LEC 3: Monday, January 13, 2020}

The Turing Machines are good for our purpose of describing algorithms because we can program them in a way that's similar to regular computers.
\\
\\
Of course, if there's another way of building these machines that's more powerful then that type of machine would be better.
\\
\\
Let's look at a couple of ways we might try to make these automata more powerful.
\\
\\
Here's some ways we might want to try to do this:
\begin{itemize}
    \item Using more than one tape
    \item Using non-deterministic machines
    \item Allowing the TM to access arbitrary places in its memory
\end{itemize}

We'll start this week by arguing that these modifications \textbf{don't change the expressive power of the TM}, we can prove this by emulating these machines using a regular TM.
\\
\\
Let's first look at multiple tapes

\subsection{Multiple Tape Turing Machines}

This means we have multiple input strings, with tape heads (that may be in different positions) for each tape.
\begin{itemize}
    \item multi-tape TMs are just as powerful as regular TMs
    \item but multi-tape TMs are faster
    \item there is actually a guaranteed amount of slow down when using single-tape TMs
\end{itemize}

We can emulate a multi-tape TM using a single-tape TM by doing the following:
\begin{itemize}
    \item concatenate the input tapes into a single one, without loss of info
    \begin{itemize}
        \item So we omit intermediate blank space symbols and replace them with another symbol (say, \#)
        \item We also use another symbol (say, \$), to denote the beginning char
        \item We'll also need another symbol to denote the tape heads (say, $.$)
    \end{itemize}
    \item How it works: two passes on the concatenated string will be used for each "next step"
    \begin{itemize}
        \item \textbf{rightwards sweep}, which will take note of current states and the next symbols for each string. By the end of the sweep, we know the states and next symbols of each string
        \item \textbf{leftwards sweep}, which will use the info we gathered beforehand to update each of the strings
    \end{itemize}
    \item We need to do some bookkeeping to remember what symbols we've seen
\end{itemize}

So we can emulate multiple tapes with a single-tape TM.
\\
\\
We end up with a time cost, though: if our strings have a maximum length of $n$, the process of moving back and forth on the tape to update our positions will take up to $n$ times as many steps.

\subsection{Non-deterministic Turing Machines}

\begin{itemize}
    \item non-determinism are for the questions of the form: "does there exist a solution for this problem"?
    \item while determinism questions are like: "is x a solution for this problem"?
\end{itemize}

Non-deterministic TMs (NTM) are defined as having multiple outgoing arrows from a state, and that we can follow any one of these arrows.

\textbf{How NTMs accept \slash reject a string}?
\begin{itemize}
    \item the input forms a tree that results from branching from all possible arrows
    \item if any one of the branches leads to a configuration with $q_\text{accept}$, then we accept the string, otherwise reject.
\end{itemize}
\\
\\
We can do this by performing a breath first search (BFS) on the configuration tree.
\\
\\
Why not use DFS? Because using DFS might cause us to explore an infinitely looping branch, even if there exists another branch with an accepting state. BFS avoids this by exploring branches level by level.
\\
\\
emulating a NTM using a deterministic TM is possible but has a cost $\Theta (k^n)$

\subsection{RAM in a Turing Machine}

Finally, we discuss RAM (random access memory)
\begin{itemize}
    \item The ability to jump to arbitray memory locations
    \item gives $n^6$ slowdown
    \item still doable
\end{itemize}
\\
\\
Key Points:
\begin{itemize}
    \item We have different ways to implement Turing Machines but all have the same expressive power
    \item all we need from these variants is that they
    \begin{itemize}
        \item allow arbitrary memory access
        \item do finite amount of work in any step
    \end{itemize}
\end{itemize}

This leads us to the Church-Turing thesis.

\begin{tcolorbox}
 The Church-Turing Thesis: Anything that we can reasonably expect to do with an algorithm can also be decided using a TM. So a problem can be solved by an algorithm iff the problem can also be solved using Turing Machines
\end{tcolorbox}
\\
\\
Of course, so far we've only talked about decision problems. If we want to talk about programs that return some non-Boolean value, we just treat whatever is on the TM's tape when it halts as its return value.
\\
\\
Now that we've argued that TMs can encode any algorithm, we'll start to look at all the things that TMs cannot do.

\section{LEC 4: Wednesday, January 15, 2020}

We'll be discussing the RAM model of computation in this lecture. In this model, each tape cell has an address, and we can jump to any address we want during the computation. In particular, we can read memory addresses from the tape in order to figure out where to jump.
\\
\\
You can see that this basically gives us something similar to a regular computer's architecture.
\\
\\
We won't describe how to implement this model in detail, but we will state that we can implement it, and that the cost is in the order if an $n^6$ slowdown where $n$ is the length of the string.
\\
\\
So all told, we have several ways of implementing TMs, and we can see that they are equivalent in terms of expressive power.
\\
\\
In fact, any model we can build in which:
\begin{itemize}
    \item We allow arbitrary memory access
    \item Every step can only do a finite amount of work
\end{itemize}
Ends up having the same expressive power. This leads us to what we call the Church-Turing Thesis.
\\
\\

Although TMs can encode any language -- there are things that TMs cannot do. A lot of the trouble comes from self-references (when TMs can take itself as input).
\\
\\
We know that we can write TMs as strings, and feed that string to another TM (or itself!)
\\
\\
Consider the language $$A_{TM} : \{ <M,w> : M\text{ accepts }w \}$$
We can probably note that there would be some self-reference in a TM that solves this.
\\
\\
We will try to prove that it isn't decidable via proof by diagonalization.
\\
\\
Basically a proof by contradiction:
\begin{itemize}
    \item assume that we can solve the language $A_{TM}$ with a TM $M_A$
    \item then we write another TM $D$, which uses $M_A$
\end{itemize}
Where $D$ does the following:

\begin{lstlisting}
on input <M>:
    run M_A on input <M,M>
    if M_A accepts, then reject
    if M_A rejects, then accept
\end{lstlisting}

What if you input $<D>$ into $D$?
\begin{itemize}
    \item if $D$ accepts $<D>$ (because $M_A$ runs in $D$), then $D$ will reject $<D>$
    \item if $D$ rejects $<D>$ (because $M_A$ runs in $D$), then $D$ will accept $<D>$
\end{itemize}
This cannot be. Basically, this logic may emulate an infinite stack, it will not halt.
\\
\\
We cannot have something like $M_A$ exist, thus, $A_{TM}$ cannot be solved.
\\
\\
There is no algorithm (and thus -- TM) that can solve $A_{TM}$. We say that $A_{TM}$ is undecidable
\\
\\
There are two types of languages:
\begin{itemize}
    \item Decidable: there exists a decider (a TM that halts and either accepts or rejects every possible input)
    \item Undecidable: there is no decider. However, the language could be recognizable, co-recognizable, or neither.
\end{itemize}
\\
\\
The basic idea is that we're going to a particular type of proof called diagnolization: we're going to try to code something called the liar's paradox into a program, under the assumption that a particular problem is decidable.
\\
\\
If we could succeed, we'd have a paradox, so there must be a flaw somewhere in our code. The flaw will be the assumption that our original program is decidable.

\begin{tcolorbox}
 A Liar's Paradox is just a statement of the form "This statement is false" (a problematic statement in computer language).
 \\
 \\
 You can see that this statement can't be true, otherwise that would mean that it would be false, in which case, it can't be true.
 \\
 \\
 But if it's false, then it can't be false either.
 \\
 \\
 Natural language can handle this kind of mess, since we don't require all of its statements to be true or false. But in logic, or in a computer program that always gives one of two answers, we should never be able to see this sort of statement.
\end{tcolorbox}

So here's the first language we'll show that is not solvable using algorithms (because solving it would involve some amount of self-reference):

$$A_{TM} = \{ <M,w> | M \text{ accepts }w \}$$

So we want to build a liar's paradox around this program,  Here's how we'll do it:

\begin{itemize}
    \item We'll assume that $A_{TM}$ can be solved by some TM $M_A$
    \item{We'll use $M_A$ to build another TM $D$. Now, $D$ will take input of the form $<M>$, where $M$ is some TM. Here's its pseudocode:
    \begin{lstlisting}
    D = on input <M>:
        run M_A on input <M,M>
        if M_A accepts, then reject
        if M_A rejects, then accept
    \end{lstlisting}}
    \item So basically, $D$ just flips the output of $M_A$, but the thing is that $M_A$ should halt and give the right answers on all of its inputs, including $<D,D>$
    \item So what happens if we plug $<D>$ as input into $D$?
    \begin{itemize}
        \item $D$ runs $M_A$ on $<D,D>$
        \item $M_A$ halts and accepts iff $D$ accepts the input $<D>$. Otherwise it halts and rejects.
        \item $D$ flips this answer, so if $D$ accepts $<D>$, it must reject. If it rejects $<D>$, it must accept.
    \end{itemize}
    \item You can see that neither of these answers is possible -- and so $D$ can't have a well-defined result.
    \item This is our contradiction: the only place in our construction where we made any assumption was when we assumed that $M_A$ existed.
\end{itemize}

Hence, there is no TM (or algorithm) $M_A$ that can solve $A_{TM}$ ($A_{TM}$ is an undecidable problem).
\\
\\
A problem that cannot be solved using a TM or an algorithm is called decidable. A problem that cannot is called undecidable. So $A_{TM}$ is undecidable.
\\
\\
So what goes wrong?
\\
\\
Think about what happens if you try to run $D$. It tries to guess its output in order to reverse it, but then, that second level $D$ would just be trying to reverse another $D$ another level down, which is trying to reverse another run of $D$...
\\
\\
You get an inifinite number of runs of $D$, all trying to second guess each other. There's no place where you can ever stop.
\\
\\
That's how we can get out of this problem: our programs can accept or reject, but they can also loop forever. That's what happens here.
\\
\\
Now once we allow algorithms that can loop, it's easy enough to write a TM that will tell if a TM $M$ accepts an input $w$:

\end{document}