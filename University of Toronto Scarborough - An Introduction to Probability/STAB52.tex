\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{dsfont}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{An Introduction to Probability -- Summer 2017}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}


\begin{document}

\title{STAB52: An Introduction to Probability}
\date{University of Toronto Scarborough -- Summer 2017}
\author{Joshua Concon}
\maketitle

Pre-reqs are MATA37, which is Calculus for Mathematical Sciences II.
Instructor is Dr. Mahinda Samarakoon. I highly recommend sitting at the front since he likes to teach with the board and can have a bit of trouble projecting his voice in large lecture halls. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{Wednesday, May 3, 2017}

\subsection{What is probability?}

He wanted us to think about what it really was. i.e. What does it mean for something to have a 50\% probability? If a coin landing on heads has a 50\% probability, it doesn't guarantee that such an event would occur if you flipped a coin twice.\\
\\
After a few guesses, he gave us a definition.

\subsection{Relative Frequency Definition of Probability}

\begin{tcolorbox}[title=Relative Frequency]

Consider the case where an experiment is performed $n$ times.\\
\\
Let $|A|$ be the number of trials resulting in 'event' $A$.\\
\\
The \textbf{Relative Frequency} of $A = \frac{|A|}{n} = \gamma_n$\\

\end{tcolorbox}

This is the Probability of $A$ when $n$ is large $\gamma_n$ by itself is not an accurate definition of the probability of $A$ occurring, so we take the limit as n goes to infinity and then we have such:

$$\lim_{n\to\infty} \gamma_n$$

This definition is difficult to use in most cases but relatively easy to understand. So here is a definition that is easier to do calculations with:

\subsection{Formal Definition of Probability}

He doesn't actually get into the definition of probability this lecture, instead he goes through a few terms that we need to define before we get to this definition.



	\begin{tcolorbox}[title=Sample Space ($S$)]
	\underline{Sample Space ($S$)}: the set of all possible outcomes in an experiment. Size of $S$ is denoted by $n(S)$, $\#S$ or $|S|$
	\end{tcolorbox}
	\underline{ex.} Tossing a coin once: $S = \{ H,T \}$, $n(S) = 2$\\
	\\
	\underline{ex.} Rolling a 6-sided die: $S = \{ 1,2,3,4,5,6 \}$, $n(S) = 6$\\
	\\
	\underline{ex.} Tossing 2 different coins: $S = \{ HH, HT, TH, TT \}$, $n(S) = 4$


	\begin{tcolorbox}[title=Events] Subsets of a sample space
	\end{tcolorbox}
	\underline{ex.} Experiment rolling a die, $S = \{ 1,2,3,4,5,6 \}$\\
	$A = \{ 1,2 \}$, $A \subseteq S$, so $A$ is an event of $S$.\\
	$B = \{ 5,7 \}$, $B \nsubseteq S$, so $B$ is not an event of $S$.\\
	\\
	You can also describe events with words.\\
	\\
	$C =$ the result is an odd number $= \{ 1,3,5 \}$, $C \subseteq S$, so $C$ is an event of $S$.\\
	\\
	An event can be the entire sample space and the null set.\\
	\\
	$D = S \subseteq S$, so $D$ is an event of $S$.\\
	$E = \varnothing \subseteq S$, so $E$ is an event of $S$.\\


	\underline{Operations on Events} (Unless specified, valid for all events $A,B$)
	\begin{enumerate}
		\item{
		Where $S = \{ 1,2,3,4,5,6 \}$, $A =  \{ 1,2 \}$, $B = \{ 2,4,5 \}$\\
		$A \cup B$ : elements in A or B\\
		\underline{ex.}
		$A \cup B = \{ 1,2,4,5 \}$
		}
		\item{
		Where $S = \{ 1,2,3,4,5,6 \}$, $A =  \{ 1,2 \}$, $B = \{ 2,4,5 \}$\\
		$A \cap B$ : elements in A and B\\
		\underline{ex.} $A \cap B = \{ 2 \}$\\
		And for $C = \{ 5,6 \}$\\
		$A \cap C = \varnothing$\\
		\\
		\begin{tcolorbox}
		\underline{Recall:} So essentially all of the logic laws from CSCA67 hold for these sets as well.\\
		\underline{i.e.}\\
		$A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$\\
		$A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
		\end{tcolorbox}
		}
		\item{
		$(A \cap B)^c = (A^c \cup B^c)$ (where $A^c$ is the complement of $A$, which is the elements in $S$ that is not in $A$)\\
		\underline{ex.}\\
		Consider $A = \{ 1,2 \}$ and $S = \{ 1,2,3,4,5,6 \}$\\
		$A^c = S - A = \{ 3,4,5,6 \}$
		}
		\item{
		$(A \cup B)^c = (A^c \cap B^c)$
		}
		\item{
		$A \cup \varnothing = A$
		}
		\item{
		$A \cap \varnothing = \varnothing$
		}
	\end{enumerate}


	\begin{tcolorbox}[title=Event Space] Is a set of all the possible events of $S$ (\underline{i.e} All the possible subsets of the set $S$). Denoted P
	\end{tcolorbox}
	\underline{ex.}
	The Event Space of the set $\{ 1,2,3 \}$ is\\
	$\{ $\{ 1 \}$,$\{ 2 \}$,$\{ 3 \}$, $\{ 1,2 \}$, $\{ 2,3 \}$, $\{ 1,3 \}$, $\{ 1,2,3 \}$ \}$



\newpage

\section{Friday, May 5, 2017}

\subsection{Formal Definition of Probability}

\begin{tcolorbox}[title= Probability Measure Function (PMF)]

A function $P:S\longmapsto [0,1]$ defined on sample space $S$ is called a \textbf{Probability Measure Function}. It satisfies the following axioms:
\begin{enumerate}
	\item{$0 \geq P(A) \geq 1$}
	\item{$P(S) = 1$ where $S$ is a sample space}
	\item{
	For disjoint events $A_1, ... , A_n$ (disjoint means where $A_i \cap A_k = \varnothing$\\
	$\forall i,k \in\mathbb{N}$ $1 \geq i < k \geq n$)\\
	then $P(A_1) \cup A_2 \cup ... \cup A_n = P(A_1) + P(A_2) + ... + P(A_n)$
	}
	\item{$P(\varnothing) = 0$}
\end{enumerate}
\end{tcolorbox}

\subsection{Properties of Probability Measure Functions}

Dr. Mahinda ends up adding \textbf{Result 1} to the definition of the Probability Measure Function (4) to stay consistent with the book, but also adds it here as well. It will be left again as a property to stay consistent with the lecture and the numbering.

\paragraph{Result 1:} $P(\varnothing) = 0$
\paragraph{Result 2:} If $A_1, ... , A_n$ is a finite collection of disjoint events, then

\begin{align*}
P(\bigcup\limits_{i=1}^n A_i) &= P(A_1 \cup A_2 \cup ... \cup A_n)\\
&= \sum\limits_{i=1}^n P(A_i)
\end{align*}

\begin{proof}

(Proof of Result 2)\\
\\
Define $A_i = \varnothing$, $i \geq n+1$\\
So since there is a finite amount of events, $A_i$ where $i \geq n+1$ are just the null set
$$\bigcup\limits_{i=1}^{\infty} A_i = \bigcup\limits_{i=1}^{n} A_i$$

\begin{align*}
\bigcup\limits_{i=1}^{\infty} A_i &= \sum\limits_{i=1}^{\infty} P(A_i)
\shortintertext{By Axiom 3 of the PMF}
&= \sum\limits_{i=n+1}^{\infty} P(A_i) + \sum\limits_{i=1}^{n} P(A_i)\\
&= \sum\limits_{i=n+1}^{\infty} P(\varnothing) + \sum\limits_{i=1}^{n} P(A_i)\\
&= 0 + \sum\limits_{i=1}^{n} P(A_i)
\shortintertext{By Axiom 4 of the PMF}
&= \sum\limits_{i=1}^{n} P(A_i)
\end{align*}

\end{proof}

\paragraph{Result 3:} If $A_1, A_2, A_3, ...$ is a partition of $S$ and $B$ is any event then $$P(B) = \sum\limits_{i=1}^{\infty} P(A_i \cap B)$$

\begin{tcolorbox}[title=Partitions]
A parition is a collection of events satisfying the following axioms:
\begin{enumerate}
	\item{$A_1, A_2, A_3, ...$ are disjoint events}
	\item{$\bigcup\limits_{i=1}^{\infty} A_i = S$}
\end{enumerate}
\end{tcolorbox}

\begin{proof}
(Proof of Result 3)\\
\\
For an arbitrary event $B$, Consider the partition $A_1, A_2, A_3, ...$ of $S$\\
The events $B \cap A_1, B \cap A_2, B \cap A_3, ...$ are, by the definition of partitions...
\begin{enumerate}
	\item{disjoint}
	\item{$\bigcup\limits_{i=1}^{\infty} B \cap A_i = B \cap S = B$}
\end{enumerate}
Therefore
$$P(B) = P(\bigcup\limits_{i=1}^{\infty} B \cap A_i) = \sum\limits_{i=1}^{\infty} P(A_i \cap B)$$
By Axiom 3 of the PMF
\end{proof}

\begin{tcolorbox}[title=Set Difference]
	For arbitrary sets $A,B$\\
	$A \setminus B$ or $A \cap B^c$ refers to everything in $A$ but not in $B$.
\end{tcolorbox}

\paragraph{Result 4:} $P(A \setminus B) = P(A) - P(A \cap B)$

\begin{proof}
	(Proof of Result 4)\\
	\\
	Note that $A \setminus B$ and $A \cap B$ are disjoint\\
	$P((A \setminus B) \cup (A \cap B)) = P(A \setminus B) + P(A \cap B)$\\
	Since $A \setminus B$ is disjoint from $A \cap B$
	\\
	So for arbitrary events $A,B$\\
	$P(A) = P(A \setminus B) + P(A \cap B) \Longrightarrow P(A \setminus B) = P(A) - P(A \cap B)$
\end{proof}

\underline{Note:} if $B \subseteq A$ then $P(A \setminus B) = P(A) - P(B)$

\begin{proof}
	(Proof of Note)\\
	\\
	if $B \subseteq A$ then $A \cap B = B$\\
	therefore $P(A \setminus B) = P(A) - P(A \cap B) = P(A) - P(B)$
\end{proof}

\paragraph{Result 5:} If $A \subseteq B$ then $P(A) \leq P(B)$\\

\begin{proof}
(Proof of Result 5)
\begin{align*}
	A \subseteq B \Longrightarrow & P(A \setminus B)\\
	= & P(B) - P(A) \geq 0
	\shortintertext{By Axiom 1 of the PMF}
	\Longrightarrow & P(A) \leq P(B)
\end{align*}

\end{proof}

\paragraph{Result 6:} $P(A^c) = 1 - P(A)$

\begin{proof}
	(Proof of Result 6)\\
	Consider an arbitrary event $A \subseteq S$\\
	Since we know that if $A \subseteq S$ then $P(A \setminus B) = P(B) - P(A)$\\
	and that $A^c = S \setminus A$\\
	\begin{align*}
		P(A^c) = P(S \setminus A) &= P(S) - P(A)\\
		&= 1 - P(A)
	\end{align*}
\end{proof}

\paragraph{Result 7:} we know $P(A \cup B) = P(A) + P(B)$ if $A,B$ are disjoint and $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ for any 2 events $A,B$.

\begin{proof}
	(Proof for Result 7)\\
	since we know that $A \cup B = A \cup (B \setminus A)$ and that events $A$ and $(B \setminus A)$ are disjoint.\\
	\begin{align*}
		P(A \cup B) &= P(A \cup (B \setminus A))\\
		&= P(A) + P(B \setminus A)\\
		&= P(A) + P(B) - P(A \cup B)
		\shortintertext{By Result 4}
	\end{align*}
\end{proof}

\underline{Note:} This result can be extended to more than 2 events \underline{e.g.}
$$P(A\cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$$

\textbf{Example 1:} Consider a bag with 3 red marbles and 7 green marbles inside the bag. What is the probability that you would select a red marble from the bag at random? How about the probability of selecting a green marble?\\
\\
\textbf{Solution 1:} Simply consider all possible events, and then for each probability $x$, take all the events where $x$ happens and divide it by the sample space. Let $R,G$ represent the events where a red and green marble respectively is selected.\\
$$P(R) = \frac{R}{R+G} = 0.3$$
$$P(G) = \frac{G}{R+G} = 0.7$$

\textbf{Example 2:} Consider a class of 33 students, 17 of them aced the midterm, 14 of them aced the final and 11 did not have As on either. How many of them aced both the midterm and the final?\\
\\
\textbf{Solution 2:} Let $A,B$ refer to the students who have aced the midterm and final respectively. We are trying to solve for $P(A \cap B)$.\\
$$P(A) = 17/33$$
$$P(B) = 14/33$$
$$P(A \cup B) = 22/33$$
\begin{align*}
	P(A \cup B) &= P(A) + P(B) - P(A \cap B)\\
	22/33 &= 17/33 + 14/33 - P(A \cap B)\\
	22/33 &= 31/33 - P(A \cap B)\\
	22/33 + P(A \cap B) &= 31/33\\
	P(A \cap B) &= 31/33 - 22/33\\
	P(A \cap B) &= 9/33\\
\end{align*}

\subsection{Uniform Probability Measure on Finite Sample Spaces}

\begin{tcolorbox}[title=Uniform Probability Measure on Finite Sample Spaces (UPMFSS)]
	If $S$ is a finite sample space (\underline{i.e.} $|S| < \infty$) of \underline{equally likely} outcomes, and $A$ is any event, then $P(A) = \frac{|A|}{|S|}$
\end{tcolorbox}

But is this sample space a probability measure?

\begin{proof}
	(Proof of UPMFSS being a probability measure)\\
	\begin{enumerate}
		\item{
		\underline{$0 \leq P(A) \leq 1$ for an arbitrary event $A$}
		\begin{align*}
			\varnothing \subseteq & A \subseteq S\\
			|\varnothing| \leq & |A| \leq |S|\\
			0 \leq & \frac{|A|}{|S|} \leq 1
		\end{align*}
		}
		\item{
		\underline{$P(S) = 1$}
		$$P(S) = \frac{|S|}{|S|} = 1$$
		}
		\item{
		\underline{$P(A_1) \cup A_2 \cup ... \cup A_n = P(A_1) + P(A_2) + ... + P(A_n)$}\\
		Let $A_1, A_2, ...$ be disjoint events\\
		Consider $|\bigcup\limits_{i=1}^{\infty} A_i|$
		\begin{align*}
			|\bigcup\limits_{i=1}^{\infty} A_i| &= |A_1| + |A_2| + ...
			\shortintertext{Since the $A_i$'s are disjoint}
			\frac{|\bigcup\limits_{i=1}^{\infty} A_i|}{|S|} &= \frac{|A_1|}{|S|} + \frac{|A_2|}{|S|} + ...\\
			P(|\bigcup\limits_{i=1}^{\infty} A_i|) &= P(A_1) + P(A_2) + ...
		\end{align*}
		}
	\end{enumerate}
\end{proof}

\subsection{Counting (Combinatorics)}

\textbf{Example 1:} How many 4 digit numbers can be formed from digits $1,2,...,9$ if repeats are not allowed?\\
\\
\textbf{Solution 1:} $P(9,4) = \frac{9!}{4!} = 9 \cdot 8 \cdot 7 \cdot 6 \cdot 5$ since there are 9 numbers to choose from and there are 4 digits that these numbers can "occupy".\\
\\
\textbf{Example 2:} Find the probability that the 4-digit number formed from the question above is divisible by 5.\\
\\
\textbf{Solution 2:} $\frac{P(8,3)}{P(9,4)} = \frac{8 \cdot 7 \cdot 6}{9 \cdot 8 \cdot 7 \cdot 6} = \frac{1}{9}$ since there are $P(9,4)$ combinations in total and the numbers divisible by 5 end with a 5 (since it is not possible for the number in the question above to end in a 0). So there are 3 digits left to be assigned a number with only 8 numbers since 5 is already used, so the numerator is $P(8,3)$.\\
\\
\textbf{Counting Subsets:} The number of subsets of size $k$ that can be formed from $n$ objects is $$\frac{n!}{k!(n-k)!} = C(n,k) = \binom{n}{k}$$

\pagebreak

\section{Wednesday, May 10, 2017}

\subsection{Counting (continued)}

\textbf{Example 1:} A professor will select 5 out of 10 questions for an exam, and a student studies the answers to 6 of these 10 questions. What is the probability that all the questions that the professor will select questions are questions that the student has studied for?\\
\\
\textbf{Solution 1:} The sample space $|S| = \binom{10}{5}$ and the cases that all the questions that the professor will select questions that the student has studied for is $\binom{6}{5}$ so the probability is:

$$\frac{\binom{6}{5}}{\binom{10}{5}}$$

\textbf{Example 2:} There are 20 members in a group. We want to divide this group into 3 sub-groups (say, $A, B, C$) of sizes $8,8,4$ respectively. In how many ways can we do this?\\

\textbf{Solution 2:} We choose 8 for $A$, and then 8 from the remaining 12 for $B$ where the order does not matter and the rest go to $C$, so we have.

$$\binom{20}{8} \binom{12}{8} \binom{4}{4} = \frac{20!}{8! 12!} \cdot \frac{12!}{8! 4!} \cdot \frac{4!}{4!} = \frac{12!}{8!8!4!} = \binom{12}{8\:\: 8\:\: 4}$$
\\
\textbf{Example 3:} A deck of 52 cards has 12 picture cards. We want to divide this among 4 players (say, $A, B, C, D$). What is the probability that each player will get exactly 3 picture cards?\\
\\
\textbf{Solution 3:} The sample space $|S| = \binom{52}{13\:\:13\:\:13\:\:13}$ The cases for distributing all of the 12 picture cards between the 4 players is $\binom{12}{3\:\:3\:\:3\:\:3}$ and the cases for distributing all of the 40 regular cards between the players is $\binom{40}{10\:\:10\:\:10\:\:10}$ so the total probability is:

$$\frac{\binom{12}{3\:\:3\:\:3\:\:3} \cdot \binom{40}{10\:\:10\:\:10\:\:10}}{\binom{52}{13\:\:13\:\:13\:\:13}}$$

\subsection{Conditional Probability}

\begin{tcolorbox}[title=Conditional Probability]
	Let $A,B$ be two events such that $P(B) > 0$, then the conditional probability of $A$ given $B$ is:
	$$P(A | B) = \frac{P(A \cap B)}{P(B)}$$
\end{tcolorbox}

\begin{proof}
	(Proof that $P(\cdot | B)$ is a probability measure)\\
	\textbf{Axiom 1}\\
	\begin{align*}
		\varnothing \subseteq A \cap B &\subseteq B\\
		P(\varnothing) \leq P(A \cap B) &\leq P(B)\\
		0 \leq \frac{P(A \cap B}{P(B)} &\leq 1
	\end{align*}
	Therefore $P(\cdot | B)$ satisfies axiom 1.\\
	\\
	\textbf{Axiom 2}\\
	Let $S$ be the sample space, and let $B$ be any event\\
	\begin{align*}
		S \cap B &= B\\
		P(S \cap B) &= P(B)\\
		P(S | B) = \frac{P(S \cap B)}{P(B)} = 1
	\end{align*}
	Therefore $P(\cdot | B)$ satisfies axiom 2.\\
	\\
	\textbf{Axiom 3}\\
	Let $A_1, A_2, ...$ be a collection of disjoint sets\\
	Note that $\forall$ events $B$ that $A_1 \cap B, A_2 \cap B,A_3 \cap B, ...$ are also disjoint.
	\begin{align*}
		P((A_1 \cap B) \cup (A_2 \cap B) \cup (A_3 \cap B) \cup ...) &= P(A_1 \cap B) + P(A_2 \cap B) + P(A_3 \cap B) + ...\\
		&= P((A_1 \cup A_2 \cup A_3 \cup ...) \cap B)\\
		\frac{P((A_1 \cup A_2 \cup A_3 \cup ...) \cap B)}{P(B)} &= \frac{P(A_1 \cap B)}{P(B)} + \frac{P(A_2 \cap B)}{P(B)} + \frac{P(A_3 \cap B)}{P(B)} + ...\\
		P(A_1 \cup A_2 \cup A_3 \cup ...  | B) &= P(A_1 | B) + P(A_2 | B) + P(A_3 | B) + ...
	\end{align*}
	Therefore $P(\cdot | B)$ satisfies axiom 3.\\
	Therefore $P(\cdot | B)$ is a probability measure.
\end{proof}

\textbf{Example 4:} Consider the two events $A,B$ such that\\
$P(A) = 0.5, P(B) = 0.3, P(AB) = 0.1$. Find...
\begin{enumerate}
	\item{$P(A|B) = \frac{P(AB)}{P(B)} = \frac{0.1}{0.3} = \frac{1}{3}$}
	\item{$P(B|A) = \frac{P(AB)}{P(A)} = \frac{0.1}{0.5} = \frac{1}{5}$}
	\item{$P(A|A\cup B) = \frac{P(A\cap(A\cup B))}{P(A \cup B)} = \frac{P(A)}{P(A)+P(B)-P(AB)} = \frac{0.5}{0.5+0.3-0.1} = \frac{5}{7}$}
	\item{$P(A|A\cap B) = \frac{P(A\cap A\cap B)}{P(A\cap B)} = \frac{P(A\cap B)}{P(A\cap B)} =1$}
	\item{$P(AB | A\cup B) = \frac{P(AB \cap (A\cup B))}{P(A \cup B)} = \frac{P(AB)}{P(A\cup B)} = \frac{0.1}{0.5+0.3-0.1} = \frac{1}{7}$}
\end{enumerate}

\newpage

\section{Friday, May 12, 2017}

\subsection{Conditional Probability (continued)}

\textbf{Example 1:} An unbiased coin is flipped twice. What is the probability that both flips result in Heads given that the first flip does?\\
\\
\textbf{Solution 1:} $S = \{ HH, TH, HT, HH \}$\\
$$P(HH | \{ HT, HH \}) = \frac{P(HH \cap \{ HT, HH \})}{P(\{ HT, HH \})} = \frac{P(HH)}{P(\{ HT, HH \})} = \frac{1}{2}$$

\begin{tcolorbox}[title=Theorem: Law of Total Conditional Probability]
	Let $A_1, A_2, ...$ be a partition of $S$ such that $P(A_i) > 0 \forall i$.\\
	Then $\forall$events $B$, $P(B) = \sum\limits_{i=1}^{\infty} P(B | A_i)\cdot P(A_i)$
\end{tcolorbox}

\begin{proof}
	(Proof of the Law of Total Conditional Probability)\\
	$$P(B) = P(B \cap A_1) + P(B \cap A_1) + ...$$
	since we know that $P(B | A_i) = \frac{P(B \cap A_i)}{P(A_i)} \Longleftrightarrow P(B \cap A_i) = P(B | A_i)\cdot P(A_i)$\\
	so now we can conclude that...\\
	$$P(B) = P(B | A_1)\cdot P(A_1) + P(B | A_2)\cdot P(A_2) + ... = \sum\limits_{i=1}^{\infty} P(B | A_i)\cdot P(A_i)$$
\end{proof}

\begin{tcolorbox}[title = Theorem: Bayes' Theorem]
	If $A,B$ are events such that $P(A), P(B) > 0$\\
	then $P(A | B) = \frac{P(A)}{P(B)} \cdot P(B | A)$
\end{tcolorbox}

\begin{proof}
	(Proof of Bayes' Theorem)\\
	$$P(A | B) = \frac{P(A \cap B)}{P(B)}\cdot \frac{P(A)}{P(A)} = \frac{P(A \cap B)}{P(B)}\cdot \frac{P(A)}{P(B)} = \frac{P(A)}{P(B)} \cdot P(B | A)$$
\end{proof}

\textbf{Example 2:} In a population of voters, 40\% are Republican and 60\% are Democrats. 30\% of Republicans and 50\% of Democrats support an election issue. A person is selected at random from this population and he supports the election issue, find the probability that he is a democrat.\\

\textbf{Solution 2:} Let $Su$ be the probability that the person supports the issue.
$$P(R) = 0.4, P(D) = 0.6, P(Su | R) = 0.3, P(Su | D) = 0.5$$
\begin{align*}
	P(Su) &= P(Su | R) \cdot P(R) + P(Su | D) \cdot P(D)\\
	&= (0.3)(0.4) + (0.5)(0.6)\\
	&= 0.42
\end{align*}
$$P(D | Su) = P(Su | D) \cdot \frac{P(D)}{P(Su)} = 0.5 \cdot \frac{0.6}{0.42} = \frac{0.3}{0.42}$$

\begin{tcolorbox}[title= Definition: Independent Events]
	Two events $A,B$ are independent $(A \perp B)$ if $P(A \cap B) = P(A) \cdot P(B)$\\
	\underline{Weaker Version:} If $P(B) > 0$ then $A \perp B$ if $P(A | B) = P(A)$
\end{tcolorbox}

\textbf{Example 3:} Consider the experiment where you roll a 6-sided die:\\
Let $A$ be the events where the result is an odd number on the die. Let $B$ be the events where the result is an even number on the die. Let $C = \{ 1,2 \}$
\begin{enumerate}
	\item{Are $A$ and $B$ independent?\\
	$P(A) = P(B) = \frac{1}{2}$\\
	$AB = \varnothing$\\
	$P(AB) = 0 \neq P(A) \cdot P(B)$\\
	No.}
	\item{Are $A$ and $C$ independent?\\
	$AC = \{ 1 \}\\
	P(AC) = \frac{1}{6} = P(A) \cdot P(C)\\
	Yes.$}
\end{enumerate}

\textbf{Result 1:} $A \perp B$ then $A \perp B^c$
\begin{proof} (Proof of Result 1)\\
	Assume $A \perp B$.\\
	Consider $P(A \cap B^c)$
	\begin{align*}
		P(A \cap B^c) &= P(A \setminus B)\\
		&= P(A) - P(AB)\\
		&= P(A) - P(A) \cdot P(B)\\
		&= P(A) \cdot (1 - P(B))\\
		&= P(A) \cdot P(B^c)
	\end{align*}
	Therefore $A \perp B \Longrightarrow A \perp B^c$
\end{proof}

\begin{tcolorbox}[title=Definition: General Definition of Independent Events]
	The events $A_1, A_2, ...$ are independent if for every subcollection of events $A_i1, A_i2, ... A_ik$ where a subcollection is a subset of a collection of events.
\end{tcolorbox}

\textbf{Example 4:} Consider the experiment where a fair 4 sided die is rolled, and Let $A = \{ 1,2 \}$, $B = \{ 1,3 \}$, $C = \{ 1,4 \}$

Are A,B,C pairwise independent? Are they independent?
\\
\textbf{Solution 4:} We know $P(A) = P(B) = P(C) = 1/2$\\
So are $A$ and $B$ independent?
$$P(AB) = P(\{ 1 \}) = 1/4 = P(A)\cdot P(B)$$
Yes, so are $A$ and $C$ independent?
$$P(AC) = P(\{ 1 \}) = 1/4 = P(A)\cdot P(C)$$
Yes, so are $B$ and $C$ independent?
$$P(BC) = P(\{ 1 \}) = 1/4 = P(B)\cdot P(C)$$
Yes, so $A, B, C$ are pairwise independent.\\
Are $A,B,C$ independent?
$P(A\cap B \cap C) = 1 \neq P(A) \cdot P(B) \cdot P(C) = 1/8$
So no, they are not independent.

\subsection{Continuity of $P$}

\begin{tcolorbox}[title=Definition: Continuous]
	A function $f$ [Domain$(f) \subseteq \mathbb{R}$] is continuous at $x_0$ if for every sequence $\{x_n\}$ such that if $$\lim_{n\to\infty} x_n = x_0$$ then $$\lim_{n\to\infty} f(x_n) = f(x_0)$$
\end{tcolorbox}

so $$\lim_{n\to\infty} f(x_n) = f(\lim_{n\to\infty} x_n)$$

\begin{tcolorbox}[title=Definition: Increase$\backslash$Decrease to]
We say that $\{A_n\}$ increases to $A$ (write $\{A_n\}\uparrow A$) if\\
$A_1 \subseteq A_2 \subseteq ... \subseteq A$ ; $\bigcup\limits_{n=1}^{\infty} A_n = A$ ; $\lim_{n\to\infty} A_n = A$\\
\\
We say that $\{A_n\}$ decreases to $A$ (write $\{A_n\}\downarrow A$) if\\
$A_1 \supseteq A_2 \supseteq ... \supseteq A$ ; $\bigcap\limits_{n=1}^{\infty} A_n = A$ ; $\lim_{n\to\infty} A_n = A$
\end{tcolorbox}

\begin{tcolorbox}[title=Theorem: Continuous Function]
$P$ is a continuous function if $\{A_n\}\uparrow A$ or $\{A_n\}\downarrow A$ then $P(A) = \lim_{n\to\infty} P(A_n)$
\end{tcolorbox}

\begin{proof} (Proof of the Continuous Function Theorem)\\
	\textbf{Case 1:} $\{A_n\}\uparrow A$\\
	Define $A_2, A_3, ..., A$ as\\
	$$A_2 = A_1 \cup (A_2 \setminus A_1)$$
	$$A_3 = A_1 \cup (A_2 \setminus A_1) \cup (A_3 \setminus A_2)$$
	$$...$$
	$$A_ = A_1 \cup (A_2 \setminus A_1) \cup (A_3 \setminus A_2) \cup ...$$
	Consider $P(A)$
	\begin{align*}
		P(A) &= P(A_1 \cup (A_2 \setminus A_1) \cup (A_3 \setminus A_2) \cup ...)\\
		&= P(A_1) + P(A_2 \setminus A_1) + P(A_3 \setminus A_2) + ...\\
		&= P(A_1) + \sum\limits_{i=2}^{\infty} P(A_i \setminus A_{i-1})\\
		&= P(A_1) + \lim_{n\to\infty} \sum\limits_{i=2}^{n} P(A_i \setminus A_{i-1})\\
		&= P(A_1) + \lim_{n\to\infty} \sum\limits_{i=2}^{n} [P(A_i) - P(A_{i-1})]
		\shortintertext{Notice that the series is telescoping}
		&= P(A_1) - P(A_1) + \lim_{n\to\infty} P(A_n)\\
		&= \lim_{n\to\infty} P(A_n)
	\end{align*}
	
	\textbf{Case 2:} $\{A_n\}\downarrow A$\\
	if $\{A_n\}\downarrow A$ then $\{A_n^c\}\uparrow A^c$\\
	\underline{i.e.} $P(A^c) = \lim_{n\to\infty} P(A_n^c)$\\
	Consider $(1 - P(A))$\\
	\begin{align*}
		(1 - P(A)) &= 1 - [\lim_{n\to\infty} P(A_n^c)]\\
		- P(A) &= - \lim_{n\to\infty} P(A_n^c)\\
		P(A) &= \lim_{n\to\infty} P(A_n^c)
	\end{align*}
\end{proof}

\textbf{Example 5:} Suppose $P([0,\frac{8}{4+n})) = \frac{2 + e^{-n}}{6}$ find $P(\{ 0 \})$\\\\
\textbf{Solution 5:} $\{A_n\}\uparrow 0$\\
\begin{align*}
	P(\{ 0 \}) &= \lim_{n\to\infty} P(A_n)\\
	&= \frac{2 + e^{-n}}{6}\\
	&= \frac{1}{3}
\end{align*}

\section{Wednesday, May 17, 2017}

\subsection{Random Variables}

\begin{tcolorbox}[title=Definition: Random Variables]
	A random variable $X$ is a function from $S$ to the set of real numbers ($\mathbb{R}$)
\end{tcolorbox}

\textbf{Example 1:} Consider the experiment where you roll a die ($S = \{ 1,2,3,4,5,6 \}$).\\
\\
Define $\forall s\in S$ that $X$ as $X(s) = s$. So $X(5) = 5$, $X(2) = 2$, and etc.
\\
\\
\textbf{Example 2:} The Indicator Function\\
$$X(s) =
\begin{cases}
	1, & s \in A\\
	0, & \text{otherwise}
\end{cases}
$$

\subsection{Distribution of a Random Variable}

\begin{tcolorbox}
	For random variable $X$ and $B \subseteq S$, $X \in B = \{ s \in S | X(S) \in B \}$. Note that $X \in B \subseteq S$ is an event.
\end{tcolorbox}

\begin{tcolorbox}[title=Definition: The Distribution of a Random Variable $X$]
	The collection of probabilities $P(x \in B)$, $\forall$ subset of $\mathbb{R}$ is called the \textbf{Distribution of $X$}.
\end{tcolorbox}

\textbf{Example 3:} Consider $S = \{ \text{Clear, Sunny, Rain} \}$ and $P(\{ \text{Clear} \}) = 0.5$, $P(\{ \text{Sunny} \}) = 0.2$, $P(\{ \text{Rain} \}) = 0.3$.\\
\\
And define $X(\text{Clear}) = 200$, $X(\text{Sunny}) = 100$, $X(\text{Rain}) = -50$.\\
\\
Find the distribution of $X$.\\
$P(x \in B) = (0.5) \cdot I_B (200) + (0.2) \cdot I_B (100) + (0.5) \cdot I_B (-50)$ for any $B \in \mathbb{R}$\\
$P(X \in \{ -50, 100, 200 \}) = 0.5 + 0.2 + 0.3 = 1.0$

\subsection{Discrete Random Variables}

\begin{tcolorbox}[title=Definition: Discrete Random Variables]
	A random variable $X$ is discrete of there is a finite or countable set of real numbers $x_1, x_2, ...$ and a corresponding sequence of non-negative real numbers $p_1, p_2, ...$ such that
	$$P(X = x_i) = p_i, \forall i, and \sum_{i} p_i = 1$$
\end{tcolorbox}

\begin{tcolorbox}[title=Probability Mass Function]
	The Probability Mass Function of a discrete random variable $X$ is the function $P_X : \mathbb{R} \rightarrow [ 0,1 ]$ defined by $P_X (x) = P(X = x)$. Note: $$P(X \in A) = \sum_{x \in A} P(X = x) = \sum_{x \in A} P_X (x)$$
\end{tcolorbox}

\subsection{Important Discrete Distributions}
\begin{enumerate}
	\item{
	\textbf{Degenerate (Point Mass) Distribution}\\
	A random variable $X$ is $S$ and has a degenerate distribution if
	$$P_X (x) = 
	\begin{cases}
	1, & x=c\\
	0, & x\neq x
\end{cases}$$
	}
	\item{
	\textbf{Bernouelli Distribution}\\
	A random variable $X$ is said to have a Bernoulli Distribution $(X \sim \text{Ber}(\theta))$ if\\
	\\
	$P_X (1) = P(X = 1) = \theta$ while $P_X (0) = P(X = 0) = 1-\theta$
	}
	\item{
	\textbf{Binomial Distribution}\\
	A random variable $X$ is said to have a binomial distribution with parameters $n$ and $\theta$ if
	$$P_X (x) =
	\begin{cases} 
		\binom{n}{x} \theta^x (1-\theta)^{n-x} & \text{ for } x = 0,1,2,...,n\\
		0 & \text{ otherwise }
	\end{cases}$$
	
	\textbf{Example 4:} Toss a coin $n$ times ($P(H) = \theta$)\\
Let $X$ be the number of heads\\
$$P_X (x) = P(X = x) = \binom{n}{x} \theta^x (1-\theta)^{n-x}$$

\underline{Binomial Experiment must satisfy the following conditions}
\begin{enumerate}
	\item{A fixed number of trials $(n)$}
	\item{Trials must be independent}
	\item{Outcomes must fall into 2 categories (either success or failure)}
	\item{Probability of a success should be the same for all trials}
\end{enumerate}

\begin{tcolorbox}[title=Note]
	\begin{enumerate}
		\item{$P_X (x) \geq 0$}
		\item{$\sum_{x=0}^n P_X (x) = 1$}
	\end{enumerate}
\end{tcolorbox}

\begin{proof}
	(Proof of (2.) in Note)\\
	\begin{align*}
		\sum_{x=0}^n P_X (x) &= \sum_{x=0}^n \binom{n}{x} \theta^x (1-\theta)^{n-x}\\
		&= (\theta + 1 - \theta)^n = 1^n = 1
	\end{align*}
	\underline{Recall that} $$(a+b)^n = \sum_{x=0}^n a^x b^{n-x}$$
\end{proof}
	
	}
	\item{
	\textbf{Geometric Distribution}\\
	A random variable $X$ is said to have a geometric distribution with parameter ($\theta \in (0,1)$) if $P_X (x) = (1 - \theta)^x \cdot \theta$ for $x = 0,1,2,...$\\
	\\
	\underline{Note:} $P_X (x) \geq 0$ $\forall x$, but is it a Probability Measure Function?\\
	\begin{proof}
	(Proof of axiom 3 and 2 of a PMF)\\
	\begin{align*}
		\sum_{n=0}^{\infty} P_X (x) &= \sum_{n=0}^{\infty} (1 - \theta)^x \theta\\
		&= \theta \sum_{n=0}^{\infty} \gamma^x (\text{ where } \gamma = 1 - \theta)\\
		&= \theta \frac{1}{1 - \gamma}\\
		&=\theta \frac{1}{\theta}\\
		&= 1
	\end{align*}
	\end{proof}
	}
	\item{
	\textbf{Negative Binomial Distribution}\\
	We need $r$ successes and where $x$ is the failures before the $r$th success
	$$P(X = x) = \binom{x+r - 1}{x} (1 - \theta)^x \theta^{r-1} \theta$$
	
	A random variable $X$ is said to have a negative binomial distribution with parameters $r$ and $\theta$ where $\theta \in (0,1)$ if\\
	$$P_X (x) = 
	\begin{cases}
		\binom{x+r - 1}{x} (1 - \theta)^x \theta^{r} \theta & x = 0,1,2,...\\
		0 & \text{otherwise}
	\end{cases}$$
	}
\end{enumerate}



\end{document}