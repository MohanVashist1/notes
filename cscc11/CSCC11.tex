\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{graphicx}
\graphicspath{ {imgs/} }

\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage{textcomp}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}
 
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Machine Learning -- Winter 2020}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}


\begin{document}

\title{CSCC11: Machine Learning\\ Lecture Notes}
\date{University of Toronto Scarborough -- Winter 2020}
\author{Joshua Concon}
\maketitle
Instructor is Dr. David Fleet. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{LEC 1: Tuesday, January 7, 2020}

\subsection{Machine Learning Definitions}

\paragraph{AI View} Automatic Learning
\paragraph{Software Engineering View} Can be fine-tuned
\paragraph{Statistics View} Machine Learning is fast Statistics

\subsection*{Machine Learning methods are broken into 2 phases}
\paragraph{Training} a model is learned from a collection of training data
\paragraph{Application} the model is used to make decisions about some new test data

\subsection*{Types of Machine Learning}
\begin{itemize}
  \item Supervised Learning: The training data is labelled with the correct answers
  \begin{itemize}
    \item Classification: Outputs are discrete labels
    \item Regression: Outputs are real-valued
  \end{itemize}
  \item Unsupervized Learning: Deriving patterns and structure from unlabelled data
  \item Reinforcement Learning: An agent seeks to learn optimal actions to take based on state of the world, and learn from the consequences of past actions
\end{itemize}

\subsection{A Simple Problem}
Consider a problem where the goal is to fit a 1D curve. There are many curves that may fit.
\\
\\
Using Machine Learning requires us to make certain choices:

\begin{enumerate}
  \item How do we parameterize the curve? (linear, quadratic, sine...)
  \item What criteria (objective function) do we use to judge the quality of the fit?
  \item How long are we willing to wait for a solution (or can we use approximations)?
  \item How do we prevent overfitting?
\end{enumerate}

\textbf{Overfitting} is when a model fits the training data well, but performs poorly on test data.

Properties of the input that are not relevant to the task at hand are called \textbf{Noise}.

\section{LEC 2: Thursday, January 9, 2020}

\subsection{Linear Regression}

Our goal is to learn a mapping $y=f(x), x\in\mathbb{R}, y\in\mathbb{R}$ and $f$ is a linear function $(y = w x + b)$ where $w$ is the weight and $b$ is the bias.
\\
\\
We want to estimate $w$ and $b$ from $N$ training pairs

$$\{(x_i, y_i)\}^N_{i=1}$$

Then once we have $w$ and $b$, we can predict outputs $y$ for some new inputs $x$.
\\
\\
When $N > 2$ we are looking for a line that is as close to the training points as possible. So we ultimately want to minimize the errors between the data points and our hypothetical model

$$\min y_i - (wx_i + b)$$

The most common way to estimate is \underline{least-squares regression}.
\\
\\
First, we define the error of the $i$th training point as

$$e_i = y_i - (wx_i +b)$$

The sum of the squared errors over all training points is then used to measure the quality of the fit.\\
\\
This is called the \textbf{Objective Function, Energy Function,} or \textbf{Imperial Loss}.

$$E(w,b) = \sum^N_{i=1} e_i^2 = \sum^N_{i=1} (y_i - (wx_i + b))^2$$

The results in an error function is 0 iff $\forall e_i, e_i = 0$ and the results are always non-negative.
\\
\\
Finding the line that minimum square error is equivalent to solving for the $w$ and $b$ that minimize $E(w,b)$. This can be solved by setting the derivatives of $E$ with respect to these parameters to 0.

\begin{align*}
     \frac{\partial E}{\partial b} &= -2 \sum_i (y_i - (wx_i + b)) = 0\\
     b^* &=  \frac{\sum_i y_i}{N} - w \frac{\sum_i x_i}{N} = \hat{y} - w \hat{x}
\end{align*}

Where $\hat{x}$ and $\hat{y}$ are the averages of $x$ and $y$ respectively. $b^*$ still depends on $w$. We get the following after substitution.

\begin{align*}
    E(w,b) &= \sum_i ((y_i - \hat{y}) - w (x_i - \hat{x}))^2\\
    \frac{\partial E}{\partial w} &= -2 \sum_i ((y_i - \hat{y}) - w(x_i - \hat{x}))(x_i - \hat{x})\\
    \text{and if we set $\frac{\partial E}{\partial w} = 0$ then we get}\\
    w^* &= \frac{\sum_i (y_i - \hat{y})(x_i - \hat{x})}{\sum_i (x_i - \hat{x})^2}
\end{align*}

The values $w^*$ and $b^*$ are the least squares estimates for the parameters of the linear regression.

\subsection{Multi-Dimensional Inputs}

To learn a mapping from $D$-dimensional inputs to scalar outputs ($x\in\mathbb{R}^D, y \in\mathbb{R}$)

$$f(x) = w^T x + b = \sum^D_{j=1} w_j x_j + b$$

For convenience, we can fold $b$ into the weight vector.

$$\tilde{w} = \begin{bmatrix}
   w_{1} \\
   \vdots \\
   w_{D}\\
   b
 \end{bmatrix}, \tilde{x} = \begin{bmatrix}
   x_{1} \\
   \vdots \\
   x_{D} \\
   1
 \end{bmatrix}$$
 
Then we can write the mapping as such

$$f(x) = \tilde{w}^T \tilde{x}$$

Given $N$ training input-output pairs, the least-squares objective function is

$$E(\tilde{w}) = \sum^N_{i=1} (y_i - \tilde{w}^T \tilde{x_i})^2$$

If we stack the outputs in a vector and the inputs in a matrix, we can also write this as:

$$E(\tilde{w}) = || y - \tilde{X}\tilde{w}||^2 \text{ where } y = \begin{bmatrix}
   y_{1} \\
   \vdots \\
   y_{N}\\
 \end{bmatrix}, \tilde{X} = \begin{bmatrix}
   x^T_{1} & 1 \\
   \vdots & \vdots \\
   x^T_{N} & 1\\
 \end{bmatrix}$$
 
We can also rewrite as such

$$E(\tilde{w}) = (y - \tilde{X}\tilde{w})^T (y - \tilde{X}\tilde{w}) = \tilde{w}^T\tilde{X}^T\tilde{X}\tilde{w} - 2y^T\tilde{X}\tilde{w} + y^T y$$

We can optimize this by setting all values of $\frac{\partial E}{\partial w_i} = 0$ and solving the resulting system of equations

$$w^* = (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T y$$


\underline{Note:} $\tilde{X}^+ \equiv (\tilde{X}^T \tilde{X})^{-1} \tilde{X}^T$ is the pseudoinverse of $\tilde{X}$

So we can represent $w^*$ as such

$$w^* = \tilde{X}^+ y$$

\subsection{Multi-Dimensional Outputs}

With multi-dimensional inputs and outputs, we have a case that handles that.
\\
\\
So for $D$ inputs and $K$ outputs we get

$$y = \tilde{W}^T \tilde{x} \text{ where } y\in\mathbb{R}^K, \tilde{W} \in\mathbb{R}^{(D+1) \times K}$$
$$\text{and where } \tilde{W} = [\tilde{w}_1,...,\tilde{w}_K] = \begin{bmatrix}
    w_1 & \dots & w_k\\
    b_1 & \dots & b_k
\end{bmatrix}$$
$$\text{and where } w_i \in\mathbb{R}^D, i\in [1,k], i\in\mathbb{Z}, x \in \mathbb{R}^{(D+1)}$$

We can then express the mapping from the input $\tilde{x}$ to the $j$th element of $y$ as

$$y_j = \tilde{w}_j^T x$$

Given $N$ training samples denoted $\{ \tilde{x}_i, y_i \}^N_{i=1}$, then squared error of $\tilde{W}$

$$E(\tilde{W}) = \sum^N_{i=1} \sum^K_{j=1} (y_{i,j} - \tilde{w}_j^T \tilde{x}_i)^2$$

$$\text{where $y_{i,j}$ is the $j$th index of the $y_i$ array}$$

\underline{Note:} Euclidean Norm of $v$ is defined as $||v|| = \sqrt{\sum_i v_i^2}$
\\
\\
With the euclidean norm, we can also express $E$ as the sum over output dimensions.

\begin{align*}
    E(\tilde{w}) &= \sum^N_{i=1} \sum^K_{j=1} (y_{i,j} - \tilde{w}_j^T\tilde{x}_i)^2\\
    &= \sum^K_{j=1} ||y_j'- \tilde{X}\tilde{w}_j ||^2\\
    \text{where } & y_j' \in\mathbb{R}^N, y_j' = [y_{1,j},\dots, y_{N,j}]^T\\
                  & y' \in\mathbb{R}^{N \times K}, y' = [y_1',\dots, y_k']\\
                  & \tilde{X} = \begin{bmatrix}
  \tilde{x}_{1} \\
   \vdots \\
   \tilde{x}_{N}\\
 \end{bmatrix}
\end{align*}

You can also stack everything into a matrix equation.

$$E(\tilde{W}) = ||Y - \tilde{X}\tilde{W}||^2_F \text{ where } Y = [y_1',\dots, y_K']$$

\underline{Note:} The Frobenius norm of $Y$ is
$$||Y||_F = \sqrt{\sum_{i,j} Y^2_{i,j}}$$

\section{LEC 3: Tuesday, January 14, 2020}

A common choice for the function $f(x)$ is a basis function representation:

$$y = f(X) = \sum_k w_k b_k (x)$$

\end{document}