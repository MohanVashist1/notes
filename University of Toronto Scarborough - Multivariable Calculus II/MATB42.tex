\documentclass[12pt]{article}

\usepackage{upgreek}

\usepackage{amsmath}

\usepackage{ esint }

\usepackage{amsthm}

\usepackage{graphicx}

\graphicspath{ {imgs/} }

\usepackage{dsfont}

\usepackage{mathtools}

\usepackage{hyperref}

\usepackage[utf8]{inputenc}

\usepackage{mathtools}

\usepackage{textcomp}

\usepackage[english]{babel}

\usepackage{tikz}

\usepackage{tcolorbox}

\usepackage{amsthm,amssymb}

\setlength{\parindent}{0cm}

\renewcommand\qedsymbol{$\blacksquare$}

\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Multivariable Calculus II -- Winter 2018}
\fancyhead[RE,LO]{Joshua Concon}
\fancyfoot[CE,CO]{\leftmark}
\fancyfoot[LE,RO]{\thepage}

\theoremstyle{plain}

\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{property}[theorem]{Property}

\begin{document}

\title{MATB42: Multivariable Calculus II\\ Lecture Notes}
\date{University of Toronto Scarborough -- Winter 2018}
\author{Joshua Concon}
\maketitle
Pre-reqs are MATB41. Instructor is Eric Moore. If you find any problems in these notes, feel free to contact me at conconjoshua@gmail.com.

\tableofcontents

\pagebreak

\section{Friday, January 5, 2018}

\subsection{Fourier Expansions}

In this section, we will focus on single variable calculus, (so where $f:\mathbb{R} \mapsto \mathbb{R}$)\\
\\
Let us say that we have a function $f(x)$ and we want to approximate it. We can use an $n$th degree Taylor Polynomial, but this requires that $f(x)$ has at least $n$ derivatives at some point $x_0$ and the $k$th derivative of $f$ ($f^{(k)} (x)$) is determined by properties of $f$ in some neighbourhood of $x_0$, but what about outside this neighbourhood? How can we be certain of the approximation outside of this neighbour.\\
\\
Our problem here is that Taylor Polynomial may only approximate "near" $x_0$\\
\\
Now, consider the following function:

$$\Delta (x) = \begin{cases}
1, &\lfloor x \rfloor < x, \lfloor x \rfloor\text{ is odd} \\
0, &\lfloor x \rfloor < x, \lfloor x \rfloor\text{ is even}\\
\end{cases}
$$

In this function, Tayler returns either 0 or 1 depending on your choice of $x_0$ and cannot work for an $x_0 = p \in \mathbb{Z}$. Therefore Taylor polynomials cannot reflect the true nature of this function. Taylor provides a "local" approximation, but we want a "global" approximation. We need an approximation that is more precise over an interval at the cost of being not as precise as precise at any particular $x_0$.\\
\\
Note that the example function is \textbf{periodic}.\\
\\

\begin{definition}
	A function $y=f(x)$ such that $f(x)=f(x+p), p \neq 0, \forall x$ is said to be \textbf{periodic} of period $p$
\end{definition}

\begin{example}
	The periodic function $\Delta (x)$ is of period 2.
\end{example}

What we want is a global approximation of a periodic function, and the Fourier Approximation will be periodic, so we can use it for exactly that.

\begin{definition}
	A \textbf{trigonometric polynomial of degree $N$} is an expression of the form
	$$\frac{a_0}{2}+ \sum^N_{k-1} a_k cos(kx) + b_k sin(kx)$$ where the $a_i, b_i$ are constants.
\end{definition}

We know that $sin(x)$ and $cos(x)$ are the simplest periodic functions and repeat in intervals of $2\pi$, so $cos(kx)$ and $sin(kx)$ have period $\frac{2\pi}{k}$, but the smallest shared period is $2\pi$. If a trigonometric polynomial has period $2\pi$ and $f(x)$ has period $p$, then we must set $x=\frac{pt}{2\pi}$ to fix the period (where $t$ is a variable).\\
\\
So to approximate $y=f(x)$ by $F_N (x)$ for some $N$, we use the following equation:

$$F_N (x) = \frac{a_0}{2}+ \sum^N_{k=1} a_k cos(kx) + b_k sin(kx)$$

Now we need to choose the $a_k, b_k$. We can define it in the following way:

$$a_0 = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) dx$$
$$a_k = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) cos(kx) dx, k=1,2,3,...$$
$$b_k = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) sin(kx) dx, k=1,2,3,...$$

When defined in this way, $a_i, b_i$ are called the \textbf{Fourier Coefficients} of $f$ over the interval $[-\pi, \pi]$ and we call $F_N (x)$ the \textbf{Fourier Polynomial of degree $N$}.\\
\\
So why do we add the $\frac{a_0}{2}$? It is the average value of $f$ over $[-\pi, \pi]$.

\begin{note}
	sometimes you will see $a_0$ used instead of $\frac{a_0}{2}$ in the Fourier polynomial where $$a_0 = \frac{1}{2\pi} \int^{\pi}_{-\pi} f(x) dx$$
\end{note}

\begin{example}
	Consider $f(x)=\frac{-x}{2}$ over $[-\pi, \pi]$. Use Fourier Approximation.\\
	\\
	$$a_k = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) cos(kx) dx = \frac{1}{\pi} \int^{\pi}_{-\pi} (-\frac{x}{2}) cos(kx) dx \overset{odd}{=} 0$$
	$$a_0 = \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) dx = \frac{1}{\pi} \int^{\pi}_{-\pi} (-\frac{x}{2}) dx \overset{odd}{=} 0$$
	\begin{align*}
		b_k &= \frac{1}{\pi} \int^{\pi}_{-\pi} f(x) sin(kx) dx\\
		&= \frac{1}{\pi} \int^{\pi}_{-\pi} (-\frac{x}{2}) sin(kx) dx\\
		&\overset{even}{=} - \frac{1}{\pi} \int^{\pi}_{-\pi}  x sin(kx) dx\\
		&\underset{u=x, dv=sin(kx)dx}{\overset{even}{=}} - \frac{1}{\pi} [-\frac{1}{k} x cos(kx) + \frac{1}{k^2} sin(kx)]^\pi_0\\
		&= \frac{1}{\pi k} [\pi cos(k\pi)]\\
		&= \frac{1}{k} cos(k\pi)\\
		&= \frac{(-1)^k}{k}
	\end{align*}

	Thus we have:
	$$F_N (x) = -sin(x) + \frac{1}{2}sin(2x) - \frac{1}{3}sin(3x) + \frac{1}{4}sin(4x) + ...$$
	$$F_1 (x) = -sin(x)$$
	$$F_2 (x) = -sin(x) + \frac{1}{2}sin(2x)$$
	$$F_3 (x) = -sin(x) + \frac{1}{2}sin(2x) - \frac{1}{3}sin(3x)$$
	$$...$$
	And so on.

\end{example}

\newpage

\section{Monday, January 8, 2018}

continuing from the last lecture...

\begin{example}
    (continued from example 1.4)\\
    $f(x) = \frac{-x}{2}$\\
    $F_N (x) = -sin(x) + \frac{1}{2}sin(2x) - \frac{1}{3}sin(3x) + \frac{1}{4}sin(4x) + ...$\\
    \\
    This can be extended to a Fourier Series:
    $$F_N (x) = \sum^\infty_{k=1} (-1)^k \frac{sin(kx)}{k}$$
\end{example}

\begin{definition}
    For $f:\mathbb{R}\mapsto\mathbb{R}$, the Fourier Series for $f$ is
    $$F(x) = \frac{a_0}{2}+ \sum^\infty_{k=1} a_k cos(kx) + b_k sin(kx)$$
    where $a_i, b_i$ are Fourier coefficients.
\end{definition}

The $N$th degree Fourier Polynomial can be regarded as the $N$th partial sum of the series.\\
\\
We haven't talked about convergence yet, but for now, we will assume the series converges $(f(x)=F(x))$

\begin{definition}
    Function $a_k cos(kx) + b_k sin(kx)$ is the $k$th harmonic of $f$. The Fourier Series expresses $f$ in terms of its harmonics.
\end{definition}

\begin{note}
    (Looking at Harmonics in a Musical Sense):\\
    the $1$st harmonic is the fundamental harmonic of $f$ (the fundamental tone).\\
    The $2$nd harmonic is the first overtone.
\end{note}

(completely rewrite this amplitude section)

\begin{definition}
    The amplitude of the $k$th harmonic is
    $$A_k = \sqrt{(a_k)^2 + (b_k)^2}$$
    And note that
    $$a_k = A_k sin\alpha, b_k = A_k cos\alpha$$
\end{definition}

\begin{definition}
    The energy $E$ of a periodic function $f$ of period $2\pi$ is
    $$E = \frac{1}{\pi} \int^\pi_{-\pi} [f(x)]^2 dx$$
\end{definition}

So the energy of the $k$th harmonic is
$$E = \frac{1}{\pi} \int^\pi_{-\pi} [a_k cos(kx) + b_k sin(kx)]^2 dx = (a_k)^2 + (b_k)^2 = (A_k)^2$$

And the energy of the constant term is

$$\frac{1}{\pi} \int^\pi_{-\pi} [a_0]^2 dx = 2(a_0)^2$$
So we put $A_0 = \frac{1}{\sqrt{2}} a_0$.\\
\\
For a "nice" periodic function, we have the following equation:

$$E = A_0^2 + A_1^2 + A_2^2 + ...$$
This is known as the Energy Theorem, and comes from the study of periodic waves.\\
\\
We can draw a graph of this as $A_k^2$ against $k$ (This graph is known as the Energy Spectrum of $f$). It shows how the energy of $f$ is distributed among its harmonics.\\
\\
\begin{note}
    Notice that
    $$E = \frac{1}{\pi} \int^\pi_{-\pi} [f(x)]^2 dx = \frac{a_0^2}{2}+ \sum^\infty_{k=1} (a_k^2 + b_k^2) \text{ Parseval's Equation}$$
\end{note}

Assume a function $f$ of period $2\pi$ is the sum of a trigonometric series

$$f(x) = \frac{a_0}{2}+ \sum^\infty_{k=1} (a_k cos(kx) + b_k sin(kx)) \text{ on the interval } [-\pi,\pi]$$

Multiply by $cos(mx)$ and integrate to get
\begin{align*}
    \int^\pi_{-\pi} f(x)cos(mx) dx &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx + \int^\pi_{-\pi} [\sum^\infty_{k=1} (a_k cos(kx) + b_k sin(kx))]dx\\
    &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx +  \sum^\infty_{k=1} (a_k \int^\pi_{-\pi}cos(kx)dx + b_k \int^\pi_{-\pi}sin(kx)dx)
\end{align*}

\begin{note}
    Recall the following trigonometric identities:
    \begin{enumerate}
        \item $cosAcosB = \frac{1}{2} [cos(A+B) + cos(A-B)]$
        \item $cosAsinB = \frac{1}{2} [sin(A+B) + sin(A-B)]$
        \item $sinAsinB = \frac{1}{2} [cos(A-B) - cos(A+B)]$
    \end{enumerate}
\end{note}

\newpage

\section{Friday, January 12, 2018}

Continuing from where we left off.

\begin{align*}
    \int^\pi_{-\pi} f(x)cos(mx) dx &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx + \int^\pi_{-\pi} [\sum^\infty_{k=1} (a_k cos(kx) + b_k sin(kx))]dx\\
    &= \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx +  \sum^\infty_{k=1} (a_k \int^\pi_{-\pi}cos(kx)dx + b_k \int^\pi_{-\pi}sin(kx)dx)
\end{align*}

We know the following from trigonometric identities
$$\int^\pi_{-\pi} cos(kx)cos(mx) dx = \begin{cases}
    0, &k\neq m\\
    \pi, &k=m
\end{cases}$$
As well as the following from odd function properties
$$\int^\pi_{-\pi} cos(kx) dx = 0$$
$$\int^\pi_{-\pi} sin(kx)cos(mx) dx = 0$$
So now we get
\begin{align*}
    \int^\pi_{-\pi} f(x)cos(mx) dx
    &=  \frac{a_0^2}{2} \int^\pi_{-\pi} cos(mx) dx +  \sum^\infty_{k=1} (a_k \int^\pi_{-\pi}cos(kx)dx + b_k \int^\pi_{-\pi}sin(kx)dx)\\
    &= a_m \pi\\
    &\implies a_m = \frac{1}{\pi} \int^\pi_{-\pi} f(x)cos(mx) dx
\end{align*}

\begin{example}
    Lets take
    $$f(x) = \begin{cases}
    1, &0\leq x < \pi\\
    -1, &-\pi \leq x < 0
    \end{cases}$$

    Note that this is an odd function, therefore $a_k = 0, \forall k \geq 0$. So now lets calculate $b_k$.

    \begin{align*}
        b_k &= \frac{1}{\pi} \int^\pi_{-\pi} f(x)sin(kx) dx\\
        &\overset{even}{=} \frac{2}{\pi} \int^\pi_{0} f(x)sin(kx) dx\\
        &= \frac{2}{\pi} \int^\pi_{0} sin(kx) dx\\
        &= \frac{2}{k\pi} [ -cos(kx) ]^\pi_0\\
        &= \begin{cases}
            \frac{4}{k\pi}, &\text{$k$ is odd}\\
            0, &\text{$k$ is even}
        \end{cases}
    \end{align*}

    And now lets right out the Fourier Polynomial $(F_N (x))$\\
    \underline{if $N$ is odd:}
    $$F_N (x) = \frac{4}{\pi}sin(x) + \frac{4}{3\pi}sin(3x) + \frac{4}{5\pi}sin(5x) + ...$$
    \underline{if $N$ is even:}
    $$F_N (x) = F_{N-1} (x)$$
    We can also write it as a Fourier Series
    $$F(x) = \frac{4}{\pi} \sum^\infty_{l=0} \frac{sin((2l+1)x)}{2l+1}$$

    The energy of the function is
    $$E = \frac{1}{\pi} \int^\pi_{-\pi} [f(x)]^2 dx = \frac{2}{\pi} \int^\pi_0 dx = 2$$

    The amplitutde of the $k$th harmonic is
    $$A_k = \sqrt{a_k^2 + b_k^2} = \sqrt{0 + \frac{16}{k^2\pi^2}} = \frac{4}{k\pi}$$

    The energy of the $k$th harmonic is
    $$A_k^2 = \frac{16}{k^2 \pi^2}$$

    Note that for this example, both the energy and the amplitude are 0 at an even $k$.\\
    \\
    Lets now evaluate the energy spectrum:
    $$k=1, E= \frac{16}{\pi^2} \approx 1.62, \frac{1.62}{2}=0.81=81\%$$
    $$k=3, E= \frac{16}{9\pi^2} \approx 0.18, \frac{0.18}{2}=0.09=9\%$$
    $$k=5, E= \frac{16}{25\pi^2} \approx 0.06, \frac{0.06}{2}=0.03=3\%$$
    $$k=7, E= \frac{16}{49\pi^2} \approx 0.03, \frac{0.03}{2}=0.015=1.5\%$$

\end{example}

However, we do not need to exclusively work with the interval $[-\pi, \pi]$ we can even work over any interval of length $2\pi$.

\subsection{General Fourier Series (interval of length $2\pi$)}

$$a_k = \frac{1}{\pi} \int^{c+2\pi}_{c} f(x) cos(kx) dx, k=1,2,3,...$$
$$b_k = \frac{1}{\pi} \int^{c+2\pi}_{c} f(x) sin(kx) dx, k=1,2,3,...$$

What about for $f$ if $f$ has period $p$?

$$f(x+p) = f(x), \forall x, \exists p \neq 0$$

We then substitute $x = \frac{pt}{2\pi}$ which gives a new function $f_p (t) = f(\frac{pt}{2\pi})$ with period $2\pi$. So

$$f_p (t+2\pi) = f(\frac{p}{2\pi} (t+2\pi)) = f(\frac{pt}{2\pi} + p) = f(\frac{pt}{2\pi}) = f_p (t)$$

So how about the Fourier Expansion for $f_p (t)$? To find this, we must replace $t$ by $\frac{2\pi x}{p}$ giving for $f(x)$.

$$F(x) = \frac{a_0}{2}+ \sum^\infty_{k-1} a_k cos(\frac{2nx\pi}{p}) + b_k sin(\frac{2nx\pi}{p})$$
$$a_k = \frac{2}{p} \int^{c+p}_{c} f(x) cos(\frac{2nx\pi}{p}x) dx, k=1,2,3,...$$
$$b_k = \frac{2}{p} \int^{c+p}_{c} f(x) sin(\frac{2nx\pi}{p}x) dx, k=1,2,3,...$$

For any function defined on $[a,b]$, we can extend $f$ to all of $\mathbb{R}$ as a periodic function. Given a periodic function $f_E$ from $f$ of period $p=b-a$, we now have:

$$a_k = \frac{2}{b-a} \int^{b}_{a} f(x) cos(\frac{2nx\pi}{b-a}x) dx, k=1,2,3,...$$
$$b_k = \frac{2}{b-a} \int^{b}_{a} f(x) sin(\frac{2nx\pi}{b-a}x) dx, k=1,2,3,...$$

\begin{example}
    Take the function $f(x) = x, 0\leq x < 1$ and extend it with period 1. For $k\neq 0$:
    \begin{align*}
        a_k &= 2 \int^{1}_{0} xcos(2k\pi x) dx\\
        &\overset{parts}{\underset{u=v, dv=cos(2\pi kx)dx}{=}} 2[\frac{xsin(2k\pi x)}{2\pi k}]^1_0 - \frac{2}{2k \pi} \int^1_0 sin(2k\pi x) dx\\
        &=0
    \end{align*}
    $$a_0 = 2 \int^1_0 x dx = [x^2 ]^1_0$$
    \begin{align*}
        b_k &= 2\int^1_0 x sin(2k\pi x)dx\\
        &\underset{u=x, dv=sin(2k\pi x)dx}{\overset{parts}{=}} 2[\frac{-xcos(2k\pi x)}{2\pi k}]^1_0 + \frac{1}{k\pi} \int^1_0 cos(2k\pi x)dx\\
        &= \frac{-cos(2k\pi)}{k\pi}\\
        &= \frac{-1}{k\pi}
    \end{align*}
    So the Fourier Series will be
    $$F(x) = \frac{1}{2} - \frac{1}{\pi} [sin(2\pi x) + \frac{sin(4\pi x)}{2} + \frac{sin(6\pi x)}{3} + ...]$$
\end{example}

\begin{example}
    $f(x) = |x|, -\pi < x \leq \pi$. Since $f(x)$ is even, $b_k=0, \forall k\in\mathbb{N}$.

    $$a_0 = \frac{1}{\pi} \int^\pi_{-\pi} |x|dx \overset{even}{=} \frac{2}{\pi} \int^\pi_0 x dx= \pi$$

   \begin{align*}
   	a_k &= \frac{1}{\pi} \int^\pi_{-\pi} |x| cos(kx) dx\\
	&\overset{even}{=} \frac{2}{pi} \int^\pi_0 x cos(kx) dx\\
	&\underset{u=x, dv=cos(kx)dx}{\overset{parts}{=}} \frac{2}{\pi} [\frac{x sin(kx)}{k} + \frac{cos(kx)}{k^2}]^\pi_0\\
	&= \frac{2}{\pi k^2} (cos(k\pi) - 1)\\
	&=\begin{cases}
		0, &\text{$k$ is even}\\
		\frac{-4}{\pi k^2}, &\text{$k$ is odd}
	\end{cases}
   \end{align*}

   So we end up with the Fourier Series

   $$F(x) = \frac{\pi}{2} - \frac{4}{\pi} \sum^\infty_{l=0} \frac{cos((2l+1)x)}{(2l+1)^2}$$

\end{example}

\begin{example}
	$f(x) = x, -\pi < x \leq \pi$. Note that since $f(x)$ is odd, $a_k = 0, \forall k \geq 0$
	\begin{align*}
		b_k &= \frac{1}{\pi} \int^\pi_{-\pi} x sin(kx) dx\\
		&\underset{u=x, dv=sin(kx)dx}{\overset{parts}{=}} \frac{1}{\pi} [\frac{-xcos(kx)}{k}]^\pi_{-\pi} + \frac{1}{\pi} \int^\pi_{-\pi} cos(kx) dx\\
		&= \frac{1}{\pi} [\frac{-\pi cos(k\pi)}{k} + \frac{-\pi cos(-k\pi)}{k}]\\
		&= \frac{-2}{k} cos(k\pi)\\
		&= \frac{(-1)^{k+1} 2}{k}
	\end{align*}
	And the Fourier Series of this is
	$$F(x) = \sum^\infty_{k=1} (-1)^{k+1} \frac{2}{k} sin(kx)$$
\end{example}

To get a Fourier cosine series or a Fourier sine series, we need
\begin{itemize}
	\item{an $f$ defined on the interval $[0,a]$, and we must extend this interval to also include $[-a,0)$ to give an even or odd function on $[-a,a]$}
	\item{$f(-t)=f(t), -a\leq t < 0$ for the even extension}
	item{$f(-t)=-f(t), -a\leq t < 0$ for the odd extension}
\end{itemize}

\newpage

\section{Monday, January 15, 2018}

\begin{example}
	We want to express $f(x)=x, 0\leq x < \pi$ as both a cosine series and a sine series.\\
	\\
	\underline{cosine series}
	For this, we need to extend the function as an even function, so we extend the function to $f(x)=|x|, -\pi < x \leq x$. This is a previous example that we computated before and gives us a cosine series\\
	\\
	\underline{sine series}
	For this, we must extend $f(x)$ as an odd function $f(x)=x, -\pi \geq x < \pi$. We've already seen from previous examples that is a sine series.\\
	\\
	So for $f(x)=x, 0 \leq x < pi$, the cosine series looks like this:
	$$F(x) = \frac{\pi}{2} - \frac{4}{\pi} \sum^\infty_{l=0} \frac{cos((2l+1)x)}{(2l+1)^2}$$
	And the sine series looks like this:
	$$F(x) = \sum^\infty_{k=0} (-1)^{k+1} \frac{2}{k} sin(kx)$$
	Both of these series on the interval $[0,\pi)$ represent $f(x)=x$
\end{example}

\begin{definition}
	A function $f(x)$ defined for $x\in[a,b]$ is \textbf{piece-wise continuous} if there exists a finite partition $P: a=t_0< .... < t_n =b$ such that $f$ is continuous on $x\in(t_{i=1}, t_i), \forall i$ and both $\lim_{x\to t^+_{i-1}} f(x)$ and $\lim_{x\to t^-_{i-1}} f(x)$ both exist and are both finite.
\end{definition}

\begin{note}
	On the $i$th subinterval, $f(x)$ coincides with some $f_i (x)$ that is continuous on that subinterval.
\end{note}

\begin{definition}
	If $f_i (x) \forall i$ has continuous 1st derivatives, $f(x)$ is called \textbf{piecewise smooth}
\end{definition}

\begin{definition}
	If $f_i (x) \forall i$ has continuous 2nd derivatives, $f(x)$ is called \textbf{piecewise very smooth}
\end{definition}

\begin{definition}
	The Fourier Series obtained from $f(x)$ converges to $f(x)$ if $f(x) = \lim_{N\to\infty} F_N (x)$

	\underline{i.e.} $$f(x) = \lim_{N\to\infty} F_N (x) = \frac{a_0}{2}+ \sum^N_{k-1} a_k cos(kx) + b_k sin(kx)$$

	assuming a period of $2\pi$ (and can be adjusted for other periods). The $a_k$ and $b_k$ are the Fourier coefficients.
\end{definition}

\begin{theorem}
	Let $f(x)$ be continuous and piece-wise very smooth for all $x$ and let $f(x)$ have period $2\pi$. Then the Fourier Series of $f(x)$ converges uniformly to $f(x), \forall x$
\end{theorem}

This helps with examples that have jump discontinuities.

\begin{theorem}
	Let $f(x)$ be defined and piece-wise very smooth for $x\in[-\pi,\pi]$ and let $f(x)$ be defined outside this interval to have period $2\pi$. Then the Fourier Series of $f(x)$ converges uniformly to $f(x)$ in each interval containing no discontinuity of $f(x)$. At each discontinuity, $x_0$, the series converges to
	$$\frac{1}{2} [\lim_{x\to x_0^+} f(x) + \lim_{x\to x_0^-} f(x)]$$
	This is the \textbf{Fundamental Theorem (for Fourier Series)}. The Theorem can be reinstated for $f$ defined on any interval of length $p\neq 0$
\end{theorem}

\begin{example}
	\begin{enumerate}
		\item $f(x) = \frac{-x}{2}$ on the interval $[-\pi, \pi]$
		\item $f(x) = \begin{cases}
			1, &0\leq x < \pi\\
			-1, &-\pi\leq x < 0
		\end{cases}
$
	\end{enumerate}
	Note that they both are piece-wise continuous and both satisfy the previous theorem.\\
	\\
	so (1.) converges to
	$$\sum^\infty_{k=1} \frac{(-1)}{k} sin(kx) = \begin{cases}
		-\frac{x}{2}, &x\in (-\pi, \pi)\\
		0, &x = \pm \pi
	\end{cases}
$$

And (2.) converges to

$$\sum^\infty_{k=1} \frac{sin((2k+1)x)}{2k+1} = \begin{cases}
		-1, &-\pi < x < 0\\
		1, &0 < x < \pi\\
		0, &x = -\pi, 0,\pi
	\end{cases}
$$

In (2.) set $x = \frac{\pi}{2}$ and we'll get
\begin{align*}
	1 &= \frac{4}{\pi}\sum^\infty_{k=1} \frac{sin((2k+1)\frac{\pi}{2})}{2k+1}\\
	1 &=\frac{4}{\pi}\sum^\infty_{k=1} \frac{(-1)^k}{2k+1}\\
	\frac{\pi}{4}&= \sum^\infty_{k=1} \frac{(-1)^k}{2k+1}\\
	\frac{\pi}{4}&=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+...
\end{align*}

So now we have $\frac{\pi}{4}$ represented by a series of numbers.

\end{example}

However, the domain of the function also play a role in the series.

\begin{example}
	For $f(x), x\in[0,1)$, we get
	$$F(x) = \frac{1}{2} - \frac{1}{\pi}\sum^\infty_{k=1} \frac{sin((2k+1)x)}{2k+1}$$
	The theorem still applies and we get $F(x) = x$ on the interval $[0,1)$, and converge to $\frac{1}{2}$ at $0$.
\end{example}

\begin{example}
	For $f(x)=|x|, x\in [-\pi,\pi]$ we get.
	$$F(x) = \frac{\pi}{2} - \frac{1}{\pi}\sum^\infty_{k=1} \frac{cos((2k+1)x)}{(2k+1)^2}$$
	on $(0,1), |x|=x$, and since $f$ is piece-wise very smooth, we can write:
	$$x=\frac{\pi}{2} - \frac{1}{\pi}\sum^\infty_{k=1} \frac{cos((2k+1)x)}{(2k+1)^2}$$
\end{example}

\section{Friday, January 19, 2018}

\begin{definition}
	The \textbf{orthonormal bases for $\mathbb{R}^n$}
	$$\{ e_1, ..., e_n \}$$
	are all orthogonal to each other and are all unit vectors.\\
	\\
	Each $v\in \mathbb{R}^n$ has a unique representation of
	$v = \lambda_1 e_1 + ... + \lambda_n e_n$ where $\lambda_k = v \cdot e_k$
\end{definition}

Given continuous functions $f,g$ that map from $[-\pi, \pi] \mapsto \mathbb{R}$ we can define an inner product by
$$<f,g> = frac{1}{\pi} \int^{\pi}_{-\pi} f(x) g(x) dx$$ defined on all continues real-valued functions defined on the interval $[-\pi, \pi] $

\begin{example}
	The set of functions $\{ cos(kx), sin(nx) \}_{n,k\in\mathbb{Z}}$ act like an orthonormal basis. since

	$$<cos(mx), cos(nx)> = \begin{cases}1, &\text{ if } m=n\\ 0, &\text{ otherwise} \end{cases}$$
	$$<sin(mx), sin(nx)> = \begin{cases}1, &\text{ if } m=n\\ 0, &\text{ otherwise} \end{cases}$$
	$$<sin(mx), cos(nx)> = 0$$

	for positive integers $m,n$
\end{example}

We can regard the Fourier Coefficients as the components of $f$ under this basis. This explains why you dont need to recalculate the coefficients for each $N$ for $F_N (x)$

\begin{corollary}
	If a function $f$ can be represented as a trigonometric polynomial, then that trigonometric polynomial is a Fourier Expansion of $f$, if the Fourier Series converges.
\end{corollary}

\begin{example}
	What is the fourier series of $f(x)=cos^2(x) - sin^2(x)$\\
	\\
	We have trigonometric identities to solve this:
	$$F(x)=cos^2(x) - sin^2(x)=cos(2x)$$
\end{example}

\begin{example}
	\begin{align*}
	f(x) &=cos^4(x)\\
	&=(cos^2(x))^2\\
	&=(cos^2(x))^2\\
	&= \frac{1}{4}(1+2cos(2x) + cos^2 (2x))\\
	&= \frac{1}{4}(1 + 2cos(2x) + \frac{1}{2} + \frac{1}{2}cos(4x))
	&= \frac{3}{8} + \frac{cos(2x)}{2} + \frac{1}{8}cos(4x) = F(x)
	\end{align*}
\end{example}

\begin{definition}
	The \textbf{Total Square Error} of $g(x)$ relative to $f(x)$ is:
	$$E = \int^\pi_{-\pi} [f(x) - g(x)]^2 dx$$
\end{definition}

\begin{note}
	if $f=g, E=0$
\end{note}

We want a constant function $y=g_0$ so the square error is as small as possible

\begin{align*}
	E(g_0) &= \int^\pi_{-\pi} [f(x) - g_0]^2 dx\\
	&= \int^\pi_{-\pi} f(x)^2 dx - 2 g_0 \int^\pi_{-\pi} f(x) dx + g_0^2 (2\pi)
\end{align*}

And if we let $A,B$ be constants such that $$A = \int^\pi_{-\pi} f(x)^2 dx, B = \int^\pi_{-\pi} f(x) dx$$

Then
$$E(g_0) = A - 2 B g_0 + 2 \pi g_0^2$$

With implies that $E(g_0)$ is a quad function in $g$ having a minimum value when its derivative is equal to 0.

$$-2B + 4\pi g_0 = 0$$

$$g_0 = \frac{B}{2\pi} = \frac{1}{2\pi} \int^\pi_{-\pi} f(x) dx = \frac{1}{2} a_0$$

Therefore $\frac{1}{2} a_0$ is the best constant approximation in the sense of the square error. We can show that the best approximation by a trigonometric polynomial is the Fourier Polynomial.

\begin{definition}
	If $f$ is piece-wise continuous on $[-\pi, \pi]$ we see that

	$$\frac{a_0}{2}+ \sum^N_{k=1} a_k^2 + b_k^2 \leq \frac{1}{\pi} \int_{-\pi}^\pi [f(x)]^2 dx = <f,f>$$
	This is called \textbf{Bessel's Inequality} and it shows that $\sum^\infty_{k=1} a_k^2 + b_k^2$ converges
\end{definition}

\begin{theorem}
(Uniqueness Theorem) Let $f(x), g(x)$ be piecewise continuous functions on the interval $[-\pi,\pi]$ and have all the same same Fourier coefficients. Then $f(x)=g(x)$ except, perhaps, at discontinuities.
\end{theorem}

\subsection{Vector-Valued Functions}

We will now spend time with vector-valued functions ($f:\mathbb{R}^n \mapsto A$)

\begin{definition}
	A \textbf{curve (or path)} in $\mathbb{R}^n$ is a function
	$$\gamma : [a,b] \subset \mathbb{R} \mapsto \mathbb{R}^n$$
	We usually call the image of $\gamma$ the curve and the function $\gamma$ as the parameterization of the curve
\end{definition}

\begin{note}
	This curve has a direction given by $\gamma$ and runs from $\gamma (a)$ to $\gamma (b)$, which are the endpoints. $\gamma (a)$ being the beginning and $\gamma (b)$ being the end
\end{note}

Think of $t, t\in [a,b]$ as a variable and $\gamma (t)$ as "tracing out" the curve in $\mathbb{R}$ as $t$ goes from $a$ to $b$.

This interval be adapted to any $(a,b)$ and even to $\mathbb{R}$ itself.

$$\gamma (t) = (\gamma_1 (t), \gamma_2 (t), ..., \gamma_n (t)), t\in[a,b]$$

\begin{example}
	where $t \in \mathbb{R}$
	\begin{align*}
		\gamma (t) &= (x_0, y_0, z_0) + t(v_1,v_2,v_3)\\
		&= x + tv
	\end{align*}
	This is a parametric representation of a line. Where $x$ is a point and $v$ is a direction vector

\end{example}

\begin{definition}
	$\gamma$ is \textbf{continuous} at $c\in (a,b)$ if $\lim_{t\to c} \gamma (t) = \gamma (c)$ iff the components $\gamma_i (t)$ for $i=1,...,n$ are continuous at $c$
\end{definition}

\begin{definition}
	If $\gamma$ is continuous at all points, $\gamma$ is called a \textbf{Continuous Path}
\end{definition}

\begin{example}
	Consider a circle of radius $3$ in $\mathbb{R}^2$ centred around the origin. A path of this circle is
	$$\gamma (t) = (3cost, 3sint), t \in [0,2\pi]$$
	We say this curve is oriented in the counter clockwise direction
\end{example}

\begin{example}
	The curve for $y=x^3 +1$ is
	$$\gamma (t) = (t, t^3 +1), t\in\mathbb{R}$$
\end{example}

\begin{example}
	In $\mathbb{R}^3, \gamma (t) = (3cost, 3sint, t) , t \in [0,2\pi]$.\\
	\\
	Since $x^2 + y^2 = 9$, this curve must live in the cylinder $x^2 + y^2 = 9$ and since $z=t$, the curve spirals upwards, this is called a helix
\end{example}

\begin{example}
	$\gamma (t)= (t, t^2, t^3), t\in [-2,2]$. This is a twisted cubic.
\end{example}

\begin{example}
	Find the parameterization of the curve $C$ of the intersection of the cylinder $x^2 + y^2 = 1$ and the plane $y+z = 2$. The projection into the $x-y$ plane is the circle $x^2 + y^2 = 1, z=0$, so

	$$x=cost, y=sint$$

	From the equation of the plane
	$$z = 2-y = 2-sint$$
	We now parameterize the curve $C$ by
	$$\gamma (t) = (cost, sint, 2-sint), t\in [0,2\pi]$$
\end{example}

\section{Monday, January 22, 2018}

\subsection{parameterization (continued)}

\begin{note}
	For any given curve, there may be several possible parameterizations of that curve
\end{note}

\begin{example}

\end{example} The $1$st quadrant piece of the unit circle in a counter clock-wise direction, from $(1,0)$ to $(0,1)$

$$\gamma_1 (t) = (cost, sint), t\in [0, \frac{\pi}{2}]$$
$$\gamma_2 (t) = (cos(\frac{t}{2}), sin(\frac{t}{2})), t\in [0, \pi]$$
$$\gamma_3 (t) = (\sqrt{1-t^2}, t), t\in [0, 1]$$
All parameterize the same curve.

\subsection{Derivatives}

\begin{definition}
	The \textbf{Derivative of a path} is
	$$D(\gamma(t)) = (\gamma_1'(t), \gamma_2'(t), ... , \gamma_n'(t))$$
	provided each $\gamma_i'(t)$ exists for $t=1,...,n$\\
	\\
	We usually write $\gamma' (t) = (\gamma_1'(t),...,\gamma_n'(t))$
\end{definition}

\begin{definition}
	If $\gamma'(t)$ exists, $\gamma$ is called a \textbf{differentiable path}
\end{definition}

\begin{definition}
	If $\gamma$ is a differentiable path with a continuous derivative, except at finitely many places, the image of $\gamma$ is called a \textbf{piece-wise smooth curve}
\end{definition}

\begin{definition}
	$\gamma'(t)$ is the tanget $v$ to the curve at the point $\gamma (t)$. Hence, the tanget line at $\gamma (t_0)$ is given by
	$$\gamma (t_0) + \lambda \gamma ' (t_0), \lambda \in \mathbb{R}$$
\end{definition}

Note that $\gamma (t)$ must be smooth (class $C^1$) for $t$ near $t_0$, be careful if $\gamma (t_0) = 0$

\begin{remark}
	If we think of $\gamma (t)$ representing the position of a particle at time $t$, we can regard $\gamma ' (t)$ as its velocity and $\gamma '' (t)$ as its acceleration.\\
	\\
	If $\gamma ' (t)$ is the velocity, then $||\gamma ' (t)||$ is the speed (magnitude of the velocity)
\end{remark}

\begin{example}
	Computing the period of a satellite when the radius of its orbit is known. The particle has constant speed $s$, and when $t=0$, assume that $\gamma (t)$ is at the point $(r,0)$.\\
	\\
	So....
	$$\gamma(t) = (rcos(kt), rsin(kt))$$
	We need to find $k$.
	$$\gamma ' (t) = (-rksin(kt), rkcos(kt))$$
	$$s = ||\gamma ' (t)|| = \sqrt{r^2 k^2 (sin^2 (kt) + cos^2 (kt))} = rk$$
	And this implies that $k = \frac{s}{r}$.\\
	\\
	Note that $r,k \in \mathbb{R}^+ > 0$

	$$\gamma '' (t) = (-rk^2 cos(kt), -rk^2 sin(kt)) = -k^2 \gamma (t)$$

	Assuming particle is a satellite of mass $m$ orbiting the earth of mass $M$.

	\begin{note}
		$$F = ma = m(\gamma '' (t)) = -mk^2 \gamma (t)$$
		$$F(\vec{v}) = \frac{-G m M \vec{v}}{||\vec{v}||^3}$$
		where $G$ is the gravitational constant and $\vec{v} = \gamma (t)$
	\end{note}

	$$-mk^2 \gamma (t) = \frac{-G m M \gamma (t)}{r^3}$$

	And this implies that $k^2 = \frac{GM}{r^3}$.\\
	\\
	What is the period? $p = \frac{2\pi}{k}$ this implies that $k = \frac{2\pi}{p}$.

	$$k^2 = (\frac{2\pi}{p})^2 = \frac{GM}{r^3}$$
	This implies that
	$$p^2 = \frac{4 \pi^2}{GM} r^3$$

	The period of a satellite squared is proportional to the radius of its orbit cubed (One of Kepler's Laws)

\end{example}

\subsection{Path Integrals}

\begin{definition}
	The \textbf{path integral of $f$} or the \textbf{integral of $f$ along the path $\gamma$} denoted
	$$\int_\gamma f ds$$
	is defined by
	$$\int_\gamma f ds = \int^b_a f(\gamma(t)) ||\gamma ' (t) || dt$$
	Whenever $\gamma: [a,b]\mapsto \mathbb{R}^n$ is a smooth curve and the composite function $t \mapsto f(\gamma(t))$ is continuous on $[a,b]$
\end{definition}

If $\gamma (t)$ is only piece-wise smooth or $f(\gamma (t))$ is only piece-wise continuous, we break $\gamma (t)$ into finitely many smooth pieces, and sum the integrals over the various pieces.

\section{Friday, January 26, 2018}

\subsection{Path Integral (continued)}

Why is this definition of a path integral a good definition?\\
\\
We can think of speed $s=||\gamma ' (t)||$ as a rate of change in position relative to change in time $t$. Often we denote $ds=||\gamma ' (t)||dt$\\
\\
If $f=1$ then
$$\int_\gamma ds = \int_a^b ||\gamma ' (t)|| ds$$
Which is just the length of the curve.

\begin{definition}
	\textbf{The length of the curve} is $$\int_\gamma ds = \int_a^b ||\gamma ' (t)|| ds$$
\end{definition}

The length of the path is the "total distance travelled."

\begin{example}
	Find the length of a circle of radius 1 from 0 to $2\pi$.\\
	\\
	$$\gamma (t) = (cos(t), sin(t)), t\in [0, 2\pi]$$
	$$\gamma ' (t) = (-sin(t), cos(t))$$
	$$||\gamma ' (t)|| = \sqrt{sin^2(t) + cos^2(t)} = 1$$


	$$s = \int_\gamma ds = \int_0^{2\pi} 1 ds = 2\pi$$
\end{example}

\begin{example}
	Find the arclength of $\gamma (t) = (sin(5t), cos(5t), \frac{10}{3} t^{1.5})$ from $t=0$ to $t=3$.\\
	\\
	$$\gamma ' (t) = (5cost, -5sint, 5 \sqrt{t})$$

	$$||\gamma ' (t)|| = \sqrt{25cos^2 (t) + 25sin^2 (t), 25 t} = 5\sqrt{1+t}$$

	$$s = \int_\gamma ds = 5 \int_0^3 \sqrt{1+t} dt = \frac{70}{3}$$
\end{example}

\begin{example}
	$f(x,y,z) = xy-z^2$ along $\gamma (t) = (3t, -2t, \sqrt{3}t), t\in [0,1]$\\
	\\
	$$\gamma ' (t) = (3, -2, \sqrt{3})$$
	$$||\gamma ' (t)|| = 4$$
	\begin{align*}
		f(\gamma (t)) &= f(3t, -2t, \sqrt{3}t)\\
		&=(3t)(-2t) - (\sqrt{3}t)^2\\
		&= -6t^2 - 3t^2 = -9t^2
	\end{align*}

	$$\int_\gamma f ds = \int_0^1 f(\gamma (t))||\gamma ' (t)||dt = \int_0^1 (-9t^2)4dt = -36 \int_0^1 t^2 dt = -12$$

\end{example}

\begin{remark}
	mass of a wire $(m)$ $=$ density $D$ $\times$ length
	$$m = \int_\gamma dm = \int_\gamma D ds$$
\end{remark}

\begin{note}
	$\int_\gamma f ds$ is independent of the choice of parameterization
\end{note}

\begin{note}
	If a curve in $\mathbb{R}^2$ is given by $y=f(x)$ and $x$ is in the domain of $[a,b]$, then the arclength is $$\int_a^b \sqrt{1+f'(x)^2} dx$$
\end{note}

\subsection{Vector Fields}

\begin{definition}
	A function $F:A \subset \mathbb{R}^n \mapsto \mathbb{R}^n$ is called a \textbf{vector field} where
	$$F(x) = (F_1(x), ..., F_n(x))$$
\end{definition}

\begin{definition}
	a function $f: \mathbb{R}^n \mapsto \mathbb{R}$ is called a \textbf{scalar field}, the components of a vector field are scalar fields.
\end{definition}

We think of a vector field $F$ as attaching to each point $x$ a vector $f(x)$

\begin{example}
	Newtons Gravitational Law\\
	\\
	$$F(x) =  -\frac{mMG}{||x||^3} x$$
	"pull of a satellite towards the earth"\\
	\\
	The strength of the force depends on $d$, the distance away. Force is in the opposite direction to the position vector.

	$$F(x) = F(x,y,z) = (- \frac{mMG}{||(x,y,z)||^3}x, - \frac{mMG}{||(x,y,z)||^3}y, - \frac{mMG}{||(x,y,z)||^3}z)$$

\end{example}

\begin{example}
	Put a toy boat into a stream, a velocity field is the velocity at each point of the stream.\\
	\\
	Let the path be $\gamma (t)$, then $\gamma ' (t) = F(\gamma (t)), \forall t$
\end{example}

\begin{definition}
	If $F$ is a vector field, a \textbf{flowline} for $F$ is a path $\gamma (t)$ such that $\gamma ' (t) = F(\gamma (t))$
\end{definition}

\begin{definition}
	The \textbf{flow of a vector field} is the family of all flow lines
\end{definition}

\begin{example}
	$F(x,y) = (3,4)$, vector $(3,4)$ at every point will be straight lines. Want to know flow line at $(1,2)$

	$$\gamma (t) = (x(t), y(t))$$
	$$\gamma ' (t) = F(\gamma (t)) = (3,4)$$
	$$\gamma (t) = (3t + 1, 4t+2)$$
	as it passes through $(1,2)$.
\end{example}

\newpage

\section{Monday, January 29, 2018}

\subsection{Line Integrals}

\begin{example}
	For time $t$, force $F = F(\gamma(t)) \cdot \frac{\gamma ' (t)}{||\gamma ' (t)||} = F(\gamma (t)) \cdot T (\gamma (t))$

	$$\Delta w = F(\gamma(t)) \cdot T (\gamma (t)) \Delta s$$

	$$w= \int_\gamma F \cdot T ds$$

\end{example}

\begin{definition}
	Let $F$ be a vector field in $\mathbb{R}^n$ and let $\gamma : [a,b] \mapsto \mathbb{R}^n$ be a smooth curve in $\mathbb{R}^n$. The \textbf{line integral} of $F$ over $\gamma$, denoted $\int_\gamma F ds$ is defined by
	$$\int_\gamma F \cdot ds = \int_\gamma ( F \cdot T) ds$$
\end{definition}

\begin{remark}
	$(F \cdot T)$ is a scalar field, and as produced before:
	$$\int_\gamma ( F \cdot T) ds = \int_a^b F(\gamma(t)) \cdot \gamma ' (t) dt$$
\end{remark}

\begin{remark}
	Within signs, the line integral is independent of the parameterization. ie.
	$$\int_\gamma F \cdot ds = - \int_{-\gamma} F \cdot ds$$
\end{remark}

\begin{remark}
	We can work over piecewise smooth curves by dividing it into smooth sub intervals and summing them up.
\end{remark}

\begin{example}
	$F(x,y) = (y,x^2), \gamma (t) = (t^2, t^3), t\in[0,1]$
	$$F(\gamma (t)) = (t^3, t^4)$$
	$$\gamma ' (t) = (2t, 3t^2)$$

	\begin{align*}
		\int_\gamma F \cdot ds &= \int_0^1 (2t^4 + 3t^6) dt\\
		&= [\frac{2}{5} t^5 + \frac{3}{7} t^7]^1_0\\
		&= \frac{2}{5}+ \frac{3}{7} = \frac{29}{35}
	\end{align*}

\end{example}

\begin{example}
	$$F(x,y) = (2y-x, -y)$$
	$$\gamma (t) = (-cost, sint), t\in[0,\pi]$$
	$$\gamma_0 (t) = (cost, sint), t\in[0,\pi]$$

	\begin{align*}
		\int_\gamma F ds &= \int_0^\pi F(\gamma(t)) \cdot \gamma ' (t) dt\\
		&= \int_0^\pi (2sin^2 t + costsint - sint cost) dt\\
		&= 2\int_0^\pi (\frac{1}{2}-\frac{1}{2}cos(2t))dt = \pi
	\end{align*}

	\begin{align*}
		\int_{\gamma_0} F ds &= \int_0^\pi F(\gamma_0 (t)) \cdot \gamma_0 ' (t) dt\\
		&= \int_0^\pi (-2sin^2 t + costsint - sint cost) dt\\
		&= -2\int_0^\pi (\frac{1}{2}-\frac{1}{2}cos(2t))dt = -\pi
	\end{align*}

\end{example}

\begin{note}
	Notice from the previous example that
	$$\int_\gamma F ds = \int_0^\pi (2sint + cost)(sint)-(sint)(cost)$$
	we notice this is in the form
	$$A = \int_a^b F_1(\gamma(t)) \gamma_1 '(t) + ... + F_n(\gamma(t)) \gamma_n '(t) dt $$
	We introduce a new notation that can take advantage of this form and rewrite $A$ as
	$$\int_a^b F_1 dx_1 + ... + F_n dx_n $$
	where $dx_i = \gamma ' (t)dt, i=1,...,n$
\end{note}

\begin{definition}
	A (first-order) \textbf{differential form} (1-form) in $\mathbb{R}^n$ is an expression of the form
	$$f_1 dx_1 + ... + f_n dx_n$$
\end{definition}

\begin{definition}
	Let $\omega = f_1 dx_1 + ... + f_n dx_n$ be a differential form in $\mathbb{R}^n$ and let $\gamma : [a,b] \mapsto \mathbb{R}^n$ be a smooth curve in $\mathbb{R}^n$. The integral of $\omega$ over $\gamma$, denoted $$\int_\gamma \omega$$ is defined by
	$$\int_\gamma \omega = \int_\gamma F \cdot ds$$
	where $F = (f_1, ..., f_n)$
\end{definition}

So

\begin{align*}
	\int_\gamma \omega &= \int_a^b [f_1 (\gamma(t)) \gamma_1 ' (t) + ... + f_n (\gamma(t)) \gamma_n ' (t)]dt\\
	&= \int_a^b F(\gamma(t)) \gamma ' (t) dt = \int_\gamma F \cdot ds
\end{align*}
where $F = (f_1, ... , f_n)$\\
\\
We can think of $F = (f_1, ... , f_n)$ and $ds = (dx_1, ... , dx_n)$ where $dx_i = \frac{d(x_i (t))}{dt} dt, i=1,...,n$ which means the dot product notation makes sense.

\begin{example}
	$$\int_\gamma zx^2 dx + yzdy + arcsin(x)dz$$
	where $\gamma (t) = (t, t^2, 1), t\in [0,1]$

	\begin{align*}
		\int_\gamma \omega &= \int_0^1 [(t^2)(1) + (t^2)(2t) + (0)(arcsint) ]dt\\
		&= \int_0^1 (t^2+ 2t^3)dt\\
		&= [\frac{1}{3} t^3 + \frac{1}{2} t^4]_0^1\\
		&=\frac{5}{6}
	\end{align*}

\end{example}

Given a vector field $F=(f_1, ...,f_n)$ we have the (1-form) $\omega = f_1 dx_1 + ... + f_n dx_n$ and vice versa

\begin{definition}
	Let $f:\mathbb{R}^n \mapsto \mathbb{R}$. The differential form corresponding to the vector field $\nabla f$, denoted $df$, is called the \textbf{total differential of $f$}
	$$\nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}), df = \frac{\partial f}{\partial x_1} d x_1 + \frac{\partial f}{\partial x_2} d x_2 + ... + \frac{\partial f}{\partial x_n} d x_n$$
\end{definition}

If we differentiate with respect to $f$, we get
$$\frac{df}{dt} = \frac{\partial f}{\partial x_1} \frac{d x_1}{dt} + \frac{\partial f}{\partial x_2} \frac{d x_2}{dt} + ... + \frac{\partial f}{\partial x_n} \frac{d x_n}{dt}$$
"the total derivative of $f$ with respect to $t$"

\begin{definition}
	Let $F:\mathbb{R}^n \mapsto \mathbb{R}^n$ be a vector field. If $F = \nabla g$, for some $g: \mathbb{R}^n \mapsto \mathbb{R}$ then $F$ is said to be \textbf{conservative}.
\end{definition}

\begin{definition}
	Let $\omega$ be a first order differential form in $\mathbb{R}^n$. If $\omega = dg$ for some $g: \mathbb{R}^n \mapsto \mathbb{R}$ then $\omega$ is said to be \textbf{exact}.
\end{definition}

\begin{note}
	$(F_1, ..., F_n) = F = \nabla g \Longleftrightarrow dg = \omega = F_1 dx_1 + F_2 dx_2 + ... + F_n dx_n$
	So this implies that $F$ is conservative iff $\omega$ is exact
\end{note}

\begin{example}
	$\int_\gamma xy dx + xdy$ where
	\begin{enumerate}
		\item{$\gamma$ is a parameterization of the top half of the unit circle}
		\item{$\gamma$ is a parameterization of the line $(1,0) \to (-1,0)$}
	\end{enumerate}

	\begin{enumerate}
	\item{
		$$\gamma (t) = (cost t, sin t), t \in [0,\pi]$$
		\begin{align*}
			\int_\gamma xy dx + xdy &= \int^\pi_0 ((cost)(sint)(-sint) + (cost)(cost))dt\\
			&= \int^\pi_0 (sin^2 t + cost + cos^2 t) dt\\
			&= \int^\pi_0 \frac{1}{2} dt = \frac{\pi}{2}
		\end{align*}

	}
	\item{
	$$\gamma (t) = (1-t,0), t\in [0,2]$$
		\begin{align*}
			\int_\gamma xy dx + xdy &= \int^2_0 (0)(-1) + (1-t)(0)dt = 0
		\end{align*}

	}
	\end{enumerate}
\end{example}

\begin{note}
	The line integral depends on the path chosen, not on how you parameterize that path
\end{note}

Think of a parameterization as a bijection, except possibly at finitely many of those points.

\begin{theorem}
	Suppose $F:\mathbb{R}^n \mapsto \mathbb{R}$ is of class $C^1$ and $\gamma: [a,b] \mapsto \mathbb{R}^n$ is a piecewise smooth path in $\mathbb{R}^n$ then
	$$\int_\gamma \nabla f ds = f(\gamma(b)) - f(\gamma(a))$$
\end{theorem}

\begin{proof}
	Apply the chain rule to the composite function $G:\mathbb{R} \mapsto \mathbb{R}$ given by $G(t) = (f \circ \gamma)(t)$
	$$G'(t) = \nabla f(\gamma(t)) \gamma '(t) \in \mathbb{R}$$
	Then we apply FTOC to get
	\begin{align*}
		\int^b_a G'(t) dt &= G(b) - G(a)\\
		&= f(\gamma(b)) - f(\gamma(a))
	\end{align*}

	$$\int_\gamma \nabla f ds = \int^b_a \nabla f(\gamma(t)) \gamma '(t) = f(\gamma(b)) - f(\gamma(a))$$

\end{proof}

\begin{corollary}
	If $F = \nabla f$ is a conservative vector field and $\gamma$ is a curve joining $x_0, x_1$ then
	$$\int_\gamma Fds = f(x_1) - f(x_0)$$
\end{corollary}

The line integral of a conservative vector field is independent of the choice of path

\underline{Remarks:}
\begin{enumerate}
	\item{This theorem and corollary can be restated in terms of differential forms. For example, If $\omega = df$ is exact and $\gamma$ is a curve joining $x_0, x_1$ then $$\int_\gamma \omega = f(x_1) - f(x_0)$$}
	\item{
	This result (any form) is a generalization of the FTOC and is often called the "General Fundamental Theorem of Calculus" (GFTC) ($\int_\gamma d\omega = \int_{\partial \gamma} \omega$)
	}
\end{enumerate}

\newpage

\section{Friday, February 2, 2018}

\begin{definition}
  A \textbf{closed curve} is a curve which joins a point $x_0$ to itself
\end{definition}

\begin{definition}
  A \textbf{simple curve} is a curve which does not intersect itself
\end{definition}

\begin{definition}
  A \textbf{Jordan curve} is a non-self-intersecting continuous loop in the plane
\end{definition}

\begin{corollary}
  If $F$ is a conservative vector field, and $\gamma$ is a closed curve, then
  $$\int_\gamma F\cdot ds = 0$$
\end{corollary}

\begin{definition}
  If $F=\nabla g$ is a conservative vector field then $g$ is called a potential function for the vector field F
\end{definition}

\begin{theorem}
  If $F$ is a vector field and $\int_\gamma F\cdot ds = 0, \forall$ closed curves $\gamma$ then $F$ is convservative. (The idea is that:)
  $$\int_{\gamma+\sigma} F\cdot ds = \int_\gamma F\cdot ds + \int_\sigma F\cdot ds = 0$$
\end{theorem}

\begin{example}
  $$\int_\gamma y^2 dx + 2xydy, \gamma (t) = (arctan(sin(\frac{\pi}{2}t)), e^{cos(\frac{\pi}{2}t)}), t\in [0,1]$$
  $g(x,y) = xy^2 + c$ is a potential function for $\omega = y^2 dx + 2xydy$.
  \\
  \begin{align*}
    \int_\gamma y^2 dx + 2xydy &= g(\gamma (1)) - g(\gamma (0))
  \end{align*}
  $$\gamma (1) = (arctan(1), e^0) = (\frac{\pi}{4}, 1)$$
  $$\gamma (0) = (arctan(0), e) = (0,e)$$
  This implies that $\int_\gamma = \frac{\pi}{4} (1) - (0)(e^2) = \frac{\pi}{4}$. Assume that $F_1,...,F_n $ are of class $C^1$, then $F = (F_1, ..., F_n)$ or $\omega = F_1 dx_1 + ... + F_n dx_n$\\
  \\
  So if $F = v \cdot g$ or $\omega = dg$ then $F_i = \frac{\partial g}{\partial x_i}$\\
  \\
  $F_1,...,F_n$ are $C^1 \Longrightarrow$ g must be $C^2$

  $$\frac{\partial F_i}{\partial x_j} = \frac{\partial^2 g}{\partial x_j \partial x_i} = \frac{\partial^2 g}{\partial x_i\partial x_j} = \frac{\partial F_j}{\partial x_i}$$
  $\Longrightarrow$ a necessary condition for $F$ to be conservative or $\omega$ to be exact is
  $$\frac{\partial F_i}{\partial x_j} = \frac{\partial F_j}{\partial x_i}, i,j \in [1,n], i\neq j$$
\end{example}

\begin{definition}
  Let $\omega = F_1 dx_1 + ... + F_n dx_n$. If
  $$\frac{\partial F_i}{\partial x_j} = \frac{\partial F_j}{\partial x_i}, i,j \in [1,n], i\neq j$$
  Then $\omega$ is said to be closed.
\end{definition}

\begin{note}
    exact $\Longrightarrow$ closed
\end{note}

\begin{definition}
  A subset $S\subset \mathbb{R}^n$ is said to be \textbf{simply connected} if given two curves with the same endpoints on $S$, one can be continuously deformed in the other within $S$.\\
  \\
  In $\mathbb{R}^n$, if $S$ is connected, it is also simply connected if for every closed curve $\gamma$ in $S$, the interior of $\gamma$ is also in $S$.
\end{definition}

\begin{example}
  $\mathbb{R}^3 \setminus \{ 0 \}$ is simply connected
\end{example}

\begin{example}
  $\mathbb{R}^3 \setminus \{ (x,0,0) | x\in \mathbb{R} \}$ is not simply connected
\end{example}

\begin{definition}
  If $\omega = F_1 dx_1 + ... + F_n dx_n$ the domain of $\omega$ is the intersection of the domains of all $F_i, i\in [1,n]$
\end{definition}

\begin{theorem}
  If $\omega$ is a differential form whose domain is simply connected then $\omega$ is closed iff $\omega$ is exact. $\mathbb{R}^n$ is simply connected $\Longrightarrow$ If $\omega$ is defined on all $\mathbb{R}^n$, $\omega$ is exact iff $\omega$ is closed
\end{theorem}

\newpage

\section{Monday, February 5, 2018}

\begin{theorem}
	Let $\omega = F_1 dx_1 + ... + F_n dx_n$ be 1-form defined on all of $\mathbb{R}^n$, then this implies that $\omega$ is exact $\Longleftrightarrow$ it is also closed, since it is defined on all of $\mathbb{R}^n$.
\end{theorem}

\begin{note}
	exact means there exists a potential function $$g:\mathbb{R}^n \mapsto \mathbb{R}, \exists \omega = dg$$
\end{note}

And so we want to find $g$. We have to do the following steps to find $g$

\subsection{Finding the potential function}

(This is the setup for $n=2, \mathbb{R}^2$).


	Let $\omega = F_1 dx + F_2 dy$\\
	(is it closed? We're checking for $\frac{\partial F_1}{\partial y} = \frac{\partial F_2}{\partial x}$).\\
	$\mathbb{R}^2$ is simply connected, so if closed $\Longrightarrow$ exact then the potential function $g$ exists.\\
	We know that $\frac{\partial g}{\partial x} = F_1$ and $\frac{\partial g}{\partial y} = F_2$
	$$g(x,y) = \int F_1 (x,y)dx + \frac{dh(x)}{dy}$$
	And then we need to find $h$
	$$\frac{\partial g(x,y)}{\partial y} = \frac{\partial}{\partial y}(\int F_1 (x,y)dx + \frac{dh(x)}{dy})$$
	$$\frac{dh(x)}{dy} = F_2 (x,y) + \frac{\partial}{\partial y}(\int F_1 (x,y)dx)$$
	All $x$ should cancel from the right hand side, leaving a function of $y$ a lone, which we can integrate to get $h$.
	$$\frac{\partial}{\partial x}(F_2 (x,y) + \frac{\partial}{\partial y}(\int F_1 (x,y)dx))$$
	$$\Longrightarrow \frac{\partial F_2}{\partial x} - \frac{\partial^2}{\partial x\partial y}  (\int F_1 (x,y)dx)$$
	$$\frac{\partial F_2}{\partial x} - \frac{\partial^2}{\partial x\partial y}  (\int F_1 (x,y)dx) = \frac{\partial F_2}{\partial x} - \frac{\partial F_2}{\partial y} = 0$$

	\begin{example}
		(This example is in $n=3, \mathbb{R}^3$)\\

		$$\omega = (z^2 + 2xy)dx + (x^2 +2yz)dy + (y^2 + 2xz + cos(z))dz$$
		$$F_1 = (z^2 + 2xy)$$
		$$F_2 = (x^2 +2yz)$$
		$$F_3 = (y^2 + 2xz + cos(z))$$

		So we start out by checking if omega is closed, and since
		$$\frac{\partial F_2}{\partial x} = 2x = \frac{\partial F_1}{\partial y}$$
		$$\frac{\partial F_1}{\partial z} = 2z = \frac{\partial F_3}{\partial x}$$
		$$\frac{\partial F_2}{\partial z} = 2y = \frac{\partial F_3}{\partial y}$$
		Since the cross partials are equal, it is closed. More formally, the Domain of $\omega$ is $\mathbb{R}^3$ which is simply connected, and closed $\Longrightarrow$ exact.\\
		\\
		We know there is a potential function $g:\mathbb{R}^3 \mapsto \mathbb{R}$\\
		\\
		Now we must find and construct $g$.

		$$\frac{\partial g}{\partial x} = F_1 = (z^2 + 2xy)$$
		$$\Longrightarrow g(x,y,z) = \int (z^2 + 2xy) + h(y,z)$$
		$$\int (z^2 + 2xy) + h(y,z) =  xz^2 + x^2y + h(y,z)$$

		And since we know that

		$$F_2 = x^2 + 2yz = \frac{\partial}{\partial y} (xz^2 + x^2y + h(y,z))$$
		$$2yz = \frac{\partial h}{\partial y}$$
		$$\Longrightarrow \int 2yz dy + k(z) = h(x,y)$$
		$$\Longleftrightarrow h(x,y) = \int 2yz dy + k(z) = y^2 z + k(z)$$
		Now we have
		$$g(x,y,z) = xz^2 + x^2y + y^2 z + k(z)$$
		$$F_3 = y^2 + 2xz + cos(z) = \frac{\partial}{\partial z}(y^2 z + k(z))$$
		$$2xz + cos(z) = \frac{\partial k(z)}{\partial z}$$
		$$k(z) = \int (2xz + cos(z)) dz = x^z + sin(z) + C$$
		Therefore, we have
		$$g(x,y,z) = 2xz^2 + x^2 y + y^2 z + sin(z) + C$$

	\end{example}

	\subsection{Green's Theorem}

	\begin{Theorem}
		Let $\gamma$ be a smooth Jordan Curve in $\mathbb{R}^2$ oriented in the counterclockwise direction, and let $R$ be the region enclosed by $\gamma$. Let $\omega = F_1 dx + F_2 dy$ where $F_1, F_2 : \mathbb{R} \mapsto \mathbb{R}$ are of class $C^1$ throughout $\mathbb{R}$. then
		$$\int_{\gamma = \partial R} F_1 dx + F_2 dy = \int_R (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA$$
	\end{Theorem}

	\begin{note}
		Can restate in terms of a vector field. The conclusion would look like this:
		$$\int_{\gamma = \partial R} F\cdot ds = \int_R (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA$$ where $F = (F_1, F_2)$
	\end{note}

	\begin{note}
		The theorem can easily be generalized to the case where $\gamma$ is a Plane Simple Jordan Curve
	\end{note}

	\begin{note}
		Sometimes the LHS is given as $$\varointctrclockwise_{\gamma = \partial R} F_1 dx + F_2 dy$$ which indicates a closed curve in the counterclockwise direction  ($\varointclockwise$ for the other direction).
	\end{note}

	\begin{note}
		Should note that this theorem is of the form $$\int_{\partial R} f = \int_R df$$
	\end{note}

	\begin{example}
		Let $R$ be a disc of radius 3 centered at the origin. Verify Green's Theorem by direct calculation when $F_1 (x,y) =  xy$ and $F_2 (x,y) = x$.\\
		\\
		So to verify Green's Theorem, we will calculate the left hand side and the right hand side separately and compare our results.\\
		\underline{Left Hand Sign}\\
		\\
		$\partial R$ is a circle of radius 3 centered at the origin\\
		We can parameterize it by $\gamma (t) = (3cost, 3sint), t\in [0,2\pi]$
		\begin{align*}
			\int_\gamma F_1 dx + F_2 dy &= \int_\gamma xydx + xdy\\
			&= \int^{2\pi}_0 (3cost)(3sint)(-3sint)+ (3cost)(3cost)dt\\
			&= 9 \int^{2\pi}_0 cos^2 t dt\\
			&= 9 \int^{2\pi}_0 (\frac{1}{2} + \frac{1}{2}cos(2t)) dt\\
			&= \frac{9}{2} \int^{2\pi}_0 dt = 9\pi
		\end{align*}

		\underline{Left Hand Sign}\\
		\\
		\begin{align*}
			\iint_{\text{disk}} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA &= \iint_{\text{disk}} (1-x) dA\\
			&\overset{\text{polars}}{=} \int^{2\pi}_0 \int^3_0 (1-rcos\theta)rdrd\theta\\
			&=\int^{2\pi}_0 \int^3_0 (r-r^2cos\theta)drd\theta\\
			&=\int^{2\pi}_0 [\frac{1}{2}r^2 - \frac{1}{3} r^2 cos\theta]^3_0 d\theta\\
			&=\int^{2\pi}_0 (\frac{9}{2}-0) d\theta = 9\pi
		\end{align*}
		Since both sides are the same, Green's theorem holds for this example.
	\end{example}

	\begin{corollary}
		(Corollary 1 of Green's Theorem)\\
		If $\gamma$ is a smooth Jordan Curve in $\mathbb{R}^2$ and $R$ is the region enclosed by $\gamma$ then the area of $R$ is given by
		$$A(R) = \frac{1}{2} \int_\gamma xdy-ydx$$
	\end{corollary}

	If we apply Green's theorem to this corollary, we get
	$$\frac{1}{2} \iint_R (1+1) dA = \iint_R dA = \text{The area of $R$}$$

	\begin{corollary}
		If the domain of $\omega$ (in $\mathbb{R}^2$) is simply connected, the $\omega$ is closed $\Longrightarrow$ exact.
	\end{corollary}

	$\omega = F_1 dx + F_2 dy$, so closed would be $(\frac{\partial F_2}{\partial x} = \frac{\partial F_1}{\partial y})$\\
	If $\gamma$ is any closed curve then
	\begin{align*}
		\int_{\partial R = \gamma} \omega &= \iint_R (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) dA\\
		&= \iint_R 0 dA = 0
	\end{align*}

	\begin{example}
		Find the area of the ellipse
		$$\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$$\\
		\\
		$\gamma (t) = (acost, bsint), t\in [0,2\pi]$
		\begin{align*}
			\text{Area} &= \frac{1}{2} \int_\gamma xdy-ydx\\
			&= \frac{1}{2} \int^{2\pi}_0 (acost)(bcost)-(bsint)(-asint)dt\\
			&= \frac{1}{2} \int^{2\pi}_0 ab (cos^2 t + sin^2 t)dt\\
			&= \pi ab
		\end{align*}
	\end{example}

	\begin{example}
		Recall the previous examples,\\
		$\int_\gammma xyd + xdy$ can go from $(1,0)\to (-1,0)$ in 2 differents way\\
		\begin{enu}
			\item{Along top half of the unit circle}
			\item{Along the $x$-axis}
		\end{enu}
		This time we'll take $\gamma$ as the top half of the unit cricle followed along the $x$-axis back to $(1,0)$.\\
		Now since Green's Theorem holds, since it's closed:
		\begin{align*}
			\int_\gamma xydx + xdy &\overset{\text{Green's}}{=} \int^1_{-1} \int_0^{\sqrt{1-x^2}} (1-x) dydx\\
			&\overset{\text{polar}}{=} \int^\pi_0 \int^1_0 (r^2 - r^2 cos\theta) dr d\theta\\
			&= \int^\pi_0 [\frac{1}{2} - \frac{1}{2} cos\theta]d\theta\\
			&= \frac{\pi}{2}
		\end{align*}
		Does this agree with our previous calculation?\\
		Yes.
	\end{example}

	\subsection{Winding Numbers}

	\begin{example}
		$$\omega = \frac{-y}{x^2+y^2} + \frac{x}{x^2+y^2}$$
		$$\frac{\partial F_1}{\partial y} = \frac{y^2 - x^2}{(x^2+y^2)^2}$$
		$$\frac{\partial F_2}{\partial y} = \frac{y^2 - x^2}{(x^2+y^2)^2}$$
		$\Longrightarrow$ closed, but not exact, since it is not simply connected as the domain is $\mathbb{R}^2 - \{ (0,0) \}$\\
		\\
		If it were exact, $\int_\gamma \omega = 0$ for all closed $\gamma$\\
		\\
		Let $\gamma$ parameterize the unit circle in the counterclockwise direction.\\
		$\gamma (t) = (cos t, sin t), t \in [0,2\pi]$\\
		\begin{align*}
			\int_\gamma \omega &= \int^{2\pi}_0 (-sint)(-sint) + (cost)(cost) dt\\
			&= \int^{2\pi}_0 (sin^2 t + cos^2 t) dt\\
			&= \int^{2\pi}_0 dt\\
			&= 2\pi \neq 0
		\end{align*}
		We have a closed curve that is $\neq 0$\\
		\\
		If the radius were $R$.
		$$(\frac{-R sint}{R^2})(-Rsint) + (\frac{-R cost}{R^2})(Rcost) &= \frac{R^2}{R^2} sin^2 t + \frac{R^2}{R^2} cos^2 t = 1$$
		$\Longrightarrow$ the integral would still be $2\pi$\\
		\\
		Does this contradict Green's Theorem?\\
		No, Since $F_1, F_2$ are not defined throughout the region $R$ enclosed by $\gamma$ and $F_1, F_2$ is not defined at $(0,0)$\\
		\\
		Is this unique to circles?\\
		No.

	\end{example}

\newpage

\section{Friday, February 9, 2018}

Continuing from the example from before:

\begin{example}
	$\omega = \frac{-y}{x^2+y^2} + \frac{x}{x^2+y^2}$ is closed but not exact. $\int_\gamma \omega = 2\pi$ when $\gamma$ is a counterclockwise parameterization of a unit circle of radius $R$.
\end{example}

	Generalizing, we have:
	\begin{theorem}
		Let $\sigma$ be any Jordan curve which circles the origin in the counterclockwise direction.
	\end{theorem}
	\underline{CLAIM:} $\int_\sigma \omega = 2\pi$\\
	\\
	\begin{proof}
		\begin{enumerate}
			\item{Does not depend on the shape of $\sigma$ (as long as it's around the origin)}
			\item{
			Any such curve can be continuously deformed in the circle\\
			Let $\sigma = \sigma_1 + \sigma_2$\\
			$\gamma = -\sigma_1 - \sigma_2$ (since it is counterclockwise)\\
			Without loss of generality, assume $\sigma$ is longer than $\gamma$\\
			$$\int_{\sigma_1} \omega = \int_\alpha \omega + \int_{\gamma_1} \omega + \int_{\beta} \omega \overset{\text{Green's}}{=} \int_{R_1} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) dA = 0$$
			$$\int_{\sigma_2} \omega = \int_{-\beta} \omega + \int_{\gamma_2} \omega + \int_{-\alpha} \omega \overset{\text{Green's}}{=} \int_{R_2} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) dA = 0$$
			Note that $(\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y}) = 0 \Longrightarrow$ closed
			}
		\end{enumerate}
		And now, if we add the 2 equations, then we get:
		\begin{align*}
			\int_{\sigma_1} \omega + \int_{\sigma_2} \omega + \int_{\gamma_1} \omega + \int_{\gamma_2} \omega &= \int_\sigma \omega + \int_{-\gamma} \omega\\
			&= \int_\sigma \omega + -\int_{\gamma} \omega = 0
		\end{align*}
		$\Longrightarrow \int_\sigma \omega = \int_{\gamma} \omega = 2\pi$
	\end{proof}

	\begin{note}
		$\frac{1}{2\pi} \int_{\gamma} \omega$ is always an integer. It will give the number of times $\gamma$ encircles the origin in the counterclockwise direction
	\end{note}

	\begin{definition}
		If $\gamma$ is a closed curve which wraps around the origin, the integrals
		$$\frac{1}{2\pi} \int_{\gamma} \frac{-y}{x^2+y^2} + \frac{x}{x^2+y^2}$$
		is called the \textbf{winding number} of $\gamma$
	\end{definition}

	\begin{example}
		if $\gamma$ never completely encircles the origin, the winding number is 0
	\end{example}

	\begin{example}
		if $\gamma$ encircles the origin once in the counterclockwise direction, then winding number is $-1$
	\end{example}

	\begin{note}
		The corresponding vector field to the 1-form used above is
		$$F(x,y) = (\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2})$$
		This vector field is called a \textbf{vortex field}.
	\end{note}

	\begin{example}
		Evaluate $\int_\gamma F ds$ where $F$ is the vortex field and $\gamma$ is the boundary curve of the square $[-1,1] \times [-1,1]$ oriented in the clockwise direction.\\
		\\
		\begin{align*}
			\varointclockwise F ds &= \int_\gamma (\frac{-y}{x^2+y^2}, \frac{x}{x^2+y^2}) ds\\
			&= 2\pi \cdot (\text{Winding Number})\\
			&= 2\pi (-1)\\
			&= -2\pi
		\end{align*}
	\end{example}

	\begin{example}
		$f(x,y) = -tan^{-1} (\frac{x}{y}) + C, y\neq 0$\\
		\\
		$$\frac{\partial f}{\partial x} = \frac{-y}{x^2+y^2}, \frac{\partial f}{\partial y} = \frac{x}{x^2+y^2}$$
		But $f(x,y)$ is not defined for all of $\mathbb{R}^2$, but the same set without the point $(0,0)$.\\
		We can find a potential function on some subset, say $\{ (x,y)\in\mathbb{R}^2 | y>0 \}$ (simply connected subset).
	\end{example}

\newpage

\section{Monday, February 12, 2018}

\subsection{Proof of Green's Theorem}

\begin{lemma}
	If $R$ is a region like $R_x$ then $\int_{\gamma = \partial R} F_1 dx = \int_R \frac{\partial F_1}{\partial y} dA$. ($R_x$ being a region bounded in between two points on the x axis and between two curves that pass through this region as well)
\end{lemma}

\begin{proof}
	Think of the boundary as $c_1 + c_2 + c_3 + c_4$\\
	So
	\begin{align*}
		\int_\gamma F_1 dx &= \int_{c_1} F_1 dx + \int_{c_2} F_1 dx + \int_{c_3} F_1 dx + \int_{c_4} F_1 dx\\
		&= \int_{c_1} F_1 dx + \int_{c_2} F_1 dx -\int_{-c_3} F_1 dx -\int_{-c_4} F_1 dx
	\end{align*}
	And now we parameterize:
	$$c_1 : x=x, y=g_1(x), x\in [a,b]$$
	$$c_2 : x=b, y=y, y \in [g_1 (b), g_2 (b)]$$
	$$c_3 : x=x, y=g_2(x), x\in [a,b]$$
	$$c_4 : x=a, y=y, y \in [g_1 (a), g_2 (a)]$$
	We noticed that $dx$ is 0 for $c_2, c_4$

	\begin{align*}
		&\Longrightarrow \int_{c_1} F_1 dx  -\int_{-c_3} F_1 dx\\
		&=\int^b_a F_1 (x,g_1 (x))dx - \int^b_a F_1 (x,g_2 (x))dx\\
		&=\int^b_a [F_1 (x,g_1 (x)) - F_1 (x,g_2 (x))]dx
	\end{align*}

	For a fixed $x, F_1 (x,y)$ and $\frac{\partial F_1}{\partial y} (x,y)$ are continuous functions for $y\in [g_1 (x), g_2 (x)]$.\\
	\\
	$\frac{\partial F_1}{\partial y}$ is the derivative of $F_1$ with respect to $y$.\\
	\\
	By the Fundamental Theoreom of Calculus,
	\begin{align*}
		F_1 (x,g_1 (x)) - F_1 (x,g_2(x)) &= \int^{g_2(x)}_{g_1(x)} - \frac{\partial F_1}{\partial y} dy\\
		\Longrightarrow \int_{\partial R} F_1 dx &= \int^b_a [F_1 (x,g_1 (x)) - F_1 (x,g_2 (x))]dx\\
		&= \int^b_a \int^{g_2(x)}_{g_1(x)} - \frac{\partial F_1}{\partial y} dy dx
	\end{align*}
	As wanted

\end{proof}

\begin{lemma}
	If $R$ is a region like $R_y$ then $\int_{\gamma = \partial R} F_1 dx = \int_R \frac{\partial F_2}{\partial x} dA$. ($R_y$ being a region bounded in between two points on the y axis and inbetween two curves that pass through this region as well)
\end{lemma}

\begin{proof}
	similar to the previous proof, but with the roles switched.
\end{proof}

\begin{lemma}
	If Green's Theorem holds on $R_1, R_2$ then it holds on $R_1 \cup R_2$
\end{lemma}

\begin{proof}
	\int_{R_1 \cup R_2} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA &= \int_{R_1} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA + \int_{R_2} (\frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})dA\\
	&\overset{\text{Green's}}{&=} \int_{\gamma_1} F_1 dx + F_2 dy + \int_{\sigma} F_1 dx + F_2 dy + \int_{-\sigma} F_1 dx + F_2 dy + \int_{-\gamma_2} F_1 dx + F_2 dy\\
	&= \int_{\gamma_1} F_1 dx + F_2 dy + \int_{\gamma_2} F_1 dx + F_2 dy\\
	&= \int_{\partial (R_1 \cup R_2)} F_1 dx + F_2 dy

	Then conclude using Lemma 1 and 2
\end{proof}

\subsection{Surfaces}

\begin{definition}
	A \textbf{parameterized surface} is a function $\Phi : D \subset \mathbb{R}^2 \mapsto \mathbb{R}^n$.
\end{definition}

\begin{definition}
	The \textbf{surface} corresponding to the function $\Phi$ is its image $\Phi (D)$
\end{definition}

\begin{definition}
	$\Phi$ is $diff(C^1)$ if each component is $diff(C^1)$. We then say that $S$ is a $diff(C^1)$ surface.
\end{definition}

\begin{remark}
	"surface" is generally a 2-dimensional surface (we'll later define k-dimensional surfaces in $\mathbb{R}^n$)
\end{remark}

\begin{remark}
	We will generally restrict to surfaces which are at least piecewise $C^1$
\end{remark}

\begin{remark}
	The domain cannot always be described by inequalities with numeric limits
\end{remark}

\begin{remark}
	Think of $\Phi$ as "twisting", "bending", "stretching", $D$ onto the surface $\Phi(D)$
\end{remark}

\begin{remark}
	There may be many different parameterizations of the same surface, for example, the top half of a unit sphere ($x^2 + y^2 + z^2 = 1, z \geq 0$). One example may be $$\phi (u,v)=(cosu sinv, sinu sinv, cosv), u\in [0,2\pi], v \in [0,\frac{\pi}{2}]$$
	and another may be
	$$\phi (s,t) = (s,t \sqrt{1-s^2 - t^2}), (s,t) \in D = \text{ (circle disk of radius 1)}$$
\end{remark}

\begin{remark}
	Graph of a function of two variables $z=f(x,y)$ is parameterized by $\phi (u,v) = (u,v,f(u,v)), (u,v) \in Dom(f)$
\end{remark}

\begin{remark}
	for $F(x,y,z) = 0$, we could try to sub in $z=g(x,y)$ like the last remark, but if it is not possible, we may need to split the surface into several pieces, generally, $z=g(x,y)$ is not the best approach.
\end{remark}

\begin{definition}
	 $\Phi : D \subset \mathbb{R}^2 \mapsto \mathbb{R}^n$ is a surface parameterization and fix $v=v_0$, then $(\phi_1 (u,v_0),..., \phi_n (u,v_0))$ is a path on the surface
\end{definition}

$$\gamma' (u) = (\frac{\partial \phi_1}{\partial u} (u,v_0),..., \frac{\partial \phi_n}{\partial u} (u,v_0)) = (\gamma_1 ' (u), ..., \gamma_n ' (u) )$$

\begin{note}
	$\gamma '$ is tanget to the surface, it represents $u$-curves or $u$-lines $(\phi_u (u,v_0) = \gamma' (u))$, similarly, we have $v$-curves and $v$-lines and tangets $\phi_v (u,v)$
\end{note}

So at the point $\phi (u_0, v_0)$, we have two tangent vectors, $\phi_u (u_0, v_0)$ and $\phi_v (u_0, v_0)$\\
\\
In the case of the $k$-dimensional surface, there will be $k$ of these tangets. (\phi_{u_1}, i = 1,...,k)\\
\\
Any other tangent vector at $\phi (u_0, v_0)$ can be written as a linear combination of $\phi_u$ and $\phi_v$.
$$T(u_0, v_0) = \lambda \phi_u (u_0, v_0) + \mu \phi_v (u_0, v_0), \lambda, \mu \in \mathbb{R}$$
$$T(u_1, ..., u_k) = \sum^k_{i=1} \lambda_i \phi_{u_i} (u_1, ..., u_k))$$

For the moment, restrict to 2-dim surfaces in $\mathbb{R}^2$, since $\phi_u$ and $\phi_v$ are tangent to the surface $\phi (u_0, v_0)$, $\phi_v \times \phi_u (u_0, v_0)$ is perpendicular to the surface (and tangent plane) at $\phi (u_0, v_0)$. Hence the tangent plane at $\phi (u_0, v_0)$ can be given by $x\cdot (\phi_v \times \phi_u (u_0, v_0)) = \phi (u_0, v_0) \cdot (\phi_v \times \phi_u (u_0, v_0))$ where $x=(x_1, x_2, x_3)$, an arbitrary point on the tangent plane.

\begin{example}
	$x^2 + y^2 + z^2 = 1, \phi (u,v) = (cosu sinv, sinu sinv, cosv), u\in [0,2\pi], v\in [0,\pi], p=1$\\
	$u$-lines are circles of latitude while $v$-lines are half circles of longitude.\\
	$$\phi_u = (-sin(u)sin(v), cos(u)sin(v), 0)$$
	$$\phi_v = (cos(u)cos(u), sin(u)sin(v), -sin(v))$$
	\begin{align*}
		\phi_u \times \phi_v &= (-cos(u)sin^2(v), -sin(u)sin^2 (v), -sin(v)cos(v))\\
		&= (-sinv)(cos(u)sin(v), sin(u)sin(v), cos(v))\\
		&= (-sinv)\cdot \phi (u,v)
	\end{align*}

	The tanget plane is:
	\begin{align*}
		(x,y,z)\cdot (\phi_u \times \phi_v) &= \phi (u,v) \cdot (\phi_u \times \phi_v)\\
		&= (-sinv)(xcos(u)sin(v), ysin(u)sin(v), zcos(v))\\
		&= (-sinv) \cdot  \phi (u,v) \cdot  \phi (u,v)\\
		&= (-sinv) || \phi (u,v) ||^2\\
		&= (-sinv)
	\end{align*}
	$$\Longrightarrow (cos(u)sin(v))x + (sin(u)sin(v))y + (cos(v))z = 1$$
	So the tangent plane at the point $(0,1,0)$ is:
	$u=\frac{\pi}{2}, v=\frac{\pi}{2}$, therefore $0x + y + 0z = 1 \Longrightarrow y=1$

\end{example}

\begin{note}
	Different parameterizations give different $u$ and $v$ lines.
\end{note}

\begin{remark}
	Think of $(u,v)$ as a new coordinate system used to locate a point $\phi (u,v)$ on the surface $\phi (D)$. In the same way as we use $(x,y)$ to locate a point in the $xy$-plane. Usually called a curvilinear-coordinate.
\end{remark}

\begin{remark}
	Tangent plane only makes sense if $\phi_u \times \phi_v \neq 0$
\end{remark}

\begin{definition}
	We say the surface is regular or smooth at $\phi (u_0, v_0)$ if $\phi$ is differentiable at $\phi (u_0, v_0)$ and $(\phi_u \times \phi_v (u_0, v_0)) \neq 0$. The surface is regular (smooth) if it is regular at each point. This depends on both the parameterization and the geometry of the surface.
\end{definition}

\newpage

\section{Friday, February 16, 2018}

\subsection{Surfaces (cont'd)}

\begin{example}
	Consider the following slide\\
	$\phi (s,t) = (\sin^3 s \cos^3 t, \sin^3 \sin^3 t, \cos^3 s), s \in [0,\pi], t\in [0,2\pi]$\\
	Note that $\phi$ is not regular since  $\phi_s \times \phi_t$ vanishes at $s=\frac{\pi}{2}, t=\frac{\pi}{2}$, so a sharp point at $(0,1,0)$
\end{example}

\begin{example}
	Consider $z=x^2 + y^2$. Compute $\phi (1,2) = (1,2,5)$\\
	We could use the following parameterization:\\
	$$\phi (u,v) = (u \cos v, \sin v, 2u), u \geq 0, v \in [0,2\pi]$$
	$$\phi_u (\cos v, \sin v, 2u), \phi_v = (-u \sin v, u \cos v, 0)$$
	$$\phi_u \times \phi_v = (-2u^2 \cos v, -2u^2 \sin v, u)$$
	This is regular except at $u=0$. The definition of regular depends on the parameterization as well as the geometry of the surface, so consider this better parameterization\\
	$$\phi (u,v) = (u,v,u^2 + v^2)$$
	$$\phi_u = (1,0,2u), \phi_v = (0,1,2v)$$
	$$\phi_u \times \phi_v = (-2u, -2v, 1)$$
	and at $\phi (1,2) \Longrightarrow \phi_u \times \phi_v = (-2,-4,1)$
	\begin{align*}
		0 &= \phi_u \times \phi_v (1,2) \cdot ((x,y,z) - (1,2,5))\\
		&= (-2,-4,1) \cdot (x-1, y-2, z-5)\\
		&= -2x + 2 - 4y + 8 +z -5\\
		&= -2x -4y + z +5
	\end{align*}
	$\Longrightarrow 2x+4y-z=5$
\end{example}

Consider the Parallelogram generated by vectors $a,b$. The area of the parallelogram is $||a \times b||$. Now, intuitively we integrate $f$ over a surface $S$ and divide $S$ into subsurfaces $S_\alpha$. And then we take $\omega_\alpha \in S_\alpha$ and evaluate $f(\omega_\alpha)$ and multiply by the area of $S_\alpha$. Then $$\sum_\alpha f(\omega_\alpha)(\text{area } S_\alpha)$$
In the limit, we get the Riemann Integral.

\begin{definition}
	Let $\phi : D \subset \mathbb{R}^2 \mapsto \mathbb{R}^3$ be a 2-dimension surface in $\mathbb{R}^3$ and let $f:(Im \phi)\mapsto \mathbb{R}$. Then the scalar surface integral of $f$ over $\phi$, denoted $\int_\phi f dS$ is given by
	$$\int_\phi f dS = \int_D f(\phi (u,v)) ||\phi_u \times \phi_v|| dA$$
\end{definition}

\begin{definition}
	the surface area of $\phi$ is given by
	$$\int_\phi dS = \int_D ||\phi_u \times \phi_v|| dA$$
\end{definition}

\begin{example}
	Let $S$ be the upper-hemisphere of the unit sphere. Find the surface area and evaluate $\int_S z dS$\\
	\\
	We choose the following parameterization
	$$\phi (u,v) = (\sin v \cos u, \sin v \sin u, \cos v), u\in [0,2\pi], v\in [0,\frac{\pi}{2}]$$
	$||\phi_u \times \phi_v||=|\sin v|=\sin v$ (since $v\in [0,\frac{\pi}{2}]$).

	\begin{align*}
		\text{area} = \int_\phi dS &= \int_D ||\phi_u \times \phi_v|| dA\\
		&= \int^{2\pi}_0 \int_0^\frac{\pi}{2} \sin v dv du\\
		&= \int^{2\pi}_0 du = 2\pi
	\end{align*}

	\begin{align*}
		\int_\phi z dS = \int_D (\cos v) ||\phi_u \times \phi_v|| dA\\
		&= \int^{2\pi}_0 \int_0^\frac{\pi}{2} (\cos v)(\sin v) dv du\\
		&= \frac{1}{2} \int^{2\pi}_0 [\sin^2 v] du\\
		&= \frac{1}{2} \int^{2\pi}_0 du = \pi
	\end{align*}
\end{example}

In the $ij$th piece, choose $\phi (u_i^*, u_j^*))$ then evaluate $f$ at that point; the volume is $f(\phi (u_i^*, u_j^*)))\cdot \text{area of the $ij$th piece}$. The sides of the the $ij$th piece are: $\phi_u \Delta u$ and $\phi_v \Delta v$
\newpage

\section{Friday, February 26, 2018}

$f(\phi (u_i^*, u_j^*)))\cdot \text{area of the $ij$th piece}$ with sides $\phi_u \Delta u$ and $\phi_v \Delta v$\\
area $= |\phi_u \Delta u \times \phi_v \Delta v| = \Delta v \Delta u ||\phi_u \times \phi_v||$\\

$$\int_\Phi f dS = \sum_{i=j} f(\phi (u_i^*, u_j^*))) ||\phi_u \times \phi_v|| \Delta v \Delta u $$
If we take the limit of the Reimann Sum, then
$$\int_\Phi f dS = \int_D f(\phi (u, u))) ||\phi_u \times \phi_v|| d v d u$$
$$\int_\Phi f dS = \int_D f(\phi (u, u))) ||\phi_u \times \phi_v|| dA$$

\begin{example}
	Let $S$ be the piece of the paraboloid $z=1-x^2 -y^2$ which lies above the $xy$-plane. Find the surface area of $S$, and evaluate $\int_S d dS$ when $f(x,y,z)=z$\\
	\\
	$\Phi (u,v) = (u,v,1-u^2,v^2)$ where $(u,v) \in$ disk\\
	This type of parameterization is bad, please try to avoid it.\\
	\\
	Better Parameterization: $\Phi (r, \theta) = (r\cos\theta, r\sin\theta, 1-r^2)$
	$$D = \begin{cases}
		r \in [0,1]\\
		\theta \in [0,2\pi]
	\end{cases}$$

	$$\phi_r = (\cos\theta, \sin\theta, -2r)$$
	$$\phi_\theta = (-r\sin\theta, r\cos\theta, 0)$$
	$$\phi_r \times \phi_\theta = (2r^2 \cos\theta, 2r^2 \sin\theta, r)$$
	$$||\phi_r \times \phi_\theta|| = \sqrt{4r^4 \cos^2 \theta + 4r^4 \sin^2 \theta + r^2} = \sqrt{4r^4 + r^2} = |r| \cdot \sqrt{4r^4 + 1} = r\sqrt{4r^4 + 1}$$

	The surface area:
	\begin{align*}
		\int_S dS &= \int_D ||\phi_r \times \phi_\theta|| dA\\
		&= \int^{2\pi}_0 \int^1_0 r\sqrt{4r^4 + 1} dr d\theta\\
		&= 2\pi \int^1_0 r\sqrt{4r^4 + 1} dr\\
		&= ... = \frac{\pi}{6} [5^{\frac{3}{2}} - 1]
	\end{align*}

	\begin{align*}
		\int_S z dS &= \int_D (z(r,\theta)) ||\phi_r \times \phi_\theta|| dA\\
		&= \int^{2\pi}_0 \int^1_0 (1-r^2) r\sqrt{4r^4 + 1} dr d\theta\\
		&= ... = \frac{\pi}{60} [5^{\frac{5}{2}} - 11]
	\end{align*}
\end{example}

\begin{example}
	Find the area of the piece of the plane $x+y+z=1$ which lies over the elliptical disk $x^2 + 2y^2 \leq 1$.\\
	\begin{note}
		Ellipse: $x^2 + 2y^2 = 1$ and $x=\cos\theta, y=\frac{1}{\sqrt{2}} \sin\theta$\\
		Elliptical Disk: $(r\cos\theta, \frac{r}{\sqrt{2}} \sin\theta)$
	\end{note}
	The surface can be parameterized by the following:
	$$\Phi (r,\theta) = (r\cos\theta, \frac{r}{\sqrt{2}} \sin\theta, 1-r\cos\theta-\frac{r}{\sqrt{2}} \sin\theta, r \in [0,1], \theta \in [0,2\pi])$$
	$$\phi_r = (\cos\theta, \frac{\sin\theta}{\sqrt{2}}, -\cos\theta - \frac{\sin\theta}{\sqrt{2}})$$
	$$\phi_\theta = (-r\sin\theta, \frac{r}{\sqrt{2}} \cos\theta, r\sin\theta - \frac{r}{\sqrt{2}} \cos\theta)$$
	$$\phi_r \times \phi_\theta = (\frac{r}{\sqrt{2}},\frac{r}{\sqrt{2}},\frac{r}{\sqrt{2}})$$
	$$\phi_r \times \phi_\theta = \frac{r}{\sqrt{2}} (1,1,1)$$
	$$||\phi_r \times \phi_\theta|| = \sqrt{\frac{3}{2}} r$$

	$$\int_\Phi dS = \int_D ||\phi_r \times \phi_\theta|| dA = \sqrt{\frac{3}{2}} \int^{2\pi}_0 \int^1_0 rdrd\theta = ... = \sqrt{\frac{3}{2}} \pi$$
\end{example}

\begin{definition}
 A \textbf{$k$-dimensional parameterized surface} is a function $\Phi: D \subset \mathbb{R}^k \mapsto \mathbb{R}^n$ where $D$ is some domain $\mathbb{R}^k, k\leq n$. The surface $S$ corresponting to the function $\Phi$ is its image $\Phi (D)$.
\end{definition}

$phi (u_1, ..., u_k) = (phi_1 (u_1, ..., u_k), ..., phi_n (u_1, ..., u_k))$. This means there are $k$ tangent vectors and $n$ lines.

\begin{align*}
	||a \times b|| &= ||a||||b||\sin\theta, \theta = \Delta (a,b)\\
	&= ||a \times b||^2 = ||a||^2||b||^2\sin^2\theta\\
	&= ||a||^2||b||^2 (1-\cos^2 \theta)\\
	&= ||a||^2||b||^2 - ||a||^2||b||^2\cos^2 \theta\\
	&= ||a||^2||b||^2 - (a\cdot b)^2\\
	&= det\begin{bmatrix}
||a||^2 & a\cdot b \\
a\cdot b & ||b||^2
\end{bmatrix} =
det\begin{bmatrix}
a\cdot a & a\cdot b \\
a\cdot b & b\cdot b
\end{bmatrix} = det(CP)
\end{align*}

$$\int_\Phi f dS = \int_D f(\Phi (u_1,...,u_k)) \sqrt{|det(CP)|} dV$$
where
$$CP = (\Phi_{u_i} \cdot \Phi_{u_j}), i,j \in [1,k], \phi_{u_i} = (\frac{\partial \phi_1}{\partial u_i}, ..., \frac{\partial \phi_n}{\partial u_i})$$

\begin{remark}
	$$||\phi_u \times \phi_v|| = \sqrt{[\frac{\partial (\phi_1, \phi_2)}{\partial (u,v)}]^2 + [\frac{\partial (\phi_2, \phi_3)}{\partial (u,v)}]^2 + [\frac{\partial (\phi_3, \phi_1)}{\partial (u,v)}]^2}$$
	where
	$$\frac{\partial (\phi_i, \phi_j)}{\partial (u,v)} = det\begin{bmatrix}
	\frac{\partial \phi_i}{\partial u} & \frac{\partial \phi_i}{\partial v} \\
	\frac{\partial \phi_j}{\partial u} & \frac{\partial \phi_j}{\partial v}
	\end{bmatrix}$$
\end{remark}

\begin{remark}
	\begin{itemize}
		\item{$\phi$ in the piece of the graph (in $\mathbb{R}^3$) of some function $g:\mathbb{R}^2\mapsto \mathbb{R}$ lying over $D$, the domain of $g$.}
		\item{$\int f dS = \int_D f(u,v,g(u,v))||\phi_u \times \phi_v||dA = \int_D f(u,v,g(u,v)) \sqrt{(\frac{\partial g}{\partial u})^2 + ()\frac{\partial g}{\partial v})^2 + 1}dA$}
		\item{The arc length for $\gamma: [a,b] \mapsto \mathbb{R}^2$ is the graph of $\gamma=g(t)$\\
		$\Longrightarrow$ arclength $= \int^b_a \sqrt{1 + (\frac{\partial g}{\partial \gamma})^2}$}
	\end{itemize}
\end{remark}

\begin{remark}
	$dS = \sec\theta dA$ and $\theta = \Delta (\tan, \text{plane}, e_3)$
\end{remark}

\begin{example}
	Area of piece of $z=x^2+4y$ which lies over $\Delta$ in $xy$-plane with vertices $(0,0), (1,0), (1,2)$\\
	\\
	$$\phi (u,v) = (u,v,u^2 + 4v), (u,v) \in D$$
	\begin{align*}
		\int_S dS &= \int_D \sqrt{(\frac{\partial z}{\partial u})^2 + (\frac{\partial z}{\partial v})^2 + 1} dA\\
		&= \int^2_0 \int^1_{\frac{r}{2}} \sqrt{24^2 + 4^4 + 1} dA\\
		&= ... = \frac{1}{6} [21^{\frac{3}{2}} - 17^{\frac{3}{2}}]
	\end{align*}
\end{example}
\newpage

\section{Friday, March 2, 2018}

We'll determine the orientation by choosing a unit normal vector.
\begin{itemize}
	\item{we know that for most surfaces, there are 2 normals, $u_1, u_2$, with\\ $u_2 = -u_1$}
	\item{We'll choose one of the normals}
	\begin{itemize}
		\item{It could be one that points up}
		\item{It could be one that points out (which is arbitrary)}
		\item{etc.}
	\end{itemize}
\end{itemize}

Let the unit sphere be oriented by the outward pointing unit normal. This means it is outside to the positive side and the inside is the negative side.\\
\\
There are surfaces where much choices are not possible.

\begin{definition}
	An \textbf{Oriented Surface} is a 2-sided regular (smooth) surface to which we have attached, at each point, a unit normal vector (which varies continuously from point to point)
\end{definition}

\begin{definition}
	An \textbf{Orientable surface is a 2-sided regular (smooth) surface to which we can attach, at each point, a unit normal vector}
\end{definition}

If $S$ is the boundary of a 3-dimension region. Then it is oriented by the outward pointing unit normal.

\begin{definition}
	Let $F$ be a vector field defined on a surface $S$ which is oriented by a unit normal $n$. The \textbf{(vector) surface integral of $F$} over $S$, denoted
	$$\int_S F \cdot dS$$
	is defined by
	$$\int_S F \cdot dS = \int_S F \cdot n dS$$
\end{definition}

\begin{note}
	\begin{enumerate}
		\item{$F \cdot n$ is a scalar field}
		\item{From physics, $\int_S F \cdot n dS$ is called a flux of the vector field $F$ across $S$. When the surface is closed, we sometimes use the notation
		$$\oiint_S F \cdot n dS $$
		In this case, refer to the \textbf{flux out of $S$} if $n$ is the unit outer normal and \textbf{flux into $S$} if $n$ is the unit inner normal.
		}
		\item{There are other forms of (vector) surface integrals, but this is the only one we'll consider in this course}
		\item{I'll (Dr. Eric Moore) will use both notations}
	\end{enumerate}
\end{note}

Why is this a reasonable definition?\\
\\
It can be written as components. One in the normal direction and the other in the tangential direction.

$F = (F \cdot n) n + G$ where $G \cdot n = 0$. $G$ is tangent to the surface and $F \cdot n$ is the normal component. It is the normal piece that we need to consider to see what passes through the surface.

$$\int_S (F \cdot n) dS = \text{(area of S)(average normal )}$$

Take $S$ to be a porous surface. What flows through in time $t$?\\
\\
Say $\delta s$ is a small piece of the surface. The amount that flows through $\delta s$ will fill a parallelepiped with length $t\cdot || F ||$ in direction $F$. What's the volume?\\
\\
$$\Delta s (||\text{projection of $t\cdot F$ onto the normal $(t\cdot \Delta S)$}||) = (\Delta s)(tF \cdot n) = (F \cdot n \Delta s)$$

How do we calculate? (Vector surface integrals)
\begin{itemize}
	\item{restrict to $\mathbb{R}^3$}
	\item{assume $S$ is a 2-dimension surface in $\mathbb{R}^3$ oriented by $u$ and assume $\Phi (u,v)$ is a regular parameterization of $S$.}
\end{itemize}
We find $\phi_u, \phi_v$ and $\phi_u \times \phi_v$ (a normal)

$$\frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||} \text{ is a unit normal}$$

This implies that
$$\frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||} = \pm n$$
which implies that
$$\phi_u \times \phi_v = \pm || \phi_u \times \phi_v|| n$$

The plus and minus determines if it is in the same direction as $n$ or in the opposite direction.

\begin{note}
	it is an adequate to check at a single point the orientation of the surface
\end{note}


\begin{align*}
	\int_\Phi F \cdot dS &= \int_\Phi F \cdot n dS\\
	&= \int_D F (\Phi (u,v)) \cdot n (\Phi(u,v)) || \phi_u \times \phi_v|| dA\\
	&= \pm \int_D F (\Phi (u,v)) \cdot \frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||} ||\phi_u \times \phi_v|| dA\\
	&= \pm \int_D (F(\Phi (u,v)) \cdot \phi_u \times \phi_v) dA
\end{align*}

\begin{example}
	Evaluate $\int_S F \cdot dS$

	$F(x,y,z) = (x,y,z), S$ is the part of the place $x+y+z=1$ which lies in the first octant and $n$ points upward.\\
	\\
	We parameterize $S$ by $\Phi (u,v) = (u,v, 1-u-v)$ where $(u,v)\in D$\\
	\\
	$D$ is the projection in the $xy$-plane.
	$$0 \leq v \leq 1-u$$
	$$0 \leq u \leq 1$$
	$$\phi_u \times \phi_v = (1,1,1)$$

	Since the $z$-component is positive, it points relatively upward. So its in the correct direction.

	\begin{align*}
		\int_\Phi F \cdot dS &= \int_D (F(\Phi (u,v)) \cdot \phi_u \times \phi_v) dA\\
		&= \int_0^1 \int_0^{1-u} (u,v,1-u-v) \cdot (1,1,1) dv du\\
		&= \int_0^1 \int_0^{1-u} (u+v+1-u-v) dv du\\
		&=\int_0^1 \int_0^{1-u} dv du\\
		&= \int_0^1 (1-u) du\\
		&= [u=\frac{1}{2}u^2]_0^1 = 1 - \frac{1}{2} = \frac{1}{2}
	\end{align*}



\end{example}

\begin{example}
	$F(x,y,z) = (x,y,z), S$ is that part of the paraboloid $z=4-x^2-y^2$ with $0 \leq x \leq 1$ and $0 \leq y \leq 1$ with $n$ pointing outward\\
	\\
	Parameterize $S$ by $\Phi (u,v) = (u,v, 4-u^2-v^2)$ where $0 \leq u \leq 1$ and $0 \leq v \leq 1$ and $\phi_u \times \phi_v = (2u, 2v, 1)$\\
	\\
	Since $z=1 > 0$, this is the correct direction.

	\begin{align*}
		\int_\Phi F \cdot dS &= \int_0^1 \int_0^1 F(\Phi (u,v)) \cdot \phi_u \times \phi_v dudv\\
		&= \int_0^1 \int_0^1 (u,v,4-u^2-v^2) \cdot (2u, 2v, 1) dudv\\
		&= \int_0^1 \int_0^1 (2u^2 + 2v^2 + 4-u^2-v^2)dudv\\
		&= \int_0^1 \int_0^1 (4+u^2+v^2) dudv\\
		&= \int_0^1 [4u+ \frac{u^3}{3} + v^2 u]^1_0 dv\\
		&= \int_0^1 (4+\frac{1}{3} + v^2)dv\\
		&=[4v + \frac{v}{3} + \frac{v^3}{3}]_0^1\\
		&= 4 + \frac{2}{3} = \frac{14}{3}
	\end{align*}

\end{example}

\begin{example}
	Integrate $F(x,y,z) = (x,y,z)$ on the covered bowl:
	\underline{bowl:} $S_1 : z = x^2 + y^2$, $0 \leq z \leq 1$\\
	\underline{cover:} $S_2 : (z-1)^2 + x^2 + y^2 =1$, $1 \leq z \leq 2$\\
	Where both $S_1, S_2$ are oriented by the outward unit normal.\\
	\\
	\underline{bowl ($S_1$):} $\Phi (u,v) = (ucos(v), usin(v), u^2)$ where $0 \leq u \leq 1$, $0 \leq v \leq 2 \pi$
	$$\phi_u = (cosv, sinv, 2u)$$
	$$\phi_v = (-usinv, ucosv, 0)$$
	$$\phi_u \times \phi_v = u(-2ucosv, -2usinv, 1)$$

	For the unit outer normal, the $z$-component would have to be negative. However, the $z$ component of $\phi_u \times \phi_v$ is $1 > u > 0$ so the unit normal vector

	\begin{align*}
		-\int_{S_1} F \cdot n ds &= - \int_{u,v} (ucosv, u^2, usinv) \cdot (-2u^2 cosv, -2u^2 sinv, u) dA\\
		&= -\int_0^1 \int_0^{2\pi} (-2u^3cos^2 v - 2u^4 sinv + u^2 sinv) dv du = \frac{\pi}{2}
	\end{align*}

	\underline{cover ($S_2$):} $\Phi (u,v) = (cos(v)sin(u), sin(v)sin(u), 1+cos(u))$ where $0\leq v \leq$, $0 \leq u \leq \frac{\pi}{2}$\\
	\\
	$$\phi_u = (cos(v)cos(u),sin(v)cos(u), -sin(u))$$
	$$\phi_v = (-sin(v)sin(u), cos(v)sin(u), 0)$$
	$$\phi_u \times \phi_v =sin(u)(cos(v)sin(u), sin(v)sin(u), cos(u))$$
	The $z$-component of both $u$ and $\phi_u \times \phi_v$ are positive, so they are the correct direction.
	\\
	\\
	$\Longrightarrow$ \begin{align*}
		\int_{S_2} F dS &= \int_{u,v} (\cos v \sin v, \cos u + 1, \sin v \sin u)\cdot (\cos v \sin^2 u, \sin v \sin^2 u, \cos u \sin u) dA\\
		&= \int_0^{\frac{\pi}{2}} \int^{2\pi}_0 (\cos^2 v \sin^3 u + \cos u \sin v \sin^2 u + \sin v \sin^2 u + \sin v \sin^2 u \cos u) dv du\\
		&= \frac{2\pi}{3}
\end{align*}
Therefore $$\int_{S_1 \cup S_2} F dS = \frac{\pi}{2} + \frac{2\pi}{3}$$
\end{example}

\newpage

\section{Monday, March 5, 2018}

\subsection{Divergence}

\begin{definition}
	Let $F : \mathbb{R}^n \mapsto \mathbb{R}^n$ be a vector field, we determine the \textbf{divergence of $F$}, denoted $div F$ or $\nabla F$ by
	$$div F = \frac{\partial F_1}{\partial x_1} + ... + \frac{\partial F_n}{\partial x_n} = \sum_{i=1}^n \frac{\partial F_i}{\partial x_i}$$
\end{definition}

\begin{note}
		The $div F$ is a funtion $div F : \mathbb{R}^n \mapsto \mathbb{R}$
\end{note}
\begin{note}
	Geometrically, $div F$ represents the rate of expansion (or contraction) per unit volume. For example, given $F:\mathbb{R}^n \mapsto \mathbb{R}^2, F(x,y,z) = (x,y,z)$
	$$div F = \frac{\partial x}{\partial x} + \frac{\partial y}{\partial y} + \frac{\partial z}{\partial z} = 3$$
	So in this example, the volume is expanding at 3 cubic units per unit volume per unit time.
\end{note}

\begin{example}
	(Using the previous equation)\\
	Let $R$ be the interior of the bowl, we want $\int_R div F dV$
	$$div F = \frac{\partial x}{\partial x} + \frac{\partial y}{\partial z} + \frac{\partial z}{\partial y} = 1 + 0 + 0 = 1$$
	$\Longrightarrow \int_R div F dV = \text{Volume of $R$}$\\
	We know $R : 0 \leq x^2 + y^2 \leq 1, x^2 + y^2 \leq z \leq sqrt{1 - (x^2 + y^2)}$
	So we will use cylindrical coordinates (polar coordinates with a $z$ axis.)
	\begin{align*}
		\text{Volume of $R$} &=  \int_0^{2\pi} \int^1_0 \int^{1+\sqrt{1-r^2}}_{r^2} rdzdrd\theta\\
		&= 2\pi  \int^1_0 \int^{1+\sqrt{1-r^2}}_{r^2} r dzdr\\
		&=  2\pi  \int^1_0 (r+ r\sqrt{1-r^2} - r^3)dr\\
		&= \frac{7\pi}{6}
	\end{align*}
\end{example}

For the following remarks, Let $S$ be a 2 dimensional surface in $\mathbb{R}^3$ oriented by $n$
\begin{remark}
	If $\Phi (u,v)$ is a regular parameterization of $S$ and $n = \frac{\phi_u \times \phi_v}{||\phi_u \times \phi_v||}$
\end{remark}
\begin{remark}
	If $\Phi_1$ and $\Phi_2$ are two orientation preserving parameterizations of $S$ and if $F$ is a continuous vector field defined on $S$, then
	$$\int_{\Phi_1} F dS = \int_{\Phi_2} F dS$$
	Should $\Phi_1$ be orientation preserving and $\Phi_2$ be orientation reserving we would have
	$$\int_{\Phi_1} F dS = -\int_{\Phi_2} F dS$$
\end{remark}
\begin{remark}
	If $\Phi_1$ and $\Phi_2$ are (regular) parameterizations of $S$ and if $f$ is a countinuous real valued function defined on $S$ then
	$$\int_{\Phi_1} f dS = \int_{\Phi_2} f dS$$
\end{remark}

\begin{example}
	$$F(x) = \frac{x}{||x||^3}$$
	$S$ is the unit sphere centered at the origin oriented by the unit outer normal $n$.\\
	\\
	\underline{normally written:} $\frac{r}{||r||^3}$ (said to satisfy the "inverse square law")\\
	we want to compute the flux of $F$ through $S$, $\int_S F \cdot n dS = \int_\Phi F \cdot n dS$ where $\Phi$ is any orientation preserving parameterization of $S$. Note that $$n = \frac{(2x,2y,2z)}{||(2x,2y,2z)||} = \frac{x}{||x||}$$\\
	This works for any sphere at the origin no matter the radius\\
	\begin{align*}
		\int_\Phi F \cdot n dS &= \int_\Phi \frac{x}{||x||^3}\cdot \frac{x}{||x||} dS\\
		&= \int_\Phi \frac{||x||^2}{||x||^4} dS\\
		&= \int_\Phi \frac{1}{||x||^2} dS\\
		&= \int_\Phi 1 dS\\
		&= \text{the surface area of $S$} = 4\pi
	\end{align*}
	Since $S$ is a unit sphere, even if the sphere had radius $S$, then $\int_\Phi F \cdot n dS = 4\pi$
\end{example}

\begin{theorem}
	\textbf{Divergence Theorem} or Gauss' Theorem is the following. Let $S$ be a closed surface in $\mathbb{R}^3$ oriented by the outward pointing unit normal $n$ and let $R$ be the region enclosed by $S (\partial R = S)$. Suppose $F$ is a smooth vector field defined throughout $R$. Then:
	$$\iiint_R div F dV = \iint_{S=\partial R} F dS$$
\end{theorem}

\begin{remark}
	The theorem could have been stated with different notation
	$$\iiint_R \nabla \cdot F dV = \iiint_R div F dV = \iint_{\partial R} F dS = \iint_{\partial R} F\cdot n dS$$
\end{remark}

\begin{remark}
	(In 2-dimensions), Divergence theorem in $\mathbb{R}^2$ is: Let $\gamma$ be a jordan curve in $\mathbb{R}^2$ oriented in the counterclockwise direction, Let $F = (F_1, F_2)$ be a smooth vector field defined throughout $R$ where $R$ is the region enclosed by $\gamma$. Put $G=(F_2 -F_1) \Longrightarrow F \cdot G = 0$ and $||F||=||G||$ Then $\gamma '(t) = (\gamma_1 '(t), \gamma_2 '(t))$ is a tangent and $(\gamma_2 '(t)-\gamma_1 '(t))$ is an outward normal $n$.\\
	Green's Theorem gives $\int_\gamma G \cdot n ds = \int_\gamma F \cdot T ds \overset{\text{Green}}{=} \iint_R (\frac{\partial G_1}{\partial x} + \frac{\partial G_2}{\partial x})dA = \iint_R div G dA$\\
	\\
	This is sometimes called the 2-dimensional divergence theorem.
\end{remark}

\begin{note}
	Gauss' Theorem is a generalization of Green's Theorem to $\mathbb{R}^3$, the closed surgace has 2 natural orientations; outward and inward. (If it does not tell you the orientation then by default the oriengation is given by the outward pointing normal)
\end{note}

\begin{remark}
	If $S$ if a closed surface in $\mathbb{R}^3$, the default orientation is the outward pointing unit normal
\end{remark}

\newpage

\section{Monday, April 2, 2018}

\begin{example}
	Evaluate
	$$\int_\gamma \omega = \int_\gamma [y + ln(x^2 + 1)]dx + [z + \arctan(y)]dy + e^{z^2} dz$$
	where $\gamma$ consists of straight line segments from $(2,0,0)$ to $(0,2,0)$ to $(0,0,4)$ and back to $(2,0,0)$.\\
	$d\omega = dydx + dzdy = - (dxdy + dydz)$
	can we find a super surface $\mathbb{S}$ containing $\gamma$?\\

	Consider the plane $z=4-2x-2y$\\
	$S$ is the piece of $\mathbb{S}$ cut out by line segements (this is the piece of $\mathbb{S}$ lying over the region in the $xy$-plane bounded by $y=0, x=0, x+y=2$).\\
	Denote this region (in $xy$-plane) by $D$, For compatibility, $n$ points upward. We can apply Stokes' Theoreom on it. (Note: since $dxdy = 1 > 0$, this is the correct orientation)
	\begin{align*}
		\int_\gamma \omega &= \int_S d\omega\\
		&= -\int (dxdy+dydz)\\
		&= -\int (\begin{vmatrix}
1 & 0 \\
0 & 1
\end{vmatrix} + \begin{vmatrix}
0 & 1 \\
-2 & -2
\end{vmatrix})dA\\
&= -\int_D (1+2)dA\\
&= -3\int_D dA\\
&= -3\int^2_0 \int^{2-x}_0 dydx\\
&= -3\int^2_0 (2-x)dx\\
&= -6
	\end{align*}
\end{example}

\begin{example}
	Compute $\int_S Fds$ where $F=(1,2,3)$ and $S$ is the cone with the vertex $(1,3,5)$ and the base is the circle $x^2+y^2 = 1$ in the $xy$-plane oriented with the normal that points away from the center line.\\
	$S$ is not closed so we can use the Divergence Theorem.\\
	but $Div F = 0$ and $F$ is defined throughout $\mathbb{R}^3$. There exists a $G$ such that $F = curl G$.\\
	Use Stokes' Theorem when $(n=3, k=2)$

	$$\int_S F dS = \int_S (curl G)dS = \int_{\partial S} G ds$$
	take $G = (2z, 3x, y)$. $\partial S$ is just the circle $x^2 + y^2 = 1$ in the $xy$-plane. $\gamma (t) = (\cos t, \sin t, 0), t \in [0,2\pi]$

	\begin{align*}
		\int_\gamma (2(0), 3\cos t, \sin t) \cdot (-\sin t, \cos t, 0)dt &=  3\int^{2\pi}_0 \cos^2 t dt\\
		&= 3\int^{2\pi}_0 (\frac{1}{2} +  \frac{1}{2} \cos t)dt\\
		&= 3 \frac{1}{2} 2\pi = 3\pi
	\end{align*}
\end{example}

\begin{example}
	Let $S$ be the surface $y=10-x^2-z^2$ with $y\geq 1$ oriented by the unit normal with positive $y$-component. Evaluate $\int_S d\omega$ when $$\omega = (2xyz+5z)dx + e^x \cos (yz) dy + x^2y dz$$
	$$d\omega = x^2 +e^x \sin (yz) dydx + 5dzdx + (e^x \cos (yz) - 2xz)dxdy$$
\end{example}

\newpage

\section{Friday, April 6, 2018}

\begin{example}
	$S$ is the surface $y=10-x^2-z^2$ with $y\geq 1$ oriented by the unit normal with a positive $y$-component. Evaluate $\int_S d\omega$ where $$\omega = (2xyz+5z)dx + e^x \cos(yz)dy + x^2ydz$$
	$$d\omega = (x^2 +e^x y \sin(yz))dydz + 5dzdx + (e^x \cos(yz) - 2xz)dxdy$$

	$\partial S$ is the circle $x^2 + z^2 = 9, y = 1$. If $S'$ is the unit disk $x^2 + z^2 \leq 9, y = 1$, then $\partial S = \partial S'$\\
	\\
	The compatible unit normal in $S'$ is $(0,1,0) = e_2$, apply stoke's theorem again, giving
	$$\int_S d\omega = \int_{\partial S} \omega = \int_{\partial S'} \omega = \int_{S'} d\omega$$

	$$dzdx = 1, dydz = 0, dxdy = 0$$
	\begin{align*}
		\int_S d\omega &= \int_{S'} 5 dzdx\\
		&= 5 \times \text{(Area of $S'$)}\\
		&= 5(\pi3^2) = 45\pi
	\end{align*}
\end{example}

We are now going introduce $\nabla$.\\
\\
$$\nabla = (\frac{\partial}{\partial x_1} + ... + \frac{\partial}{\partial x_n})$$
$\nabla$ acts on functions of vector fields
$$\nabla f = (\frac{\partial}{\partial x_1} + ... + \frac{\partial}{\partial x_n})f = (\frac{\partial f}{\partial x_1} + ... + \frac{\partial f}{\partial x_n}) $$
$$\nabla \cdot F = (\frac{\partial}{\partial x_1} + ... + \frac{\partial}{\partial x_n}) (f_1,...,f_n) = (\frac{\partial f_1}{\partial x_1} + ... + \frac{\partial f_n}{\partial x_n})$$
\underset{in $\mathbb{R}^3$}, $f=(f_1, f_2, f_3)$

$$\nabla \times f = (\frac{\partial}{\partial x_1} + ... + \frac{\partial}{\partial x_n}) \times (f_1,...,f_n) = (\frac{\partial F_3}{\partial y} - \frac{\partial F_2}{\partial z}, \frac{\partial F_1}{\partial z} - \frac{\partial F_3}{\partial x}, \frac{\partial F_2}{\partial x} - \frac{\partial F_1}{\partial y})$$

$$\nabla^2 = \Delta = \frac{\partial^2}{\partial x_1^2} + ... + \frac{\partial^2}{\partial x_n^2} = \nabla \cdot \nabla$$

$\Delta f = 0 \Longrightarrow f$ is harmonic.

\subsection{Calculus of Variations}

The fundamental problem is to maximize or minimize
$$V[a]=\int^T_0 F[t, a(t, a'(t))]dt$$
subject to $a(0) = a, a(T)=b$ with $a,b,T$ given.\\
We assume that $F$ is $C^2$ with respect to all 3 variables and that $a$ is a smooth path.

\begin{definition}
	A smooth path $a^*$ that yields an extremum of $V[a]$ is called an \textbf{extremal} and $V$ is called a \textbf{functional} (assigns a numerical value to each of a class of functions $a(t)$). The initial and terminal constraints are the \textbf{endpoint conditions}.
\end{definition}

We want to find a curve $y=c(t)$ such that $a(c_0) = b_0, a(c_1) = b_1$ and such that the length of the curve is minimized.\\
\\
The curve of $a(t)$ goes from $(c_0, b_0)$ to $(c_1, b_1)$\\
\\
We need to minimize
$$V[a] = \int^{c_1}_{c_0} \sqrt{1 + (a (t))^2} dt$$

\begin{example}
	A brachistochrone problem (Johan Brenoulli, 1696).\\
	Suppose a thin wire (in the shape of a curve) joins 2 points at different heights. Suppose a bead is placed on the wire at a higher point and allowed to slide under gravity, starting from the rest and assuming no friction. What should be the shape of the curve so the bead reaches the lwoer point in the least time? Find a curve $a(t)$ such that $a(0)=0$ and $a(x)=y$ and such that the time taken by a freely sliding particle, starting at rest, is minimized.

	$$V[a] = \int^x_0 \sqrt{\frac{1+(a'(x))^2}{2 g x}}$$
	where $g$ is the gravitational force.
\end{example}

\begin{example}
	A company wants to maximize profit $\pi$ from $t_0 = 0$ to $t_1 = T$ and finds that the demand depends not only on price, but also on the rate of change in price over time. (Both price and rate of change are functions of time.)

	$$max V[p]=\int^T_0 \pi[t, p(t), p'(t)]dt$$
\end{example}

A necessary, but not sufficient, first order condition for an extremal is \textbf{Euler's Equation}

$$\frac{\partial F}{\partial a} = \frac{d}{dt} \frac{\partial F}{\partial a'}$$

$$\frac{\partial V}{\partial \epsilon}, \epsilon \text{ is some new variable}$$
$a(t) = a^* (t) + \epsilon p(t)$\\

If we use the Chain Rule and expand, we get
$$\frac{\partial F}{\partial a} = \frac{\partial^2 F}{\partial t \partial a'} + \frac{\partial^2 F}{\partial a \partial a'}(a'(t)) + \frac{\partial^2 F}{\partial (a')^2}(a''(t))$$
for all $t\in [0,T]$, a second order differential equation

\begin{remark}
	Since the Euler Equation is (generally) a 2nd order non-linear differential equation, there will be 2 aarbitrary constants. Any endpoint conditions should provide sufficient information to obtain values for the arbitrary constants
\end{remark}

\begin{remark}
	Should the integrand be linear in $a'$ we could see no solution or all solutions being equivalent ($V$ depending only on the endpoints).
\end{remark}

\begin{remark}
	We can also find extrema subject to functional constraints.
\end{remark}

\begin{example}
	Consider
	$$V[a]=\int^T_0 F[t, a(t, a'(t))]dt$$
	subject to the constraint
	$$\int^T_0 G[t, a(t, a'(t))]dt = c$$
	we multiply the constraint by $\lambda$
	$$\int^T_0 H[t, a(t, a'(t))]dt$$
	where $H = F + \lambda G$. Euler becomes
	$$\frac{\partial H}{\partial a} = \frac{d}{dt} \frac{\partial H}{\partial a'}$$
\end{example}

\begin{example}
	Optimize
	$$\int^1_0 [3(a'(t))^2 + 6t a(t) - 7t]dt$$
	subject to $a(0)=1, a(1)=2$. Put $F = 3(a'(t))^2 + 6t a(t) - 7t$
	$$\frac{\partial F}{\partial a} = 6t, \frac{\partial F}{\partial a'} = 6a'$$
	Euler becomes:
	$$\frac{\partial F}{\partial a} = \frac{d}{dt} \frac{\partial F}{\partial a'}$$
	$$6t = \frac{d}{dt} (6a') = 6a''$$
	Integrate twice, add a constraint each time
	$$\int 6tdt = \int 6a''dt \Longleftrightarrow 3t^2+c_1 = 6a'$$
	\begin{align*}
		t^3 + c_1 t + c_2 &= 6a\\
		a(t) &= \frac{1}{6}t^3 + \frac{c_1}{6}t + \frac{c_2}{6}
	\end{align*}
	$$a(0)=1, 1 = \frac{1}{6}0 + \frac{c_1}{6}0 + \frac{c_2}{6} \Longrightarrow c_2 = 6$$
	$$a(1)=2, 2 = \frac{1}{6} + \frac{c_1}{6} + 1 \Longrightarrow c_1 = 5$$
	So the solution is
	$$a(t)=\frac{1}{6} t^3 + \frac{5}{6} t + 1$$
\end{example}

\begin{example}
	Find the curve which minimizes the distance between $(1,1) \& (3,5)$ in the $xy$-plane\\
	\\
	Let the curve be $y(x), y(1)=1, y(3)=5$
	$$\int^3_1 \sqrt{1+ (y'(x))^2} dx$$
	$$F = \sqrt{1+ (y')^2}, \frac{\partial F}{\partial y} = 0, \frac{\partial F}{\partial y'} = \frac{y'}{\sqrt{1+ (y')^2}}$$
	Euler would then be:
	$$0 = \frac{d}{dx} (\frac{y'}{\sqrt{1+ (y')^2}})$$
	So we know that $c=\frac{y'}{\sqrt{1+ (y')^2}}$ and we must solve for $y'$

	$$y' = \sqrt{\frac{c^2}{1-c^2}} = c_1$$
	We will integrate this
	$$y(x) = c_1 x + c_2$$
	a straight line, as expected.\\
	So the two equations we've left with is
	$$1=y(1)=c_1 + c_2, 5 = y(3)=3c_1 + c_2$$
	we substract them to get
	$$-4 = -2c_1 \Longrightarrow c_1 = 2 \Longrightarrow c_2 = -1 \Longrightarrow y = 2x-1$$
	Why is this a minimum?
\end{example}

\begin{example}
	Find the curve $a(t)$ which optimizes the integral
	$$\int^2_0 [(a'(t))^2+4ta(t)+t]dt$$
	with the constraints $a(0)=1, a(2)=4$. Let $F = [(a'(t))^2+4ta(t)+t]$
	$$\frac{\partial F}{\partial a} = 4t, \frac{\partial F}{\partial a'} = 2a', \frac{d}{dt} (2a'') = 4t$$
	Integrating we get
	$$2t^2 + c_1 = 2a' \Longleftrightarrow \frac{1}{3}t^2 + \frac{1}{2} c_1 t + \frac{1}{2}c_2 = a(t)$$
	So $a(0)=1 \Longrightarrow c_2 = 2$ and $$a(2)=4 \Longrightarrow \frac{8}{3} + \frac{2c_1}{2} + 1 = 4 \Longrightarrow c_1 = 4 - \frac{8}{3} - 1 = \frac{1}{3}$$
	So the curve is
	$$a(t) = \frac{1}{3}t^3 + \frac{1}{6} t + 1$$
\end{example}

\begin{example}
	Evaluate
	$$\int_\gamma (x^2 + z^2)dx + ydy + zdz$$
	where $\gamma (t) = (\cos t, \sin t, \cos^2 t - \sin^2 t), t \in [0,2\pi]$\\
	$\gamma$ is a closed curve in $\mathbb{R}^3$. The surface $z=x^2-y^2$ will contain $\gamma$. $\mathbb{S}$ is the graph of $z=x^2-y^2$ and $S$ is the piece of $\mathbb{S}$ cut out by $\gamma$. $\gamma$ is counterclockwise when viewed from above. For $S$ to be oriented in a compatible manner, we need $n$ to be the upward pointing normal. Since the conditions of Stokes Theorem are satisfied, we can apply it, giving
	$$\int_{\partial S =\gamma} \omega = \int_S d\omega$$
	$$\Phi (u,v) = (u,v, u^2-v^2), v \in [-\sqrt{1-u^2}, \sqrt{1-u^2}], u\in [-1,1]$$
	To check the orientation, we look at the $z$-component.
	$$dxdy = \frac{\partial (x,y)}{\partial (u,v)} = \begin{vmatrix}
1 & 0 \\
0 & 1
\end{vmatrix} = 1$$
A positive $z$-component $\Longrightarrow \Phi$ is orientation preserve.
$$\omega = (x^2 + z^2)dx + ydy + zdz, d\omega = 2zdzdx$$
\begin{align*}
	\int_\gamma \omega &= \omega_S d\omega\\
	&= \int_\Phi 2zdzdx = \int^1_{-1} \int^{\sqrt{1-u^2}}_{-\sqrt{1-u^2}} 2(u^2-v^2) \begin{vmatrix}
2u & -2v \\
1 & 0
\end{vmatrix}dv du\\
&= \int^1_{-1} \int^{\sqrt{1-u^2}}_{-\sqrt{1-u^2}} 2(u^2-v^2) (2v)dv du\\
&= 4\int^1_{-1} \int^{\sqrt{1-u^2}}_{-\sqrt{1-u^2}} (u^2v-v^3)dvdu\\
&\overset{\text{odd}}{=} 4 \int^-1_1 0 du = 0
\end{align*}
\end{example}

\end{document}
